{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-\\.]+"},"docs":[{"location":"","text":"Welcome Welcome to the Plasma Engine documentation. You will find all pages listed in the sidebar on the left. This is split into 2 major sections mapping to each engine version. The organization of docs between engine version may differ as they are build for different audiences. Useful For Programmer Building Source Library Structure String Usage String Formatting Container Usage Custom Code Overview File System Reflection Resource Management Useful For Animators Blender Export Animated Mesh Asset Skeleton Asset Skeletal Animation Overview Animation Controller Property Animation Curves Gradients Useful For 3D Artist Mesh Overview Texture Overview Lighting Overview Render Pipeline Camera Component Occluder Component Terrain If you have any feedback or recommendations then please feel free to create a ticket/PR on the github or reach out via discord","title":"Home"},{"location":"#welcome","text":"Welcome to the Plasma Engine documentation. You will find all pages listed in the sidebar on the left. This is split into 2 major sections mapping to each engine version. The organization of docs between engine version may differ as they are build for different audiences.","title":"Welcome"},{"location":"#useful-for-programmer","text":"Building Source Library Structure String Usage String Formatting Container Usage Custom Code Overview File System Reflection Resource Management","title":"Useful For Programmer"},{"location":"#useful-for-animators","text":"Blender Export Animated Mesh Asset Skeleton Asset Skeletal Animation Overview Animation Controller Property Animation Curves Gradients","title":"Useful For Animators"},{"location":"#useful-for-3d-artist","text":"Mesh Overview Texture Overview Lighting Overview Render Pipeline Camera Component Occluder Component Terrain If you have any feedback or recommendations then please feel free to create a ticket/PR on the github or reach out via discord","title":"Useful For 3D Artist"},{"location":"Shipping/project-export/","text":"Project Export Project Export is the step to create a package that contains all the files needed to play your game, excluding the files that are only needed for development. The data directories of your project contain a lot of files that are only needed during development. Additionally, they are usually stored in various locations, for example the Base directory, which is needed by all games, is located in the SDK folder, whereas your project files probably are located somewhere entirely different. Finally, you also need various binaries ( EXE and DLL on Windows) to launch your game, which are again located somewhere different. The project export feature consolidates all these files into a single directory, so that it is easy to distribute. Export Project Dialog To get started with generating a self-contained package of your game, use the project export dialog that you find in the editor under Editor > Export Project... . Select an output folder and click Export Project . Once the export is finished, it will automatically open the folder where the files have been copied to. The export also generates one .bat file for every scene in your project. These scripts launch the respective scene with plPlayer . Configuring Project Export When you export your project for the first time, these files are added to your project directory: ProjectBinaries.plExportFilter ProjectData.plExportFilter The first file is used to determine which binaries should get included in the package. The second file is used to determine which data files should get included. By default these files #include predefined export filters: Data\\Base\\CommonData.plExportFilter Data\\Base\\CommonBinaries.plExportFilter These files set up the rules for typical use-cases. You can extend them in your project config files, or you can remove the #include and fully define your own rules. The plExportFilter files contain two sections: [EXCLUDE] and [INCLUDE] . Without any filter, all files are included in the output package. To exclude certain files or file types, a pattern has to be added to the [EXCLUDE] section. However, sometimes you want to exclude all files of a certain type (e.g. exe ) but include a single one regardless (for example your game.exe). In this case, add it to the [INCLUDE] section, to override the exclusion filter. Each line in the file represents one file path pattern: If it starts with a * , it matches paths that end with this pattern. If it ends with a * , it matches paths that start with this pattern. If it starts and ends with a * , it matches paths that contain this pattern. At any other location, * is not allowed. All paths are considered to be relative to their respective data directory. Note: For inspiration how to use these path patterns, see the files Data\\Base\\CommonData.plExportFilter and Data\\Base\\CommonBinaries.plExportFilter . Limitations At this time it is not analyzed which plugin DLLs are actually needed, instead all DLLs are included. Edit your ProjectBinaries.plExportFilter to control this. Currently the export step always creates .bat files to load each scene with plPlayer. There is no way to automatically set up something different. You currently can't automatically execute custom logic (C++ code, or a script) to finalize the package. See Also Profiling Supported Platforms plPlayer","title":"Project Export"},{"location":"Shipping/project-export/#project-export","text":"Project Export is the step to create a package that contains all the files needed to play your game, excluding the files that are only needed for development. The data directories of your project contain a lot of files that are only needed during development. Additionally, they are usually stored in various locations, for example the Base directory, which is needed by all games, is located in the SDK folder, whereas your project files probably are located somewhere entirely different. Finally, you also need various binaries ( EXE and DLL on Windows) to launch your game, which are again located somewhere different. The project export feature consolidates all these files into a single directory, so that it is easy to distribute.","title":"Project Export"},{"location":"Shipping/project-export/#export-project-dialog","text":"To get started with generating a self-contained package of your game, use the project export dialog that you find in the editor under Editor > Export Project... . Select an output folder and click Export Project . Once the export is finished, it will automatically open the folder where the files have been copied to. The export also generates one .bat file for every scene in your project. These scripts launch the respective scene with plPlayer .","title":"Export Project Dialog"},{"location":"Shipping/project-export/#configuring-project-export","text":"When you export your project for the first time, these files are added to your project directory: ProjectBinaries.plExportFilter ProjectData.plExportFilter The first file is used to determine which binaries should get included in the package. The second file is used to determine which data files should get included. By default these files #include predefined export filters: Data\\Base\\CommonData.plExportFilter Data\\Base\\CommonBinaries.plExportFilter These files set up the rules for typical use-cases. You can extend them in your project config files, or you can remove the #include and fully define your own rules. The plExportFilter files contain two sections: [EXCLUDE] and [INCLUDE] . Without any filter, all files are included in the output package. To exclude certain files or file types, a pattern has to be added to the [EXCLUDE] section. However, sometimes you want to exclude all files of a certain type (e.g. exe ) but include a single one regardless (for example your game.exe). In this case, add it to the [INCLUDE] section, to override the exclusion filter. Each line in the file represents one file path pattern: If it starts with a * , it matches paths that end with this pattern. If it ends with a * , it matches paths that start with this pattern. If it starts and ends with a * , it matches paths that contain this pattern. At any other location, * is not allowed. All paths are considered to be relative to their respective data directory. Note: For inspiration how to use these path patterns, see the files Data\\Base\\CommonData.plExportFilter and Data\\Base\\CommonBinaries.plExportFilter .","title":"Configuring Project Export"},{"location":"Shipping/project-export/#limitations","text":"At this time it is not analyzed which plugin DLLs are actually needed, instead all DLLs are included. Edit your ProjectBinaries.plExportFilter to control this. Currently the export step always creates .bat files to load each scene with plPlayer. There is no way to automatically set up something different. You currently can't automatically execute custom logic (C++ code, or a script) to finalize the package.","title":"Limitations"},{"location":"Shipping/project-export/#see-also","text":"Profiling Supported Platforms plPlayer","title":"See Also"},{"location":"ai/sensor-components/","text":"Sensor Components Sensor components are used to detect objects in a certain area and inform other game code about them. Contrary to triggers , they use the spatial system , so they work even without a physics engine. However, the sensor component can utilize additional physics raycasts, to determine whether something inside the volume is also visible and not occluded by walls. Sensor components are meant for AI use cases, such as determining line of sight, hearing noises or even smelling odors. Generally the sensors query the spatial system to detect certain objects. Use the marker component to make something detectable. For example, to make a creature able to smell the player, regularly drop markers at the player's current location, that vanish after a while, so that the creature can detect and follow these markers. State Reporting Sensors keep track of the objects that entered their volume. During every update they determine whether objects are still inside the volume or whether their visibility changed (line-of-sight occluded). If anything changes, a sensor sends the message plMsgSensorDetectedObjectsChanged , which contains the full array of currently detected objects. Performance Considerations Sensor components poll the world in regular intervals and thus incur a performance cost. The UpdateRate determines how often this polling happens. Internally updates from many sensors are automatically distributed evenly across frames, to prevent performance spikes at regular intervals. Still, it is best to reduce the update rate as much as possible. For example in a game with large levels, you should check how close the player is to an NPC and dynamically adjust the update rate. At a large distance, the sensor can be set to update only every second, or you could even deactivate the sensor entirely. Similarly, you can use the 'alterness' state of an NPC to increase or decrease the sensor update rate. Finally, you should decide whether doing a visibility check is always necessary. The sensor would do this check for every possible target at every update. However, for a lot of game logic, once something has attracted attention, further visibility checks are not necessary. In such a case, it can be more efficient to do visibility raycasts only while a creature is not yet alert. Component Properties Shared Component Properties UpdateRate : How often the sensor component should query the world for changes. The higher the update rate, the more responsive it will be, and the less likely that short events are missed. However, higher update rates also require more processing time. SpatialCategory : The spatial category of objects that should trigger the sensor component. TestVisibility : If enabled, the sensor will cast additional rays using the physics engine, to determine whether the target is occluded by walls or clearly visible. CollisionLayer : The collision layer to use for the visibility raycast. ShowDebugInfo : If enabled, additional debug geometry is rendered to visualize the sensor volume and state. Color : This color is used for the debug visualization. Can be used to easily distinguish what type of sensor this is. Sphere Sensor Component Radius : The size of the sensor sphere. Cylinder Sensor Component Radius , Height : The dimensions of the sensor cylinder. Cone Sensor Component NearDistance , FarDistance , Angle : These all define the cone volume. Note that the cone not only has an angle and a length ( FarDistance ) but also a NearDistance . Enable ShowDebugInfo to see the exact cone shape. The near distance allows to ignore things that are up close, or to have that range covered by another sensor shape. See Also Marker Component State Machine Component Custom Code Blackboards","title":"Sensor Components"},{"location":"ai/sensor-components/#sensor-components","text":"Sensor components are used to detect objects in a certain area and inform other game code about them. Contrary to triggers , they use the spatial system , so they work even without a physics engine. However, the sensor component can utilize additional physics raycasts, to determine whether something inside the volume is also visible and not occluded by walls. Sensor components are meant for AI use cases, such as determining line of sight, hearing noises or even smelling odors. Generally the sensors query the spatial system to detect certain objects. Use the marker component to make something detectable. For example, to make a creature able to smell the player, regularly drop markers at the player's current location, that vanish after a while, so that the creature can detect and follow these markers.","title":"Sensor Components"},{"location":"ai/sensor-components/#state-reporting","text":"Sensors keep track of the objects that entered their volume. During every update they determine whether objects are still inside the volume or whether their visibility changed (line-of-sight occluded). If anything changes, a sensor sends the message plMsgSensorDetectedObjectsChanged , which contains the full array of currently detected objects.","title":"State Reporting"},{"location":"ai/sensor-components/#performance-considerations","text":"Sensor components poll the world in regular intervals and thus incur a performance cost. The UpdateRate determines how often this polling happens. Internally updates from many sensors are automatically distributed evenly across frames, to prevent performance spikes at regular intervals. Still, it is best to reduce the update rate as much as possible. For example in a game with large levels, you should check how close the player is to an NPC and dynamically adjust the update rate. At a large distance, the sensor can be set to update only every second, or you could even deactivate the sensor entirely. Similarly, you can use the 'alterness' state of an NPC to increase or decrease the sensor update rate. Finally, you should decide whether doing a visibility check is always necessary. The sensor would do this check for every possible target at every update. However, for a lot of game logic, once something has attracted attention, further visibility checks are not necessary. In such a case, it can be more efficient to do visibility raycasts only while a creature is not yet alert.","title":"Performance Considerations"},{"location":"ai/sensor-components/#component-properties","text":"","title":"Component Properties"},{"location":"ai/sensor-components/#shared-component-properties","text":"UpdateRate : How often the sensor component should query the world for changes. The higher the update rate, the more responsive it will be, and the less likely that short events are missed. However, higher update rates also require more processing time. SpatialCategory : The spatial category of objects that should trigger the sensor component. TestVisibility : If enabled, the sensor will cast additional rays using the physics engine, to determine whether the target is occluded by walls or clearly visible. CollisionLayer : The collision layer to use for the visibility raycast. ShowDebugInfo : If enabled, additional debug geometry is rendered to visualize the sensor volume and state. Color : This color is used for the debug visualization. Can be used to easily distinguish what type of sensor this is.","title":"Shared Component Properties"},{"location":"ai/sensor-components/#sphere-sensor-component","text":"Radius : The size of the sensor sphere.","title":"Sphere Sensor Component"},{"location":"ai/sensor-components/#cylinder-sensor-component","text":"Radius , Height : The dimensions of the sensor cylinder.","title":"Cylinder Sensor Component"},{"location":"ai/sensor-components/#cone-sensor-component","text":"NearDistance , FarDistance , Angle : These all define the cone volume. Note that the cone not only has an angle and a length ( FarDistance ) but also a NearDistance . Enable ShowDebugInfo to see the exact cone shape. The near distance allows to ignore things that are up close, or to have that range covered by another sensor shape.","title":"Cone Sensor Component"},{"location":"ai/sensor-components/#see-also","text":"Marker Component State Machine Component Custom Code Blackboards","title":"See Also"},{"location":"ai/AiPlugin/ai-plugin-overview/","text":"AiPlugin Overview The AiPlugin is an optional engine plugin that provides functionality for doing typical game AI tasks. To enable the plugin, use the plugin selection dialog and enable the AiPlugin . Some functionality will show up in the form of components , other functionality may only be available through C++ code. To get access to the C++ functionality, your code needs to additionally link against the AiPlugin library. Navigation The plugin provides functionality to create navmeshes on-demand at runtime. See this chapter for details. Additionally there is C++ functionality available for searching paths and steering characters along the found path. See Also Runtime Navmesh","title":"AiPlugin Overview"},{"location":"ai/AiPlugin/ai-plugin-overview/#aiplugin-overview","text":"The AiPlugin is an optional engine plugin that provides functionality for doing typical game AI tasks. To enable the plugin, use the plugin selection dialog and enable the AiPlugin . Some functionality will show up in the form of components , other functionality may only be available through C++ code. To get access to the C++ functionality, your code needs to additionally link against the AiPlugin library.","title":"AiPlugin Overview"},{"location":"ai/AiPlugin/ai-plugin-overview/#navigation","text":"The plugin provides functionality to create navmeshes on-demand at runtime. See this chapter for details. Additionally there is C++ functionality available for searching paths and steering characters along the found path.","title":"Navigation"},{"location":"ai/AiPlugin/ai-plugin-overview/#see-also","text":"Runtime Navmesh","title":"See Also"},{"location":"ai/AiPlugin/navmesh-path-test-component/","text":"Navmesh Path Test Component This component is for testing the runtime navmesh . Place an object with this component in your scene, then specify another object as the PathEnd . Make sure you have a navmesh config and a path search config set up and chosen for this component to use. Path searches are only possible while simulating a scene , so press the play button to test it. Now the plugin will automatically generate the navmesh and display the found path. If it doesn't show anything, check the Visualize options. VisualizePathState can help figuring out what went wrong. For instance the start and end points might not be in a location where the navmesh is reachable, at all, or they might be too high above the ground. Also use the navmesh visualization functionality to make sure any mesh was generated successfully, at all. Component Properties VisualizePathCorridor : If enabled, the polygons that form the corridor of the path search result are visualized. VisualizePathLine : If enabled, the shortest line through the corridor is visualized. VisualizePathState : If enabled, the current state of the path search is printed as text at the location of this object. PathEnd : A references to another object that acts as the path's destination. NavmeshConfig : Which navmesh type to do the search on. PathSearchConfig : Which path search type to use for the path search. See Also Runtime Navmesh AiPlugin Overview","title":"Navmesh Path Test Component"},{"location":"ai/AiPlugin/navmesh-path-test-component/#navmesh-path-test-component","text":"This component is for testing the runtime navmesh . Place an object with this component in your scene, then specify another object as the PathEnd . Make sure you have a navmesh config and a path search config set up and chosen for this component to use. Path searches are only possible while simulating a scene , so press the play button to test it. Now the plugin will automatically generate the navmesh and display the found path. If it doesn't show anything, check the Visualize options. VisualizePathState can help figuring out what went wrong. For instance the start and end points might not be in a location where the navmesh is reachable, at all, or they might be too high above the ground. Also use the navmesh visualization functionality to make sure any mesh was generated successfully, at all.","title":"Navmesh Path Test Component"},{"location":"ai/AiPlugin/navmesh-path-test-component/#component-properties","text":"VisualizePathCorridor : If enabled, the polygons that form the corridor of the path search result are visualized. VisualizePathLine : If enabled, the shortest line through the corridor is visualized. VisualizePathState : If enabled, the current state of the path search is printed as text at the location of this object. PathEnd : A references to another object that acts as the path's destination. NavmeshConfig : Which navmesh type to do the search on. PathSearchConfig : Which path search type to use for the path search.","title":"Component Properties"},{"location":"ai/AiPlugin/navmesh-path-test-component/#see-also","text":"Runtime Navmesh AiPlugin Overview","title":"See Also"},{"location":"ai/AiPlugin/runtime-navmesh/","text":"Runtime Navmesh The AiPlugin adds functionality to generate navmeshes on-demand at runtime. This is a very convenient workflow, as the navmesh incorporates level changes every time you restart the simulation. Input Data The runtime navmesh generation uses the collision geometry and its surfaces as the input data from which to build the navmesh. The data is queried on-demand in an area around where path searches are done. To see the resulting navmesh, enable the navmesh visualization . Navigation Configuration In a scene document select Project > Plugin Settings > Ai Project Settings to open the configuration dialog. Ground Types The Ground Types tab is used to define the types of walkable (or non-walkable) ground that appear in your game. There is a maximum of 32 ground types. In this dialog you can edit their names (double click or F2 on an entry) and select whether an entry should generally be in use (checkmark). Entries that are not checked, will not show up in other places as selectable. The <None> and <Default> ground types always exist and cannot be removed or renamed. Which ground type a physical surface represents, is specified in the surfaces . If a surface uses the <None> ground type, it will not be part of the navmesh, at all, meaning the navmesh will have a hole there. For surfaces that are generally not walkable, by no character, this is preferable over using a dedicated ground type that is then ignored by a path search type . Navmesh Types The Navmesh tab is used to configure how a navmesh is generated. Most games only need a single navmesh type, but if you have agents of very different height or radius, you may need additional ones. Enable the navmesh visualization and utilize the the navmesh path test component to test how these options affect the result. Navmesh Properties Collision Layer : The collision layer to use for finding geometry from which to generate the navmesh. This is mainly used to filter out high detail geometry, to speed up navmesh generation. It can also be used to ignore ground types that are generally not traversable (such as water). Cell Size (XY): How detailed the navmesh generation shall be. This has to be less than the desired character radius, and the agent radius should be a whole multiple of this value. For example if your cell size is 0.2 , the effective agent radius can only be 0.2 , 0.4 , 0.6 and so on. Cell Height (Z): The detail along the up axis with which to generate the navmesh. This mainly affects how well steps are detected and dealt with. Similar to the agent radius , the agent step height should be a multiple of the cell height. Agent Radius: How wide the characters are that walk over this type of navmesh. This affects how narrow passages can be, and how far away agents stay from walls. The agent radius should be a multiple of the cell size . Don't use multiple navmeshes only because characters shall have a slightly different radius. Use the same radius for all agents that are more or less the same size. Only use a separate navmesh type, for characters that are extremely different in size and should be forced to take different paths throughout narrow terrain. Agent Height: Similar to agent radius , this defines how tall a character may be. Should be a multiple of cell height . The same caveats as for agent radius apply. Agent Step Height: The maximum height of obstacles (such as stair steps) that an agent is allowed to step up. Should be a multiple of cell height . Agent Walkable Slope: The maximum slope of triangles that are considered for navmesh generation. Triangles steeper than this will be discarded as non-walkable. Path Search Types Each navmesh defines where an agent can walk, and which ground types are in which area. Path search types are used to configure how fast an agent can traverse each ground type and thus which areas the agent would prefer or avoid. Additionally a path search type can be used to prevent an agent from walking on certain ground types, at all. In this tab you can add different path search types, and for each one, you can configure the cost of traversing a ground type and whether it is allowed to be traversed at all. The higher the cost, the more an agent will try to avoid it and use a different route if possible. Traversal costs can't be zero or negative. Use the the navmesh path test component to see how these options affect path finding. Navmesh Visualization To be able to see the navmesh, use the CVar Ai.Navmesh.Visualize . There can be multiple different navmeshes for different character heights etc. This CVar enables visualization for one of them by index. So set it to 0 to see the first navmesh, 1 if you have a second one and so on. Important: No navmesh will be generated, as long as no path searches are done. See Also AiPlugin Overview Navmesh Path Test Component","title":"Runtime Navmesh"},{"location":"ai/AiPlugin/runtime-navmesh/#runtime-navmesh","text":"The AiPlugin adds functionality to generate navmeshes on-demand at runtime. This is a very convenient workflow, as the navmesh incorporates level changes every time you restart the simulation.","title":"Runtime Navmesh"},{"location":"ai/AiPlugin/runtime-navmesh/#input-data","text":"The runtime navmesh generation uses the collision geometry and its surfaces as the input data from which to build the navmesh. The data is queried on-demand in an area around where path searches are done. To see the resulting navmesh, enable the navmesh visualization .","title":"Input Data"},{"location":"ai/AiPlugin/runtime-navmesh/#navigation-configuration","text":"In a scene document select Project > Plugin Settings > Ai Project Settings to open the configuration dialog.","title":"Navigation Configuration"},{"location":"ai/AiPlugin/runtime-navmesh/#ground-types","text":"The Ground Types tab is used to define the types of walkable (or non-walkable) ground that appear in your game. There is a maximum of 32 ground types. In this dialog you can edit their names (double click or F2 on an entry) and select whether an entry should generally be in use (checkmark). Entries that are not checked, will not show up in other places as selectable. The <None> and <Default> ground types always exist and cannot be removed or renamed. Which ground type a physical surface represents, is specified in the surfaces . If a surface uses the <None> ground type, it will not be part of the navmesh, at all, meaning the navmesh will have a hole there. For surfaces that are generally not walkable, by no character, this is preferable over using a dedicated ground type that is then ignored by a path search type .","title":"Ground Types"},{"location":"ai/AiPlugin/runtime-navmesh/#navmesh-types","text":"The Navmesh tab is used to configure how a navmesh is generated. Most games only need a single navmesh type, but if you have agents of very different height or radius, you may need additional ones. Enable the navmesh visualization and utilize the the navmesh path test component to test how these options affect the result.","title":"Navmesh Types"},{"location":"ai/AiPlugin/runtime-navmesh/#navmesh-properties","text":"Collision Layer : The collision layer to use for finding geometry from which to generate the navmesh. This is mainly used to filter out high detail geometry, to speed up navmesh generation. It can also be used to ignore ground types that are generally not traversable (such as water). Cell Size (XY): How detailed the navmesh generation shall be. This has to be less than the desired character radius, and the agent radius should be a whole multiple of this value. For example if your cell size is 0.2 , the effective agent radius can only be 0.2 , 0.4 , 0.6 and so on. Cell Height (Z): The detail along the up axis with which to generate the navmesh. This mainly affects how well steps are detected and dealt with. Similar to the agent radius , the agent step height should be a multiple of the cell height. Agent Radius: How wide the characters are that walk over this type of navmesh. This affects how narrow passages can be, and how far away agents stay from walls. The agent radius should be a multiple of the cell size . Don't use multiple navmeshes only because characters shall have a slightly different radius. Use the same radius for all agents that are more or less the same size. Only use a separate navmesh type, for characters that are extremely different in size and should be forced to take different paths throughout narrow terrain. Agent Height: Similar to agent radius , this defines how tall a character may be. Should be a multiple of cell height . The same caveats as for agent radius apply. Agent Step Height: The maximum height of obstacles (such as stair steps) that an agent is allowed to step up. Should be a multiple of cell height . Agent Walkable Slope: The maximum slope of triangles that are considered for navmesh generation. Triangles steeper than this will be discarded as non-walkable.","title":"Navmesh Properties"},{"location":"ai/AiPlugin/runtime-navmesh/#path-search-types","text":"Each navmesh defines where an agent can walk, and which ground types are in which area. Path search types are used to configure how fast an agent can traverse each ground type and thus which areas the agent would prefer or avoid. Additionally a path search type can be used to prevent an agent from walking on certain ground types, at all. In this tab you can add different path search types, and for each one, you can configure the cost of traversing a ground type and whether it is allowed to be traversed at all. The higher the cost, the more an agent will try to avoid it and use a different route if possible. Traversal costs can't be zero or negative. Use the the navmesh path test component to see how these options affect path finding.","title":"Path Search Types"},{"location":"ai/AiPlugin/runtime-navmesh/#navmesh-visualization","text":"To be able to see the navmesh, use the CVar Ai.Navmesh.Visualize . There can be multiple different navmeshes for different character heights etc. This CVar enables visualization for one of them by index. So set it to 0 to see the first navmesh, 1 if you have a second one and so on. Important: No navmesh will be generated, as long as no path searches are done.","title":"Navmesh Visualization"},{"location":"ai/AiPlugin/runtime-navmesh/#see-also","text":"AiPlugin Overview Navmesh Path Test Component","title":"See Also"},{"location":"animation/common/color-gradients/","text":"Color Gradients Color gradients are used to color things using a 1D lookup. They are typically set up once and shared across many assets, such as particle effects . Modifying a color gradient will affect all resources that use the gradient. Therefore it is advisable to create common gradients early during development. The color gradient can be sampled along the X-axis. This is frequently used to sample a color over time, but other parameters may be used for the lookup as well. If a gradient is supposed to be used in a looping fashion, the first and last color have to be set up to match. Gradient Editor The image below shows a color gradient and its editing controls: The gradient is displayed in four different ways: The first row shows a black and white representation of the alpha channel . The second row shows the color values of the gradient multiplied with the alpha channel, to give an impression how the alpha channel may affect the color perception, when used for transparency. The third row uses the alpha channel to fade between the color values and the background checker pattern, to visualize the effect of the alpha channel in a different way. The fourth row only shows the color channel . Not all gradients use the alpha channel. For those, the first row will be entirely white and the second to fourth row will all appear identical. Editing Keyframes Above the alpha channel and below the color channel you will notice greyscale and colored circles. These are keyframes that define the value at that specific position. All values in between are interpolated. You can grab a keyframe and move it left or right. Double clicking a color keyframe will open a color picker to change its value. When you select a keyframe, its exact position and value can also be changed with the UI elements at the bottom. To create a keyframe double click either the color channel bar or the alpha channel bar (where the respective keyframes are shown as circles). A newly inserted keyframe always gets the interpolated value at that position. To delete a keyframe just select it and press the Del key. View You can scroll left and right using right click and drag . You can zoom in and out using the mouse wheel . The [ Frame ] button will zoom and scroll the view such that all keyframes are framed. Gradient Range Each keyframe has a position along the X axis. Most code that looks up a gradient doesn't actually care about the exact positions. Instead the gradient is looked up in a normalized way, meaning the leftmost keyframe is mapped to position 0 and the rightmost keyframe is mapped to position 1 . For instance, particle effects do all of their lookups this way. However, it is good practice to author the gradients already within this range, unless you have a use case where you indeed want keyframes outside the [0; 1] range. To make this easier, you can use the Adjust X to [0 - 1] button. This will automatically map all existing keyframes to this range for you. The dotted lines indicate which keyframe is the leftmost and which is the rightmost. See Also Curves Property Animation (TODO)","title":"Color Gradients"},{"location":"animation/common/color-gradients/#color-gradients","text":"Color gradients are used to color things using a 1D lookup. They are typically set up once and shared across many assets, such as particle effects . Modifying a color gradient will affect all resources that use the gradient. Therefore it is advisable to create common gradients early during development. The color gradient can be sampled along the X-axis. This is frequently used to sample a color over time, but other parameters may be used for the lookup as well. If a gradient is supposed to be used in a looping fashion, the first and last color have to be set up to match.","title":"Color Gradients"},{"location":"animation/common/color-gradients/#gradient-editor","text":"The image below shows a color gradient and its editing controls: The gradient is displayed in four different ways: The first row shows a black and white representation of the alpha channel . The second row shows the color values of the gradient multiplied with the alpha channel, to give an impression how the alpha channel may affect the color perception, when used for transparency. The third row uses the alpha channel to fade between the color values and the background checker pattern, to visualize the effect of the alpha channel in a different way. The fourth row only shows the color channel . Not all gradients use the alpha channel. For those, the first row will be entirely white and the second to fourth row will all appear identical.","title":"Gradient Editor"},{"location":"animation/common/color-gradients/#editing-keyframes","text":"Above the alpha channel and below the color channel you will notice greyscale and colored circles. These are keyframes that define the value at that specific position. All values in between are interpolated. You can grab a keyframe and move it left or right. Double clicking a color keyframe will open a color picker to change its value. When you select a keyframe, its exact position and value can also be changed with the UI elements at the bottom. To create a keyframe double click either the color channel bar or the alpha channel bar (where the respective keyframes are shown as circles). A newly inserted keyframe always gets the interpolated value at that position. To delete a keyframe just select it and press the Del key.","title":"Editing Keyframes"},{"location":"animation/common/color-gradients/#view","text":"You can scroll left and right using right click and drag . You can zoom in and out using the mouse wheel . The [ Frame ] button will zoom and scroll the view such that all keyframes are framed.","title":"View"},{"location":"animation/common/color-gradients/#gradient-range","text":"Each keyframe has a position along the X axis. Most code that looks up a gradient doesn't actually care about the exact positions. Instead the gradient is looked up in a normalized way, meaning the leftmost keyframe is mapped to position 0 and the rightmost keyframe is mapped to position 1 . For instance, particle effects do all of their lookups this way. However, it is good practice to author the gradients already within this range, unless you have a use case where you indeed want keyframes outside the [0; 1] range. To make this easier, you can use the Adjust X to [0 - 1] button. This will automatically map all existing keyframes to this range for you. The dotted lines indicate which keyframe is the leftmost and which is the rightmost.","title":"Gradient Range"},{"location":"animation/common/color-gradients/#see-also","text":"Curves Property Animation (TODO)","title":"See Also"},{"location":"animation/common/curves/","text":"Curves Curves are used to animate properties. They are typically set up once and shared across many assets, such as particle effects . Modifying a curve will affect all resources that use it. Therefore it is advisable to create common curves early during development. The curve can be sampled along the X-axis. This is frequently used to sample a value over time, but other parameters may be used for the lookup as well. If a curve is supposed to be used in a looping fashion, the first and last control point have to be set up to match. Curve Editor To add a control point, double click where you want to insert one. View Right click and drag to pan the view. Use the mouse wheel to zoom in and out. Hold Shift or Ctrl to zoom the view only along the X axis or the Y axis. Press Ctrl + F or select Frame from the context menu to frame the view either on the selected control points, or the entire curve, when no point is selected. Selection Left click a point to select it. Press ESC to clear any selection. Drag a rectangle to select multiple points. Holding Shift always adds points to the selection. Holding Alt always removes points from the selection. Holding Ctrl toggles the selection of a control point. Distinguishing between add and remove is particularly useful when changing the selection by dragging a rectangle. Left click and drag selected points to move them around. Press Shift after you started dragging , to limit modifications to either left/right or up/down. The axis along which you moved the control points the most, before you pressed Shift , determines which axis gets limited. Drag the handles at the edges of the selection rect, to scale the selected points. Tangents Every control point has two tangents to determine the slope of the curve coming from the left into the point and going out to the right of the point. Each tangent can use one of four modes. Some modes are fully automatic, some allow you to edit the tangents. To not clutter the UI, tangents that can be edited are only shown for selected control points. To change the mode, select a control point, open the context menu and choose from Left/Right/Both Tangents > ... . Auto: The default mode. Automatically configures the tangents to create a smooth curve. Bplier: Gives you full control over the tangents. Both slope and length of the tangents will affect the curve. Fixed Length: Although you can change both slope and length of the tangents, only the slope affects the curve. This is often easier to use than Bplier . Linear: This mode deactivates any curvature. Allows you to make hard corners. Link / Break Tangents By default adjusting one tangent at a control point, mirrors the change over to the other tangent. That's because the two tangents are linked . If you want the tangents to be independent of each other, you can break the link between the tangents. Use the context menu to do so. Looping Curves The rightmost control point determines the overall length of the curve. As you can see, the editor repeats the display of the curve at that point, overlayed with a grey pattern. This enables you to see how the curve would look when used in a looping fashion. To make a curve loop nicely, open the context menu and select Curve > Loop: Adjust First/Last Point . This will modify either the first or the last control point in the curve to match up with the respective other point. Make sure that the two control points use the same tangent mode . You may need to switch to Bplier or Fixed Length tangents to make the curvature match perfectly. Curve Presets In the context menu under Presets you will find a number of presets for commonly used curves. You can also save the current curve as a preset . You can save presets anywhere, but as long as you save it under the path Editor/Presets/Curves (in any data directory ), it will automatically show up in the context menu. You can also use sub-folders to organize presets, as folders will introduce sub-menus. See Also Color Gradients Property Animation (TODO)","title":"Curves"},{"location":"animation/common/curves/#curves","text":"Curves are used to animate properties. They are typically set up once and shared across many assets, such as particle effects . Modifying a curve will affect all resources that use it. Therefore it is advisable to create common curves early during development. The curve can be sampled along the X-axis. This is frequently used to sample a value over time, but other parameters may be used for the lookup as well. If a curve is supposed to be used in a looping fashion, the first and last control point have to be set up to match.","title":"Curves"},{"location":"animation/common/curves/#curve-editor","text":"To add a control point, double click where you want to insert one.","title":"Curve Editor"},{"location":"animation/common/curves/#view","text":"Right click and drag to pan the view. Use the mouse wheel to zoom in and out. Hold Shift or Ctrl to zoom the view only along the X axis or the Y axis. Press Ctrl + F or select Frame from the context menu to frame the view either on the selected control points, or the entire curve, when no point is selected.","title":"View"},{"location":"animation/common/curves/#selection","text":"Left click a point to select it. Press ESC to clear any selection. Drag a rectangle to select multiple points. Holding Shift always adds points to the selection. Holding Alt always removes points from the selection. Holding Ctrl toggles the selection of a control point. Distinguishing between add and remove is particularly useful when changing the selection by dragging a rectangle. Left click and drag selected points to move them around. Press Shift after you started dragging , to limit modifications to either left/right or up/down. The axis along which you moved the control points the most, before you pressed Shift , determines which axis gets limited. Drag the handles at the edges of the selection rect, to scale the selected points.","title":"Selection"},{"location":"animation/common/curves/#tangents","text":"Every control point has two tangents to determine the slope of the curve coming from the left into the point and going out to the right of the point. Each tangent can use one of four modes. Some modes are fully automatic, some allow you to edit the tangents. To not clutter the UI, tangents that can be edited are only shown for selected control points. To change the mode, select a control point, open the context menu and choose from Left/Right/Both Tangents > ... . Auto: The default mode. Automatically configures the tangents to create a smooth curve. Bplier: Gives you full control over the tangents. Both slope and length of the tangents will affect the curve. Fixed Length: Although you can change both slope and length of the tangents, only the slope affects the curve. This is often easier to use than Bplier . Linear: This mode deactivates any curvature. Allows you to make hard corners.","title":"Tangents"},{"location":"animation/common/curves/#link-break-tangents","text":"By default adjusting one tangent at a control point, mirrors the change over to the other tangent. That's because the two tangents are linked . If you want the tangents to be independent of each other, you can break the link between the tangents. Use the context menu to do so.","title":"Link / Break Tangents"},{"location":"animation/common/curves/#looping-curves","text":"The rightmost control point determines the overall length of the curve. As you can see, the editor repeats the display of the curve at that point, overlayed with a grey pattern. This enables you to see how the curve would look when used in a looping fashion. To make a curve loop nicely, open the context menu and select Curve > Loop: Adjust First/Last Point . This will modify either the first or the last control point in the curve to match up with the respective other point. Make sure that the two control points use the same tangent mode . You may need to switch to Bplier or Fixed Length tangents to make the curvature match perfectly.","title":"Looping Curves"},{"location":"animation/common/curves/#curve-presets","text":"In the context menu under Presets you will find a number of presets for commonly used curves. You can also save the current curve as a preset . You can save presets anywhere, but as long as you save it under the path Editor/Presets/Curves (in any data directory ), it will automatically show up in the context menu. You can also use sub-folders to organize presets, as folders will introduce sub-menus.","title":"Curve Presets"},{"location":"animation/common/curves/#see-also","text":"Color Gradients Property Animation (TODO)","title":"See Also"},{"location":"animation/property-animation/color-animation-component/","text":"Color Animation Component The color animation component allows you to apply an animated color gradient to other components, such as meshes or light sources or any other component type that handles the message type plMsgSetColor . Important: This component has no effect on its own. It tries to change the color of other attached components. If no other component has a main color or doesn't handle the message plMsgSetColor , there will be no effect. Note: If an attached component does handle the plMsgSetColor , but doesn't properly update its color dynamically when combined with this component, it may not invalidate its cached render data correctly. A temporary work around is to set the game object's mode to Force Dynamic . Component Properties Gradient : The color gradient to use. The gradient will be sampled from left to right over Duration seconds. Each time the sampled color is put into an plMsgSetColor and that message is sent to all other components that are attached to the same object. Duration : The duration that the color gradient should last before it is being repeated. SetColorMode : The mode with which to modify the color of the affect object. Although the color can be blended into the target object's color, for many components this quickly results in a fully black or fully white result, as the modifications accumulate with each change. AnimationMode : How to continue sampling the color gradient, once the end has been reached. RandomStartOffset : If enabled, the component starts with a random time offset. This way prevents synchronous playback, if multiple objects use the same animation. ApplyToChildren : Whether to send the plMsgSetColor only to components on the same game object, or also to all components in the entire sub-graph. This can be used to modify the color of many objects in sync. See Also Property Animation (TODO) Color Gradients","title":"Color Animation Component"},{"location":"animation/property-animation/color-animation-component/#color-animation-component","text":"The color animation component allows you to apply an animated color gradient to other components, such as meshes or light sources or any other component type that handles the message type plMsgSetColor . Important: This component has no effect on its own. It tries to change the color of other attached components. If no other component has a main color or doesn't handle the message plMsgSetColor , there will be no effect. Note: If an attached component does handle the plMsgSetColor , but doesn't properly update its color dynamically when combined with this component, it may not invalidate its cached render data correctly. A temporary work around is to set the game object's mode to Force Dynamic .","title":"Color Animation Component"},{"location":"animation/property-animation/color-animation-component/#component-properties","text":"Gradient : The color gradient to use. The gradient will be sampled from left to right over Duration seconds. Each time the sampled color is put into an plMsgSetColor and that message is sent to all other components that are attached to the same object. Duration : The duration that the color gradient should last before it is being repeated. SetColorMode : The mode with which to modify the color of the affect object. Although the color can be blended into the target object's color, for many components this quickly results in a fully black or fully white result, as the modifications accumulate with each change. AnimationMode : How to continue sampling the color gradient, once the end has been reached. RandomStartOffset : If enabled, the component starts with a random time offset. This way prevents synchronous playback, if multiple objects use the same animation. ApplyToChildren : Whether to send the plMsgSetColor only to components on the same game object, or also to all components in the entire sub-graph. This can be used to modify the color of many objects in sync.","title":"Component Properties"},{"location":"animation/property-animation/color-animation-component/#see-also","text":"Property Animation (TODO) Color Gradients","title":"See Also"},{"location":"animation/property-animation/move-to-component/","text":"MoveTo Component The state of this component is currently unclear and it needs to be revisited. See Also","title":"MoveTo Component"},{"location":"animation/property-animation/move-to-component/#moveto-component","text":"The state of this component is currently unclear and it needs to be revisited.","title":"MoveTo Component"},{"location":"animation/property-animation/move-to-component/#see-also","text":"","title":"See Also"},{"location":"animation/property-animation/property-animation-asset/","text":"Property Animation Asset Property animations are fully functional (except for some known limitations), but undocumented. See Also Property Animations (TODO) Property Animation Component (TODO)","title":"Property Animation Asset"},{"location":"animation/property-animation/property-animation-asset/#property-animation-asset","text":"Property animations are fully functional (except for some known limitations), but undocumented.","title":"Property Animation Asset"},{"location":"animation/property-animation/property-animation-asset/#see-also","text":"Property Animations (TODO) Property Animation Component (TODO)","title":"See Also"},{"location":"animation/property-animation/property-animation-component/","text":"Property Animation Component Property animations are fully functional (except for some known limitations), but undocumented. See Also Property Animations (TODO) Property Animation Asset (TODO)","title":"Property Animation Component"},{"location":"animation/property-animation/property-animation-component/#property-animation-component","text":"Property animations are fully functional (except for some known limitations), but undocumented.","title":"Property Animation Component"},{"location":"animation/property-animation/property-animation-component/#see-also","text":"Property Animations (TODO) Property Animation Asset (TODO)","title":"See Also"},{"location":"animation/property-animation/property-animation-overview/","text":"Property Animations Property animations are fully functional (except for some known limitations), but undocumented. See Also Property Animation Asset (TODO) Property Animation Component (TODO)","title":"Property Animations"},{"location":"animation/property-animation/property-animation-overview/#property-animations","text":"Property animations are fully functional (except for some known limitations), but undocumented.","title":"Property Animations"},{"location":"animation/property-animation/property-animation-overview/#see-also","text":"Property Animation Asset (TODO) Property Animation Component (TODO)","title":"See Also"},{"location":"animation/property-animation/rotor-component/","text":"Rotor Component The rotor component is a very useful utility component to rotate an object around an axis. This can be used for simple animations, such as having an itme in the world spin around itself. Component Properties Speed : The maximum speed at which the rotor will turn. Running : Whether the rotor will move right from the start. If this is disabled, external code needs to switch the state to on, for the rotor to do anything. ReverseAtEnd, ReverseAtStart : Whether the rotor should automatically reverse its direction when it reaches the end, and when it reaches the start point again. If both are enabled, the rotor will turn back and forth indefinitely. Otherwise it will stop either at the end, or at the start. In both cases the Running state is reset to false , and the rotor can be restarted by setting the Running state to true again. Axis : The local axis around which the rotor should turn. AxisDeviation : A random deviation to apply to the Axis . This allows you to place multiple copies of the same object next to each other, and have them all rotate slightly differently. DegreesToRotate : How far to turn before the rotor needs to stop or reverse direction. This is not limited to 360 degrees, it may represent multiple revolutions. Use 0 , if the rotor should just rotate continuously in one direction. In that case acceleration and decelaration have no effect. Acceleration, Deceleration : How fast to speed up and slow down. Use zero if the rotor should reverse course instantly. See Also Slider Component Property Animation (TODO)","title":"Rotor Component"},{"location":"animation/property-animation/rotor-component/#rotor-component","text":"The rotor component is a very useful utility component to rotate an object around an axis. This can be used for simple animations, such as having an itme in the world spin around itself.","title":"Rotor Component"},{"location":"animation/property-animation/rotor-component/#component-properties","text":"Speed : The maximum speed at which the rotor will turn. Running : Whether the rotor will move right from the start. If this is disabled, external code needs to switch the state to on, for the rotor to do anything. ReverseAtEnd, ReverseAtStart : Whether the rotor should automatically reverse its direction when it reaches the end, and when it reaches the start point again. If both are enabled, the rotor will turn back and forth indefinitely. Otherwise it will stop either at the end, or at the start. In both cases the Running state is reset to false , and the rotor can be restarted by setting the Running state to true again. Axis : The local axis around which the rotor should turn. AxisDeviation : A random deviation to apply to the Axis . This allows you to place multiple copies of the same object next to each other, and have them all rotate slightly differently. DegreesToRotate : How far to turn before the rotor needs to stop or reverse direction. This is not limited to 360 degrees, it may represent multiple revolutions. Use 0 , if the rotor should just rotate continuously in one direction. In that case acceleration and decelaration have no effect. Acceleration, Deceleration : How fast to speed up and slow down. Use zero if the rotor should reverse course instantly.","title":"Component Properties"},{"location":"animation/property-animation/rotor-component/#see-also","text":"Slider Component Property Animation (TODO)","title":"See Also"},{"location":"animation/property-animation/slider-component/","text":"Slider Component The slider component is a very useful utility component to move an object back and forth along a single axis. This can be used for simple animations, such as having a pickup item bounce up and down, or for simple buttons or sliding doors. Component Properties Speed : The maximum speed at which the slider will move. Running : Whether the slider will move right from the start. If this is disabled, external code needs to switch the state to on, for the slider to do anything. ReverseAtEnd, ReverseAtStart : Whether the slider should automatically reverse its direction when it reaches the end, and when it reaches the start point again. If both are enabled, the slider will go back and forth indefinitely. Otherwise it will stop either at the end, or at the start point. In both cases the Running state is reset to false , and the slider can be restarted by setting the Running state to true again. Axis : The local axis along which the slider should move. Distance : The distance that the slider should travel before stopping or returning. Acceleration, Deceleration : How fast to speed up and slow down. Use zero if the slider should reverse course instantly. RandomStart : If set to zero, the slider will always start from the beginning. Otherwise a random time offset between zero and RandomStart is used to pre-simulate the slider position. This is useful if you have multiple moving objects next to each other and you don't want them all to move in perfect unison. If you just want any random start position, just pick a very large RandomStart value. Scripting SetDirectionForwards : Allows to force the direction of the slider to either forwards or backwards along its axis. IsDirectionForwards : Queries whether the slider is currently moving forwards or backwards. ToggleDirection : Toggles the current direction of the slider. See Also Rotor Component Property Animation (TODO) MoveTo Component (TODO)","title":"Slider Component"},{"location":"animation/property-animation/slider-component/#slider-component","text":"The slider component is a very useful utility component to move an object back and forth along a single axis. This can be used for simple animations, such as having a pickup item bounce up and down, or for simple buttons or sliding doors.","title":"Slider Component"},{"location":"animation/property-animation/slider-component/#component-properties","text":"Speed : The maximum speed at which the slider will move. Running : Whether the slider will move right from the start. If this is disabled, external code needs to switch the state to on, for the slider to do anything. ReverseAtEnd, ReverseAtStart : Whether the slider should automatically reverse its direction when it reaches the end, and when it reaches the start point again. If both are enabled, the slider will go back and forth indefinitely. Otherwise it will stop either at the end, or at the start point. In both cases the Running state is reset to false , and the slider can be restarted by setting the Running state to true again. Axis : The local axis along which the slider should move. Distance : The distance that the slider should travel before stopping or returning. Acceleration, Deceleration : How fast to speed up and slow down. Use zero if the slider should reverse course instantly. RandomStart : If set to zero, the slider will always start from the beginning. Otherwise a random time offset between zero and RandomStart is used to pre-simulate the slider position. This is useful if you have multiple moving objects next to each other and you don't want them all to move in perfect unison. If you just want any random start position, just pick a very large RandomStart value.","title":"Component Properties"},{"location":"animation/property-animation/slider-component/#scripting","text":"SetDirectionForwards : Allows to force the direction of the slider to either forwards or backwards along its axis. IsDirectionForwards : Queries whether the slider is currently moving forwards or backwards. ToggleDirection : Toggles the current direction of the slider.","title":"Scripting"},{"location":"animation/property-animation/slider-component/#see-also","text":"Rotor Component Property Animation (TODO) MoveTo Component (TODO)","title":"See Also"},{"location":"animation/skeletal-animation/animated-mesh-asset/","text":"Animated Mesh Asset The animated mesh asset is very similar to the mesh asset . However, it adds the necessary data to a mesh such that it can be used for skeletal animation . Animated meshes are placed in a scene with a dedicated animated mesh component . Which animations are played on it can be controlled with a simple animation component or an animation controller component . Asset Properties MeshFile : The file that contains the mesh data. For animated meshes prefer to use GLB (binary GLTF) files. FBX files can be used as well, though due to FBX's complexity chances are higher that it won't work as expected. The referenced file must contain the mesh data with skinning information. It doesn't need to contain any animation clips. DefaultSkeleton : The skeleton asset that is used to skin the animated mesh by default. RecalculateNormals , RecalculateTangents : See the mesh asset properties . NormalPrecision , TexCoordPrecision : See the mesh asset properties . ImportMaterials : See the mesh asset properties . Materials : See the mesh asset properties . See Also Animated Mesh Component Skeleton Asset Skeletal Animations Animation Clip Asset Simple Animation Component","title":"Animated Mesh Asset"},{"location":"animation/skeletal-animation/animated-mesh-asset/#animated-mesh-asset","text":"The animated mesh asset is very similar to the mesh asset . However, it adds the necessary data to a mesh such that it can be used for skeletal animation . Animated meshes are placed in a scene with a dedicated animated mesh component . Which animations are played on it can be controlled with a simple animation component or an animation controller component .","title":"Animated Mesh Asset"},{"location":"animation/skeletal-animation/animated-mesh-asset/#asset-properties","text":"MeshFile : The file that contains the mesh data. For animated meshes prefer to use GLB (binary GLTF) files. FBX files can be used as well, though due to FBX's complexity chances are higher that it won't work as expected. The referenced file must contain the mesh data with skinning information. It doesn't need to contain any animation clips. DefaultSkeleton : The skeleton asset that is used to skin the animated mesh by default. RecalculateNormals , RecalculateTangents : See the mesh asset properties . NormalPrecision , TexCoordPrecision : See the mesh asset properties . ImportMaterials : See the mesh asset properties . Materials : See the mesh asset properties .","title":"Asset Properties"},{"location":"animation/skeletal-animation/animated-mesh-asset/#see-also","text":"Animated Mesh Component Skeleton Asset Skeletal Animations Animation Clip Asset Simple Animation Component","title":"See Also"},{"location":"animation/skeletal-animation/animated-mesh-component/","text":"Animated Mesh Component An animated mesh component is used to instantiate an animated mesh asset . Animated mesh components are mostly identical to regular mesh components except that they can only be used with animated mesh assets . An animated mesh will be skinned with its current animation pose. Which pose is applied to an animated mesh can be controlled with a simple animation component or an animation controller component . Component Properties Mesh : The animate mesh asset to render. Color : See mesh component . Materials : See mesh component . See Also Meshes Skeletal Animations Simple Animation Component Animated Mesh Asset","title":"Animated Mesh Component"},{"location":"animation/skeletal-animation/animated-mesh-component/#animated-mesh-component","text":"An animated mesh component is used to instantiate an animated mesh asset . Animated mesh components are mostly identical to regular mesh components except that they can only be used with animated mesh assets . An animated mesh will be skinned with its current animation pose. Which pose is applied to an animated mesh can be controlled with a simple animation component or an animation controller component .","title":"Animated Mesh Component"},{"location":"animation/skeletal-animation/animated-mesh-component/#component-properties","text":"Mesh : The animate mesh asset to render. Color : See mesh component . Materials : See mesh component .","title":"Component Properties"},{"location":"animation/skeletal-animation/animated-mesh-component/#see-also","text":"Meshes Skeletal Animations Simple Animation Component Animated Mesh Asset","title":"See Also"},{"location":"animation/skeletal-animation/animation-clip-asset/","text":"Animation Clip Asset The animation clip asset is used to import a single animation for an animated mesh . ![[anim-clip.gif]] An animation clip represents a single motion, such as a walk cycle, a jump or other action. Simple animations can be played on a mesh using a simple animation component . For complex behavior you will need to use multiple clips and fade from one to the other at the right times. Use an animation controller for that. Asset Properties File : The file from which to import the animation clip. UseAnimationClip : The (case sensitive) name of the animation clip to import from the file. Transform the asset once to populate the list of AvailableClips . Then type the name of the desired clip into this field and transform the asset again. If a clip doesn't show up in the list, make sure it is correctly exported. See the chapter Authoring and Exporting Animations with Blender for known issues. AvailableClips : After you transform the asset, this list will show all the animation clips that have been found in the given file. FirstFrame , NumFrames : It is best to put every animation into a separate clip and export them that way. However, sometimes files contain only a single animation and each clip is found at another interval. By specifying the index of the first frame and the number of frames to use, you can extract individual clips from such data. Note that setting NumFrames to zero always means to use all the remaining frames after the first frame. Note: It can be difficult to know the exact indices. Sometimes the data is authored at 24 frame per second and also exported that way, then you can plug in the numbers straight away. However, GLTF/GLB files are always exported at 1000 FPS. That means if your animation clip was authored with 24 FPS and starts at the one second mark, in the GLB file this wouldn't be at keyframe 24, but at keyframe 1000. PreviewMesh : The animated mesh to use for previewing this animation clip. This has to be set to see any preview. RootMotion : If the animation clip should be able to move the game object , this can be achieved through root motion . This option allows you to select how root motion should be incorporated into the animation clip. For the time being the only mode available is constant motion , which means that when this clip is played, the parent object will be moved at a constant speed into a single direction. This can be used for walking animations, but it might be tricky to avoid foot sliding . Playback The toolbar buttons allow you to play/pause/reset and slow-down the animation playback. Additionally you can use the time scrubber right below the 3D viewport to manually play the animation. It is best to pause the automatic playback then. Event Track Below the time scrubber there is an additional strip to edit animation events . Here you can add events that shall occur at specific times during the animation clip playback, such as foot-down or fire-weapon . Use the time scrubber above to play the clip and inspect at which time the event shall occur. Then right click into the event track and select Add Event . Which type of event will be added is specified with the combo box at the bottom right. See Also Skeletal Animations","title":"Animation Clip Asset"},{"location":"animation/skeletal-animation/animation-clip-asset/#animation-clip-asset","text":"The animation clip asset is used to import a single animation for an animated mesh . ![[anim-clip.gif]] An animation clip represents a single motion, such as a walk cycle, a jump or other action. Simple animations can be played on a mesh using a simple animation component . For complex behavior you will need to use multiple clips and fade from one to the other at the right times. Use an animation controller for that.","title":"Animation Clip Asset"},{"location":"animation/skeletal-animation/animation-clip-asset/#asset-properties","text":"File : The file from which to import the animation clip. UseAnimationClip : The (case sensitive) name of the animation clip to import from the file. Transform the asset once to populate the list of AvailableClips . Then type the name of the desired clip into this field and transform the asset again. If a clip doesn't show up in the list, make sure it is correctly exported. See the chapter Authoring and Exporting Animations with Blender for known issues. AvailableClips : After you transform the asset, this list will show all the animation clips that have been found in the given file. FirstFrame , NumFrames : It is best to put every animation into a separate clip and export them that way. However, sometimes files contain only a single animation and each clip is found at another interval. By specifying the index of the first frame and the number of frames to use, you can extract individual clips from such data. Note that setting NumFrames to zero always means to use all the remaining frames after the first frame. Note: It can be difficult to know the exact indices. Sometimes the data is authored at 24 frame per second and also exported that way, then you can plug in the numbers straight away. However, GLTF/GLB files are always exported at 1000 FPS. That means if your animation clip was authored with 24 FPS and starts at the one second mark, in the GLB file this wouldn't be at keyframe 24, but at keyframe 1000. PreviewMesh : The animated mesh to use for previewing this animation clip. This has to be set to see any preview. RootMotion : If the animation clip should be able to move the game object , this can be achieved through root motion . This option allows you to select how root motion should be incorporated into the animation clip. For the time being the only mode available is constant motion , which means that when this clip is played, the parent object will be moved at a constant speed into a single direction. This can be used for walking animations, but it might be tricky to avoid foot sliding .","title":"Asset Properties"},{"location":"animation/skeletal-animation/animation-clip-asset/#playback","text":"The toolbar buttons allow you to play/pause/reset and slow-down the animation playback. Additionally you can use the time scrubber right below the 3D viewport to manually play the animation. It is best to pause the automatic playback then.","title":"Playback"},{"location":"animation/skeletal-animation/animation-clip-asset/#event-track","text":"Below the time scrubber there is an additional strip to edit animation events . Here you can add events that shall occur at specific times during the animation clip playback, such as foot-down or fire-weapon . Use the time scrubber above to play the clip and inspect at which time the event shall occur. Then right click into the event track and select Add Event . Which type of event will be added is specified with the combo box at the bottom right.","title":"Event Track"},{"location":"animation/skeletal-animation/animation-clip-asset/#see-also","text":"Skeletal Animations","title":"See Also"},{"location":"animation/skeletal-animation/animation-events/","text":"Animation Events Animation events are markers that are placed in animation clip assets to indicate that something of interest happens at a certain time during playback. These events can be used for mundane things like foot down markers, in a walking animation, to vital gameplay information like shot fired in an attack animation. Whenver the Plasma animation system plays back any animation clip, it also inspects all the event track . For every event that it encounters, it broadcasts an plMsgGenericEvent with Message set to the value of the event's name. If you have a TypeScript or Visual Script (or custom C++ components that is also an event handler) attached to any parent node of the animated mesh, you can handle this type of event and react with the desired game logic. ![[anim-clip.gif]] The clip above shows the event track at the bottom of an animation clip asset . See Also Skeletal Animations Event Nodes Custom Code Messaging","title":"Animation Events"},{"location":"animation/skeletal-animation/animation-events/#animation-events","text":"Animation events are markers that are placed in animation clip assets to indicate that something of interest happens at a certain time during playback. These events can be used for mundane things like foot down markers, in a walking animation, to vital gameplay information like shot fired in an attack animation. Whenver the Plasma animation system plays back any animation clip, it also inspects all the event track . For every event that it encounters, it broadcasts an plMsgGenericEvent with Message set to the value of the event's name. If you have a TypeScript or Visual Script (or custom C++ components that is also an event handler) attached to any parent node of the animated mesh, you can handle this type of event and react with the desired game logic. ![[anim-clip.gif]] The clip above shows the event track at the bottom of an animation clip asset .","title":"Animation Events"},{"location":"animation/skeletal-animation/animation-events/#see-also","text":"Skeletal Animations Event Nodes Custom Code Messaging","title":"See Also"},{"location":"animation/skeletal-animation/blender-export/","text":"Authoring and Exporting Animations with Blender This page contains various pieces of information that are good to know when one uses Blender to build and export animated meshes. It is assumed that you know Blender well enough to create animated meshes. Exporting Animated Meshes To get animated meshes out of Blender and into Plasma Engine, export the animated mesh to a binary GLTF file ( .glb ). You can enable +Y up or not. In both cases you need to adjust the transformation on the skeleton asset . Make sure that the GLTF export contains Animations and Skinning information. Don't disable animation sampling on export. Be aware that GLTF uses 1000 frames per second for all exported animation clips. Blender, by default, uses 24 frames per second. If you want to only use a sub-range of an animation in Plasma, you will need to re-calculate the frame indices accordingly. You can set Blender to use 25 or 50 frames per second to make this calculation easier. Enable Export Deformation Bones Only to strip IK pole targets and other unnecessary bones from the file. Importing Meshes into Plamsa When importing a mesh, Plasma remaps the model space to its own convention. You may need to change the mapping, to get the desired result. For static meshes, this is configure on the mesh asset . For animated models, the mapping is chosen on the root node of the corresponding skeleton asset . Plasma uses the following convention: +X is the forward axis +Y is the right axis +Z is the up axis By default all code uses +X as its main direction. For example AI nodes move characters forwards along the +X axis, spot lights and cameras \"look\" into the +X direction and so on. In Blender it is common to have a character look along the -Y axis so that it faces the user when pressing Numpad 1 . This also means that the right side of the character will be along the -X axis. If you export such a mesh to a GLB and enable Y UP convention, you need to configure the mapping this way: Set Right Dir to Negative X Set Up Dir to Positive Y Set FlipForwardDir to off Authoring Meshes Make sure all triangles face into the same direction. Use Blender's Face Orientation viewport option to see whether there are flipped triangles. If there are flipped triangles, they will show up incorrectly in Plasma. Authoring Animations Plasma only supports skeletal animations via skinned meshes. That means every vertex in the mesh needs to have a bone assigned via vertex weights. Blender can move entire objects through bone animations, but if they are only parented to a bone, and don't use vertex skinning (vertex weights),Plasma will not show those objects as animated. Use the Vertex Group Weights visualization in Blender to inspect which vertices are set up properly and which aren't. Plasma does not support scaling of bones. All bones must have scaling values of 1. If you have an object scaled in object mode and attached to a bone, the scaling will be represented by the bone, so even if your animation keyframes do not use scaling, the exported animation track does. To fix this, select your scaled object and use Mesh > Apply > Scaling to bake the object scaling into the vertex positions and get rid of the scaling in the bone transforms. Plasma uses a maximum of 4 bones per vertex. By default Blender's GLTF export already restricts vertex weights to 4 bones, though there is an option to allow more influences. This won't have a positive effect in Plasma though. Be aware that Blender exports ALL keyframes of an animation. The preview window of an animation has no effect on the exported animation data. Blender always sets the first keyframe of all animations to index 1 and that is also how the data is exported. Plasma expects the first keyframe to be at index 0 , though. So set the animation range in Blender to start at index 0 and put the first keyframe there. Use the Action Editor in Blender to create and manage multiple animations in a single file. Be sure to set the Fake User flag on all of them to not lose any work. Now it gets weird: If you add multiple animations in Blender, usually 4 to 6 of them will be exported in the GLTF file. Once you add more animations, seemingly random ones won't be exported. This is because Blender thinks those animations are unused and won't export such unreferenced animations. The fix is to fake reference every animation in Blenders NLA editor. The way to do this, is to push the Stash button in the action editor on every animation. You will then see this reference show up in the scene outline/hierarchy window. Don't rename the stashed item! Every animation should be stashed only once, and if you keep the auto-generated name, Blender won't create another stash reference, if you push the stash button multiple times on the same animation. If you rename the item, that won't work anymore. To delete an animation that has been stashed (and thus referenced by the NLA editor), remove the Fake User flag and also open the NLA editor and delete any reference to the animation by pointing with the mouse on a track and pressing X or Del . Once no reference exists anymore, Blender removes the animation when you save and close the program. Animation Cycles To create an animation that can be repeated, such as walk cycles, the first and the last keyframe must be identical. Furthermore, Blender will typically use cubic interpolation between the keyframes. For the first and last keyframe this will result in an interpolation that slows down and speeds up and is therefore not smooth. The simplest solution is to set these (or all) keyframes to use linear interpolation instead. Another option is to insert duplicated dummy keyframes before the first and after the last keyframe, to force the desired interpolation, but then you need to configure the animation clip in Plasma to only use the proper sub-range of keyframes, which can be tricky to figure out. Rigging Meshes There are many good tutorials how to rig meshes. However, here are some additional tips, some that are specific to using animated meshes in game engines. Make sure your mesh is rigged perfectly before you start animating. Even small adjustments to the rig later may require you to redo all your animations. Make sure that all bones are connected correctly to each other. For example, hand and foot bones MUST be connected to their respective arm and leg bones. You must not have any disconnected bones that would be connected in a real skeleton. Some tutorials suggest to disconnect hand and foot bones and use a copy transform constraint instead, when setting up IK in Blender. This is a really bad practice. It will appear to work at first, but once you use partial animation blending (for example to play an animation only on the upper or lower body), it won't work, because the disconnected bones are not part of the correct hierarchy. Similarly, setting up rag dolls for physics requires the bone hierarchy to be correct. Other engines have the same requirement. If you want to add IK to your Blender rig, duplicate the desired bone (for instance the hand bone), then disconnect that bone from the hierarchy (making it a root bone), disable Deformation on that bone, and then use that as the IK target bone. Since Deformation is disabled, this bone won't be exported to GLTF either, which is what you want. It will only be a control bone. If you want your actual hand bone to fully follow your IK target bone, add a Copy Rotation Constraint to it to have it follow both position and rotation animations that you add to the IK target bone. You may also want to hide the original hand bone, such that you don't accidentally pick and animate it, when instead you want to animate the IK target bone (which will most of the time be in the exact same location). Be aware though, that once a bone is hidden, it is quite a pain to make any modifications to it, because Blender won't allow you to select it anymore, not even from the outliner tree view. Instead you must unhide the bone first . Either unhide everything (using ALT+H ) or unhide only the desired bone through its context menu in the outliner pane. For IK bones, make sure your pole targets really work correctly. Most tutorials mention that you need to use a rotation offset of +90, -90 or 180 degree, but I have also observed the need for 45 degrees (and consequently 135 degrees) etc. The best way to check is to toggle between Edit Mode and Pose Mode (with the rest pose active) and check that bones with IK don't have extreme twist in pose mode. The bones should only slightly move to fulfill their IK configuration, but if for example arm bones twist by a large amount, then your pole target configuration isn't correct. If you seem to not get the pole target configuration correct, first make sure the target joint has a slight bend (for instance an elbow shouldn't be fully straight). Then remove the IK constraint on the bone entirely and set it up from scratch. Blender seems to have internal state that can't be fixed differently. See Also Skeletal Animations","title":"Authoring and Exporting Animations with Blender"},{"location":"animation/skeletal-animation/blender-export/#authoring-and-exporting-animations-with-blender","text":"This page contains various pieces of information that are good to know when one uses Blender to build and export animated meshes. It is assumed that you know Blender well enough to create animated meshes.","title":"Authoring and Exporting Animations with Blender"},{"location":"animation/skeletal-animation/blender-export/#exporting-animated-meshes","text":"To get animated meshes out of Blender and into Plasma Engine, export the animated mesh to a binary GLTF file ( .glb ). You can enable +Y up or not. In both cases you need to adjust the transformation on the skeleton asset . Make sure that the GLTF export contains Animations and Skinning information. Don't disable animation sampling on export. Be aware that GLTF uses 1000 frames per second for all exported animation clips. Blender, by default, uses 24 frames per second. If you want to only use a sub-range of an animation in Plasma, you will need to re-calculate the frame indices accordingly. You can set Blender to use 25 or 50 frames per second to make this calculation easier. Enable Export Deformation Bones Only to strip IK pole targets and other unnecessary bones from the file.","title":"Exporting Animated Meshes"},{"location":"animation/skeletal-animation/blender-export/#importing-meshes-into-plamsa","text":"When importing a mesh, Plasma remaps the model space to its own convention. You may need to change the mapping, to get the desired result. For static meshes, this is configure on the mesh asset . For animated models, the mapping is chosen on the root node of the corresponding skeleton asset . Plasma uses the following convention: +X is the forward axis +Y is the right axis +Z is the up axis By default all code uses +X as its main direction. For example AI nodes move characters forwards along the +X axis, spot lights and cameras \"look\" into the +X direction and so on. In Blender it is common to have a character look along the -Y axis so that it faces the user when pressing Numpad 1 . This also means that the right side of the character will be along the -X axis. If you export such a mesh to a GLB and enable Y UP convention, you need to configure the mapping this way: Set Right Dir to Negative X Set Up Dir to Positive Y Set FlipForwardDir to off","title":"Importing Meshes into Plamsa"},{"location":"animation/skeletal-animation/blender-export/#authoring-meshes","text":"Make sure all triangles face into the same direction. Use Blender's Face Orientation viewport option to see whether there are flipped triangles. If there are flipped triangles, they will show up incorrectly in Plasma.","title":"Authoring Meshes"},{"location":"animation/skeletal-animation/blender-export/#authoring-animations","text":"Plasma only supports skeletal animations via skinned meshes. That means every vertex in the mesh needs to have a bone assigned via vertex weights. Blender can move entire objects through bone animations, but if they are only parented to a bone, and don't use vertex skinning (vertex weights),Plasma will not show those objects as animated. Use the Vertex Group Weights visualization in Blender to inspect which vertices are set up properly and which aren't. Plasma does not support scaling of bones. All bones must have scaling values of 1. If you have an object scaled in object mode and attached to a bone, the scaling will be represented by the bone, so even if your animation keyframes do not use scaling, the exported animation track does. To fix this, select your scaled object and use Mesh > Apply > Scaling to bake the object scaling into the vertex positions and get rid of the scaling in the bone transforms. Plasma uses a maximum of 4 bones per vertex. By default Blender's GLTF export already restricts vertex weights to 4 bones, though there is an option to allow more influences. This won't have a positive effect in Plasma though. Be aware that Blender exports ALL keyframes of an animation. The preview window of an animation has no effect on the exported animation data. Blender always sets the first keyframe of all animations to index 1 and that is also how the data is exported. Plasma expects the first keyframe to be at index 0 , though. So set the animation range in Blender to start at index 0 and put the first keyframe there. Use the Action Editor in Blender to create and manage multiple animations in a single file. Be sure to set the Fake User flag on all of them to not lose any work. Now it gets weird: If you add multiple animations in Blender, usually 4 to 6 of them will be exported in the GLTF file. Once you add more animations, seemingly random ones won't be exported. This is because Blender thinks those animations are unused and won't export such unreferenced animations. The fix is to fake reference every animation in Blenders NLA editor. The way to do this, is to push the Stash button in the action editor on every animation. You will then see this reference show up in the scene outline/hierarchy window. Don't rename the stashed item! Every animation should be stashed only once, and if you keep the auto-generated name, Blender won't create another stash reference, if you push the stash button multiple times on the same animation. If you rename the item, that won't work anymore. To delete an animation that has been stashed (and thus referenced by the NLA editor), remove the Fake User flag and also open the NLA editor and delete any reference to the animation by pointing with the mouse on a track and pressing X or Del . Once no reference exists anymore, Blender removes the animation when you save and close the program.","title":"Authoring Animations"},{"location":"animation/skeletal-animation/blender-export/#animation-cycles","text":"To create an animation that can be repeated, such as walk cycles, the first and the last keyframe must be identical. Furthermore, Blender will typically use cubic interpolation between the keyframes. For the first and last keyframe this will result in an interpolation that slows down and speeds up and is therefore not smooth. The simplest solution is to set these (or all) keyframes to use linear interpolation instead. Another option is to insert duplicated dummy keyframes before the first and after the last keyframe, to force the desired interpolation, but then you need to configure the animation clip in Plasma to only use the proper sub-range of keyframes, which can be tricky to figure out.","title":"Animation Cycles"},{"location":"animation/skeletal-animation/blender-export/#rigging-meshes","text":"There are many good tutorials how to rig meshes. However, here are some additional tips, some that are specific to using animated meshes in game engines. Make sure your mesh is rigged perfectly before you start animating. Even small adjustments to the rig later may require you to redo all your animations. Make sure that all bones are connected correctly to each other. For example, hand and foot bones MUST be connected to their respective arm and leg bones. You must not have any disconnected bones that would be connected in a real skeleton. Some tutorials suggest to disconnect hand and foot bones and use a copy transform constraint instead, when setting up IK in Blender. This is a really bad practice. It will appear to work at first, but once you use partial animation blending (for example to play an animation only on the upper or lower body), it won't work, because the disconnected bones are not part of the correct hierarchy. Similarly, setting up rag dolls for physics requires the bone hierarchy to be correct. Other engines have the same requirement. If you want to add IK to your Blender rig, duplicate the desired bone (for instance the hand bone), then disconnect that bone from the hierarchy (making it a root bone), disable Deformation on that bone, and then use that as the IK target bone. Since Deformation is disabled, this bone won't be exported to GLTF either, which is what you want. It will only be a control bone. If you want your actual hand bone to fully follow your IK target bone, add a Copy Rotation Constraint to it to have it follow both position and rotation animations that you add to the IK target bone. You may also want to hide the original hand bone, such that you don't accidentally pick and animate it, when instead you want to animate the IK target bone (which will most of the time be in the exact same location). Be aware though, that once a bone is hidden, it is quite a pain to make any modifications to it, because Blender won't allow you to select it anymore, not even from the outliner tree view. Instead you must unhide the bone first . Either unhide everything (using ALT+H ) or unhide only the desired bone through its context menu in the outliner pane. For IK bones, make sure your pole targets really work correctly. Most tutorials mention that you need to use a rotation offset of +90, -90 or 180 degree, but I have also observed the need for 45 degrees (and consequently 135 degrees) etc. The best way to check is to toggle between Edit Mode and Pose Mode (with the rest pose active) and check that bones with IK don't have extreme twist in pose mode. The bones should only slightly move to fulfill their IK configuration, but if for example arm bones twist by a large amount, then your pole target configuration isn't correct. If you seem to not get the pole target configuration correct, first make sure the target joint has a slight bend (for instance an elbow shouldn't be fully straight). Then remove the IK constraint on the bone entirely and set it up from scratch. Blender seems to have internal state that can't be fixed differently.","title":"Rigging Meshes"},{"location":"animation/skeletal-animation/blender-export/#see-also","text":"Skeletal Animations","title":"See Also"},{"location":"animation/skeletal-animation/joint-attachment-component/","text":"Joint Attachment Component The joint attachment component is used to expose the animated position of a bone, such that you can attach objects there. Component Properties JointName : The name of the joint/bone of which you want to use the position as an attachment point. You can look up the bone names in the respective skeleton asset . PositionOffset , RotationOffset : Additional local position and rotation offsets added to the bone location. The same could be achieved by adding another child game object with an offset, but using these properties is more efficient. How To Use Whenever an animated mesh receives a new pose, it passes that pose on to all interested components that are attached to the same object or any child object. The joint attachment component listens to this message and positions its owner game object at the relative position of the selected bone. To attach an object to a certain bone, follow these steps: Create an empty game object as a child of the animated mesh. Add a joint attachment component to it. Set its JointName property to the desired bone name. You can look up the bone name on the skeleton asset that is used by the animated mesh asset on the animated mesh component . Add the desired object or component to the joint attachment object. The local transform of the attachment object will be overwritten by the component when it receives an animation pose. Thus setting any values here doesn't have any useful effect during simulation. To see where your attachment ends up, you need to simulate the scene and an animation has to actively play on the animated mesh. While the scene is simulating, you can use the position and rotation offset properties to tweak the exact location of the joint attachment. Note: The position and rotation offset properties are useful for minor tweaks. The same can also be achieved with another child object. However, it can be difficult to position an attachment perfectly and it might turn out that the attachment position needs tweaking depending on the animation as well. In such cases it is better to add a dedicated bone to the skeleton instead, such that you have full control over the attachment in your modeling tool. See Also Skeletal Animations Joint Override Component","title":"Joint Attachment Component"},{"location":"animation/skeletal-animation/joint-attachment-component/#joint-attachment-component","text":"The joint attachment component is used to expose the animated position of a bone, such that you can attach objects there.","title":"Joint Attachment Component"},{"location":"animation/skeletal-animation/joint-attachment-component/#component-properties","text":"JointName : The name of the joint/bone of which you want to use the position as an attachment point. You can look up the bone names in the respective skeleton asset . PositionOffset , RotationOffset : Additional local position and rotation offsets added to the bone location. The same could be achieved by adding another child game object with an offset, but using these properties is more efficient.","title":"Component Properties"},{"location":"animation/skeletal-animation/joint-attachment-component/#how-to-use","text":"Whenever an animated mesh receives a new pose, it passes that pose on to all interested components that are attached to the same object or any child object. The joint attachment component listens to this message and positions its owner game object at the relative position of the selected bone. To attach an object to a certain bone, follow these steps: Create an empty game object as a child of the animated mesh. Add a joint attachment component to it. Set its JointName property to the desired bone name. You can look up the bone name on the skeleton asset that is used by the animated mesh asset on the animated mesh component . Add the desired object or component to the joint attachment object. The local transform of the attachment object will be overwritten by the component when it receives an animation pose. Thus setting any values here doesn't have any useful effect during simulation. To see where your attachment ends up, you need to simulate the scene and an animation has to actively play on the animated mesh. While the scene is simulating, you can use the position and rotation offset properties to tweak the exact location of the joint attachment. Note: The position and rotation offset properties are useful for minor tweaks. The same can also be achieved with another child object. However, it can be difficult to position an attachment perfectly and it might turn out that the attachment position needs tweaking depending on the animation as well. In such cases it is better to add a dedicated bone to the skeleton instead, such that you have full control over the attachment in your modeling tool.","title":"How To Use"},{"location":"animation/skeletal-animation/joint-attachment-component/#see-also","text":"Skeletal Animations Joint Override Component","title":"See Also"},{"location":"animation/skeletal-animation/joint-override-component/","text":"Joint Override Component The joint override component enables you to take control over the local transform of a specific bone. Any transform for that bone that comes from an animation is discarded and replaced with the local transform of the object on which this component is attached. This component can be very useful if you have an object that is typically driven by animations, but a specific bone is meant to be controlled procedurally through game code. An example would be a turret which has a shoot animation for recoil etc, but you want to control the direction into which the barrel points from game code. The joint override component works in the reverse way that the joint attachment component works. Every time an animation pose becomes available, this component will overwrite the transform of the selected bone with its own local transform. Thus you can control the bone transform simply by moving and rotating the game object on which the joint override component is attached. Component Properties JointName : The name of the joint/bone whose transform you want to take control over. You can look up the bone names in the respective skeleton asset . OverridePosition , OverrideRotation , OverrideScale : Whether the component shall override the bone position , rotation and/or scale . See Also Skeletal Animations Joint Attachment Component","title":"Joint Override Component"},{"location":"animation/skeletal-animation/joint-override-component/#joint-override-component","text":"The joint override component enables you to take control over the local transform of a specific bone. Any transform for that bone that comes from an animation is discarded and replaced with the local transform of the object on which this component is attached. This component can be very useful if you have an object that is typically driven by animations, but a specific bone is meant to be controlled procedurally through game code. An example would be a turret which has a shoot animation for recoil etc, but you want to control the direction into which the barrel points from game code. The joint override component works in the reverse way that the joint attachment component works. Every time an animation pose becomes available, this component will overwrite the transform of the selected bone with its own local transform. Thus you can control the bone transform simply by moving and rotating the game object on which the joint override component is attached.","title":"Joint Override Component"},{"location":"animation/skeletal-animation/joint-override-component/#component-properties","text":"JointName : The name of the joint/bone whose transform you want to take control over. You can look up the bone names in the respective skeleton asset . OverridePosition , OverrideRotation , OverrideScale : Whether the component shall override the bone position , rotation and/or scale .","title":"Component Properties"},{"location":"animation/skeletal-animation/joint-override-component/#see-also","text":"Skeletal Animations Joint Attachment Component","title":"See Also"},{"location":"animation/skeletal-animation/root-motion/","text":"Root Motion By default a skeletal animation has its origin at the position of the game object on which it is played. Relative to that location animations will move the bones and the skinned mesh will move accordingly. The game object itself stays fixed at that location, though. This is sufficient if either the game object shouldn't change its location anyway, or when any change in position is controlled through other means anyway. For example a player character might be moved around the world through custom code and a walking animation is only played to visualize the action. This approach can be the right solution, depending on the type of game. Such a method is, however, very prone to foot sliding , meaning an artifact where the feet move, but don't stick to the ground. It the movement of a game object should generally be determined by the exact blend of animation clips, it is better to have the motion be part of each animation clip. For example a walk animations would contain the information into which direction and at what speed a game object should be moved to fit the animation. When a forward and walk right animation get mixed together, their root motion information is equally mixed and the object would be moved diagonally. Defining Root Motion There are multiple ways how root motion could be defined for a clip. It could come from a dedicated bone for overall motion, or it could be extracted from how the feet touch the ground, etc. For the time being Plasma only implements the most simple method. An animation clip either has no root motion at all, or it has a constant motion that is used for the entire clip. This is sufficient to build basic locomotion animations. Finally, for now only positional root motion is available. That means an animation can change the position of a game object, but not its rotation. It is planned to add more sophisticated methods for root motion in the future. Applying Root Motion The simple animation component and the animation controller component get the root motion data from the played animation clips. There are these modes to apply it to their owner game object: Ignore : No root motion is applied, the game object will not be moved by the animation. ApplyToOwner : Any available root motion is directly applied to the game object and thus moves it without restriction. This mode is useful for objects that have to follow a fixed path. For example moving platforms (which are kinematic physics actors ), or objects that don't physically interact with the player. This mode is not suited for characters that should obey physical restrictions. SendMoveCharacterMsg : If this mode is used, root motion is not applied to any object, instead the message plMsgMoveCharacterController is sent to the top most game object in the hierarchy. This way, if there is also a character controller or other component that accepts this type of message, it can apply the root motion as it sees fit. See Also Skeletal Animations Simple Animation Component Animation Controller Component","title":"Root Motion"},{"location":"animation/skeletal-animation/root-motion/#root-motion","text":"By default a skeletal animation has its origin at the position of the game object on which it is played. Relative to that location animations will move the bones and the skinned mesh will move accordingly. The game object itself stays fixed at that location, though. This is sufficient if either the game object shouldn't change its location anyway, or when any change in position is controlled through other means anyway. For example a player character might be moved around the world through custom code and a walking animation is only played to visualize the action. This approach can be the right solution, depending on the type of game. Such a method is, however, very prone to foot sliding , meaning an artifact where the feet move, but don't stick to the ground. It the movement of a game object should generally be determined by the exact blend of animation clips, it is better to have the motion be part of each animation clip. For example a walk animations would contain the information into which direction and at what speed a game object should be moved to fit the animation. When a forward and walk right animation get mixed together, their root motion information is equally mixed and the object would be moved diagonally.","title":"Root Motion"},{"location":"animation/skeletal-animation/root-motion/#defining-root-motion","text":"There are multiple ways how root motion could be defined for a clip. It could come from a dedicated bone for overall motion, or it could be extracted from how the feet touch the ground, etc. For the time being Plasma only implements the most simple method. An animation clip either has no root motion at all, or it has a constant motion that is used for the entire clip. This is sufficient to build basic locomotion animations. Finally, for now only positional root motion is available. That means an animation can change the position of a game object, but not its rotation. It is planned to add more sophisticated methods for root motion in the future.","title":"Defining Root Motion"},{"location":"animation/skeletal-animation/root-motion/#applying-root-motion","text":"The simple animation component and the animation controller component get the root motion data from the played animation clips. There are these modes to apply it to their owner game object: Ignore : No root motion is applied, the game object will not be moved by the animation. ApplyToOwner : Any available root motion is directly applied to the game object and thus moves it without restriction. This mode is useful for objects that have to follow a fixed path. For example moving platforms (which are kinematic physics actors ), or objects that don't physically interact with the player. This mode is not suited for characters that should obey physical restrictions. SendMoveCharacterMsg : If this mode is used, root motion is not applied to any object, instead the message plMsgMoveCharacterController is sent to the top most game object in the hierarchy. This way, if there is also a character controller or other component that accepts this type of message, it can apply the root motion as it sees fit.","title":"Applying Root Motion"},{"location":"animation/skeletal-animation/root-motion/#see-also","text":"Skeletal Animations Simple Animation Component Animation Controller Component","title":"See Also"},{"location":"animation/skeletal-animation/simple-animation-component/","text":"Simple Animation Component The simple animation component is used to play a single animation clip on an animated mesh . The component has to be attached on a game object that also has an animated mesh component . The selected animation clip has to be compatible with the mesh's skeleton . For more complex scenarios use an animation controller instead. Component Properties AnimationClip : The animation clip to play. AnimationMode : How to play the animation: Once : The animation is played exactly once and then stops. Loop : The animation is played in an endless loop. BackAndForth : The animation is played to its end, then it reverses and plays back to the start. Then the cycle repeats. Speed : The playback speed. A speed of zero pauses playback. A negative speed makes the animation play backwards. The speed can be changed at any time. RootMotionMode : Selects how root motion is applied to the owning game object. Events The component will broadcast the event plMsgGenericEvent every time it encounters an animation event in the animation clip. Custom code can listen for these events and trigger relevant game mechanics. See Also Skeletal Animations Animation Clip Asset Animated Mesh Component Animation Controller","title":"Simple Animation Component"},{"location":"animation/skeletal-animation/simple-animation-component/#simple-animation-component","text":"The simple animation component is used to play a single animation clip on an animated mesh . The component has to be attached on a game object that also has an animated mesh component . The selected animation clip has to be compatible with the mesh's skeleton . For more complex scenarios use an animation controller instead.","title":"Simple Animation Component"},{"location":"animation/skeletal-animation/simple-animation-component/#component-properties","text":"AnimationClip : The animation clip to play. AnimationMode : How to play the animation: Once : The animation is played exactly once and then stops. Loop : The animation is played in an endless loop. BackAndForth : The animation is played to its end, then it reverses and plays back to the start. Then the cycle repeats. Speed : The playback speed. A speed of zero pauses playback. A negative speed makes the animation play backwards. The speed can be changed at any time. RootMotionMode : Selects how root motion is applied to the owning game object.","title":"Component Properties"},{"location":"animation/skeletal-animation/simple-animation-component/#events","text":"The component will broadcast the event plMsgGenericEvent every time it encounters an animation event in the animation clip. Custom code can listen for these events and trigger relevant game mechanics.","title":"Events"},{"location":"animation/skeletal-animation/simple-animation-component/#see-also","text":"Skeletal Animations Animation Clip Asset Animated Mesh Component Animation Controller","title":"See Also"},{"location":"animation/skeletal-animation/skeletal-animation-overview/","text":"Skeletal Animations Skeletal animations are used to animated meshes. This is typically used for game characters and robots, but is equally useful for other complex moving objects. Describing how skeletal animation works in general is out of scope for the Plasma documentation, but there are a vast amount of resources about this topic online. It is assumed here, that you are familiar with the concepts and know the basics about modelling, rigging and animating a mesh with a tool such as Blender . The rest of this document gives a high-level overview, how to get started with getting animated meshes into Plasma. For more in-depth descriptions of each feature, please consult the respective documentation pages. The Animation System Pieces The following elements are involved to make an animated mesh: Animated Mesh Asset The animated mesh asset represents the mesh of the animated object. This is a special version of the mesh asset . It works mostly the same way, except that it adds the necessary skinning information. Consequently, only these kinds of meshes can be used for skeletal animation. An animated mesh asset requires you to specify a default skeleton asset , otherwise it won't even transform the data. Skeleton Asset The skeleton asset stores the bone hierarchy of the animated object. This is also where you configure the overall scaling and which direction should be the forward, right and up vector of the imported model. The skeleton asset is also where you would set up physics collision shapes. Animation Clip Asset The animation clip asset represents a single animation , such as a walk or a jump animation. You may have multiple animations stored in a single .fbx or .glb file, but you need to create one animation clip asset for each animation that you want to import into Plasma. Just reference the same source file each time. The animation clip asset has a property UseAnimationClip through which you can choose which animation to extract from the source file. Currently you have to type the name of the animation. The Available Clips list just below it shows you which animations have been found in the file. Additionally, in case all animations are in one large clip, you can use the FirstFrame and NumFrames properties to extract only a subset of the animation. This allows you to import the same source file many times, each time extracting only a specific range as a single clip. The root motion properties are meant for enabling an animation clip to move an animated character (ie. the actual character controller that sits on top of the animated mesh). The event track property allows you to add markers to the clip, that indicate what happens at what time in the clip. This can be used to indicate when a foot touches the ground, or at what point in the animation a weapon fires. Using this information, the game logic could react by, for example, spawning an effect. Animation events are sent as event messages and therefore can only be captured by event handler components such as visual scripts or TypeScript components . Simple Animation Playback Once you have an animated mesh asset, a skeleton asset and an animation clip asset all set up, you can create an animated object in a scene by attaching an animated mesh component and a simple animation component to a game object like so: Advanced Animation Playback To create a playable character, you need multiple animations for all the actions that the character should be able to do. A big part revolves around locomotion , ie making the character walk around. Here it is not sufficient anymore to just play one animation, you will need to have multiple animations and blend them together in a convincing way. This is where the animation controller comes into play. This asset allows you to define how animations should be combined to make a character move fluidly and react to various inputs. Once you have basic animation playback working, getting familiar with animation controllers is the next step to make the most out of your animated characters. How to Import Animation Data To import an animated object, you need to set up multiple assets (the mesh, the skeleton, the animation clips). The easiest way is to drag drop it into the asset broweser or press CTRL+I to open the asset import dialog . Select your .fbx or .glb file and choose to import it as multiple asset types like so: This will create all three necessary assets. You can repeat this process, if you want to import multiple animation clips (select No Import for the other asset types then). You'll need to change the filename for each additional animation clip asset. Animation Utility Components The following components are currently available: Joint Attachment Component If you put a child object under an object with an animated mesh component, and attach a joint attachment component , the animation system will take control over that object's position. With the JointName property you specify a bone that you are interested in (see the skeleton asset , if you need to know what bones are available). Every game tick, the animation system will then move that game object to the position of the animated bone. This allows you to have game objects move in sync with the animation. This can be used to put an object into the hand of a creature, or to have an effect follow the animation. In the sample scene this is used to place an object at the (animated) front of the turret's barrel, such that the projectile will be spawned at the proper position. Joint Override Component The joint override component is the opposite to the joint attachment component . It works similar in that it has to be attached to a child object of an animated mesh and you have to type in the name of a bone. However, it will then override the specified bone's local transform with its own local transform . The idea here is, that game code can use this game object as a controller to steer the animated object. So for example your turret might be one complex animated object, with many bones but somewhere in the bone hierarchy there is one bone that controls the turrets aim (up/down or left/right or both). You want cool animations to \"unfold\" the turret when it is built and all sorts of other animations that are only possible with proper skeletal animation, but once the game runs you also want to procedurally control where the turret is aiming. Using this component you can take control of certain bones and drive their animation yourself. In the sample scene there are two objects for each turret that override two bones: one for left/right rotation and one for pointing up/down. You can manually modify these values from the property grid, while the editor is in play mode, to control the turrets aim. Note how the recoil animation continues to play properly relative to the turret's main direction. See Also Animated Mesh Asset Skeleton Asset Animation Clip Asset Animation Controller","title":"Skeletal Animations"},{"location":"animation/skeletal-animation/skeletal-animation-overview/#skeletal-animations","text":"Skeletal animations are used to animated meshes. This is typically used for game characters and robots, but is equally useful for other complex moving objects. Describing how skeletal animation works in general is out of scope for the Plasma documentation, but there are a vast amount of resources about this topic online. It is assumed here, that you are familiar with the concepts and know the basics about modelling, rigging and animating a mesh with a tool such as Blender . The rest of this document gives a high-level overview, how to get started with getting animated meshes into Plasma. For more in-depth descriptions of each feature, please consult the respective documentation pages.","title":"Skeletal Animations"},{"location":"animation/skeletal-animation/skeletal-animation-overview/#the-animation-system-pieces","text":"The following elements are involved to make an animated mesh:","title":"The Animation System Pieces"},{"location":"animation/skeletal-animation/skeletal-animation-overview/#animated-mesh-asset","text":"The animated mesh asset represents the mesh of the animated object. This is a special version of the mesh asset . It works mostly the same way, except that it adds the necessary skinning information. Consequently, only these kinds of meshes can be used for skeletal animation. An animated mesh asset requires you to specify a default skeleton asset , otherwise it won't even transform the data.","title":"Animated Mesh Asset"},{"location":"animation/skeletal-animation/skeletal-animation-overview/#skeleton-asset","text":"The skeleton asset stores the bone hierarchy of the animated object. This is also where you configure the overall scaling and which direction should be the forward, right and up vector of the imported model. The skeleton asset is also where you would set up physics collision shapes.","title":"Skeleton Asset"},{"location":"animation/skeletal-animation/skeletal-animation-overview/#animation-clip-asset","text":"The animation clip asset represents a single animation , such as a walk or a jump animation. You may have multiple animations stored in a single .fbx or .glb file, but you need to create one animation clip asset for each animation that you want to import into Plasma. Just reference the same source file each time. The animation clip asset has a property UseAnimationClip through which you can choose which animation to extract from the source file. Currently you have to type the name of the animation. The Available Clips list just below it shows you which animations have been found in the file. Additionally, in case all animations are in one large clip, you can use the FirstFrame and NumFrames properties to extract only a subset of the animation. This allows you to import the same source file many times, each time extracting only a specific range as a single clip. The root motion properties are meant for enabling an animation clip to move an animated character (ie. the actual character controller that sits on top of the animated mesh). The event track property allows you to add markers to the clip, that indicate what happens at what time in the clip. This can be used to indicate when a foot touches the ground, or at what point in the animation a weapon fires. Using this information, the game logic could react by, for example, spawning an effect. Animation events are sent as event messages and therefore can only be captured by event handler components such as visual scripts or TypeScript components .","title":"Animation Clip Asset"},{"location":"animation/skeletal-animation/skeletal-animation-overview/#simple-animation-playback","text":"Once you have an animated mesh asset, a skeleton asset and an animation clip asset all set up, you can create an animated object in a scene by attaching an animated mesh component and a simple animation component to a game object like so:","title":"Simple Animation Playback"},{"location":"animation/skeletal-animation/skeletal-animation-overview/#advanced-animation-playback","text":"To create a playable character, you need multiple animations for all the actions that the character should be able to do. A big part revolves around locomotion , ie making the character walk around. Here it is not sufficient anymore to just play one animation, you will need to have multiple animations and blend them together in a convincing way. This is where the animation controller comes into play. This asset allows you to define how animations should be combined to make a character move fluidly and react to various inputs. Once you have basic animation playback working, getting familiar with animation controllers is the next step to make the most out of your animated characters.","title":"Advanced Animation Playback"},{"location":"animation/skeletal-animation/skeletal-animation-overview/#how-to-import-animation-data","text":"To import an animated object, you need to set up multiple assets (the mesh, the skeleton, the animation clips). The easiest way is to drag drop it into the asset broweser or press CTRL+I to open the asset import dialog . Select your .fbx or .glb file and choose to import it as multiple asset types like so: This will create all three necessary assets. You can repeat this process, if you want to import multiple animation clips (select No Import for the other asset types then). You'll need to change the filename for each additional animation clip asset.","title":"How to Import Animation Data"},{"location":"animation/skeletal-animation/skeletal-animation-overview/#animation-utility-components","text":"The following components are currently available:","title":"Animation Utility Components"},{"location":"animation/skeletal-animation/skeletal-animation-overview/#joint-attachment-component","text":"If you put a child object under an object with an animated mesh component, and attach a joint attachment component , the animation system will take control over that object's position. With the JointName property you specify a bone that you are interested in (see the skeleton asset , if you need to know what bones are available). Every game tick, the animation system will then move that game object to the position of the animated bone. This allows you to have game objects move in sync with the animation. This can be used to put an object into the hand of a creature, or to have an effect follow the animation. In the sample scene this is used to place an object at the (animated) front of the turret's barrel, such that the projectile will be spawned at the proper position.","title":"Joint Attachment Component"},{"location":"animation/skeletal-animation/skeletal-animation-overview/#joint-override-component","text":"The joint override component is the opposite to the joint attachment component . It works similar in that it has to be attached to a child object of an animated mesh and you have to type in the name of a bone. However, it will then override the specified bone's local transform with its own local transform . The idea here is, that game code can use this game object as a controller to steer the animated object. So for example your turret might be one complex animated object, with many bones but somewhere in the bone hierarchy there is one bone that controls the turrets aim (up/down or left/right or both). You want cool animations to \"unfold\" the turret when it is built and all sorts of other animations that are only possible with proper skeletal animation, but once the game runs you also want to procedurally control where the turret is aiming. Using this component you can take control of certain bones and drive their animation yourself. In the sample scene there are two objects for each turret that override two bones: one for left/right rotation and one for pointing up/down. You can manually modify these values from the property grid, while the editor is in play mode, to control the turrets aim. Note how the recoil animation continues to play properly relative to the turret's main direction.","title":"Joint Override Component"},{"location":"animation/skeletal-animation/skeletal-animation-overview/#see-also","text":"Animated Mesh Asset Skeleton Asset Animation Clip Asset Animation Controller","title":"See Also"},{"location":"animation/skeletal-animation/skeleton-asset/","text":"Skeleton Asset The main function of the Skeleton asset is to store information about the bone hierarchy in an animated mesh . Here you adjust the overall scale and rotation of imported animated meshes. In the future the skeleton asset will also be used to define physics shapes for bones for collision detection and ragdolls. This is currently not yet implemented, though. Asset Properties File : The file from which to import the skeleton information. This is typically the same file as in the animated mesh asset . RightDir , UpDir , FlipForwardDir : These properties are the same as on the mesh asset . Depending on how the mesh was exported, you may need to adjust these to have the skeleton (and every mesh that uses this skeleton) stand upright and look into the correct direction. In Blender it is common to model meshes such that they face the user when the front view is active ( Numpad 1 ): With such an orientation the right side of the model points into the -X direction. The +Z axis corresponds to the up direction and the model looks into the +Y direction. When you export such a model to GLTF/GLB, you can keep this convention ( Y up disabled in the export settings) or you can export it in the more common convention of using +Y as the up axis ( Y up enabled). Both conventions can be mapped to PLASMA's preferred convention like so: For GLB files exported from Blender with Y up use: * RightDir = Negative X * UpDir = Positive Y * FlipForward = off For GLB files exported from Blender with Z up use: * RightDir = Negative X * UpDir = Positive Z * FlipForward = off Note that inPlasmathe convention is that models look along the +X axis. Every component (such as AI) assumes that moving along the +X axis will move the mesh forward , moving along +Y moves it to the right and moving along +Z moves it upwards . It is therefore best to import all meshes this way right away. UniformScale : The overall size of the skeleton. Use this if you need to adjust from centimeters to meters. BoneDirection : This setting only affects the visualization of the skeleton. It has no effect on the actual mesh skinning. It is used to tell the visualizer which cardinal direction the bones should point into. You only need to change this setting, if the skeleton visualization looks all wrong (all lines point into weird directions). You need to transform the asset to apply the change. Just try all options until it looks right. See Also Skeletal Animations Animated Mesh Asset Skeleton Component","title":"Skeleton Asset"},{"location":"animation/skeletal-animation/skeleton-asset/#skeleton-asset","text":"The main function of the Skeleton asset is to store information about the bone hierarchy in an animated mesh . Here you adjust the overall scale and rotation of imported animated meshes. In the future the skeleton asset will also be used to define physics shapes for bones for collision detection and ragdolls. This is currently not yet implemented, though.","title":"Skeleton Asset"},{"location":"animation/skeletal-animation/skeleton-asset/#asset-properties","text":"File : The file from which to import the skeleton information. This is typically the same file as in the animated mesh asset . RightDir , UpDir , FlipForwardDir : These properties are the same as on the mesh asset . Depending on how the mesh was exported, you may need to adjust these to have the skeleton (and every mesh that uses this skeleton) stand upright and look into the correct direction. In Blender it is common to model meshes such that they face the user when the front view is active ( Numpad 1 ): With such an orientation the right side of the model points into the -X direction. The +Z axis corresponds to the up direction and the model looks into the +Y direction. When you export such a model to GLTF/GLB, you can keep this convention ( Y up disabled in the export settings) or you can export it in the more common convention of using +Y as the up axis ( Y up enabled). Both conventions can be mapped to PLASMA's preferred convention like so: For GLB files exported from Blender with Y up use: * RightDir = Negative X * UpDir = Positive Y * FlipForward = off For GLB files exported from Blender with Z up use: * RightDir = Negative X * UpDir = Positive Z * FlipForward = off Note that inPlasmathe convention is that models look along the +X axis. Every component (such as AI) assumes that moving along the +X axis will move the mesh forward , moving along +Y moves it to the right and moving along +Z moves it upwards . It is therefore best to import all meshes this way right away. UniformScale : The overall size of the skeleton. Use this if you need to adjust from centimeters to meters. BoneDirection : This setting only affects the visualization of the skeleton. It has no effect on the actual mesh skinning. It is used to tell the visualizer which cardinal direction the bones should point into. You only need to change this setting, if the skeleton visualization looks all wrong (all lines point into weird directions). You need to transform the asset to apply the change. Just try all options until it looks right.","title":"Asset Properties"},{"location":"animation/skeletal-animation/skeleton-asset/#see-also","text":"Skeletal Animations Animated Mesh Asset Skeleton Component","title":"See Also"},{"location":"animation/skeletal-animation/skeleton-component/","text":"Skeleton Component The skeleton component can be used to visualize the current pose of an animated mesh . This component is only meant for debugging purposes. Component Properties Skeleton : The skeleton asset to visualize. You can set a skeleton asset manually, or keep it empty. In the latter case the skeleton will automatically be taken from a sibling animated mesh component , but only when the pose of that mesh is changed. VisualizeSkeleton : Whether the component should visualize the skeleton at this time. BonesToHighlight : A semicolon-separated list of bone names (case sensitive). Every bone whose name appears in the list will be rendered with a highlight. To know the available bone names, inspect the bone hierarchy in the respective skeleton asset . See Also Skeletal Animations Animated Mesh Component","title":"Skeleton Component"},{"location":"animation/skeletal-animation/skeleton-component/#skeleton-component","text":"The skeleton component can be used to visualize the current pose of an animated mesh . This component is only meant for debugging purposes.","title":"Skeleton Component"},{"location":"animation/skeletal-animation/skeleton-component/#component-properties","text":"Skeleton : The skeleton asset to visualize. You can set a skeleton asset manually, or keep it empty. In the latter case the skeleton will automatically be taken from a sibling animated mesh component , but only when the pose of that mesh is changed. VisualizeSkeleton : Whether the component should visualize the skeleton at this time. BonesToHighlight : A semicolon-separated list of bone names (case sensitive). Every bone whose name appears in the list will be rendered with a highlight. To know the available bone names, inspect the bone hierarchy in the respective skeleton asset .","title":"Component Properties"},{"location":"animation/skeletal-animation/skeleton-component/#see-also","text":"Skeletal Animations Animated Mesh Component","title":"See Also"},{"location":"animation/skeletal-animation/skeleton-pose-component/","text":"Skeleton Pose Component The skeleton pose component allows you to assign a custom, static pose to an animated mesh. This can be used for decorative purposes, for instance to place an animated mesh in different poses in your scenes, but it can also be used as a start pose for a mesh that is further animated, for example through ragdoll physics (TODO) . Component Properties Skeleton : The skeleton for posing. Mode : Selects the source for the custom pose. Bones : The array of bones and their current values. Through the UI you can modify bone rotations or reset them to their default by clicking the 'x' button. However, it is more convenient to edit the bones in the 3D viewport. Click the blue property to enable the bone manipulator, then click on one of the joints to select it for editing, as seen in the image above. Using Ragdolls for Posing When a pose component and a ragdoll component (TODO) are both present on an animated mesh, the pose component can be used to define the starting pose of the ragdoll (make sure to configure the ragdoll to wait for a pose before it starts simulating). Additionally, when you simulate a scene in editor, you can save the result of a ragdoll simulation in a pose component: Place your ragdoll, make sure a pose component is attached, simulate the scene and then, while the simulation is still running, make sure your ragdoll is selected and press K (or Scene > Utilities > Keep Simulation Changes ). Once you stop the simulation, the pose component gets updated to mimic the pose that the ragdoll was in. You can then adjust individual limbs and repeat the process with a new starting pose for the ragdoll, until you are happy with the result. You can also remove the ragdoll component afterwards. See Also Skeletal Animations Animated Mesh Component Ragdoll Component (TODO)","title":"Skeleton Pose Component"},{"location":"animation/skeletal-animation/skeleton-pose-component/#skeleton-pose-component","text":"The skeleton pose component allows you to assign a custom, static pose to an animated mesh. This can be used for decorative purposes, for instance to place an animated mesh in different poses in your scenes, but it can also be used as a start pose for a mesh that is further animated, for example through ragdoll physics (TODO) .","title":"Skeleton Pose Component"},{"location":"animation/skeletal-animation/skeleton-pose-component/#component-properties","text":"Skeleton : The skeleton for posing. Mode : Selects the source for the custom pose. Bones : The array of bones and their current values. Through the UI you can modify bone rotations or reset them to their default by clicking the 'x' button. However, it is more convenient to edit the bones in the 3D viewport. Click the blue property to enable the bone manipulator, then click on one of the joints to select it for editing, as seen in the image above.","title":"Component Properties"},{"location":"animation/skeletal-animation/skeleton-pose-component/#using-ragdolls-for-posing","text":"When a pose component and a ragdoll component (TODO) are both present on an animated mesh, the pose component can be used to define the starting pose of the ragdoll (make sure to configure the ragdoll to wait for a pose before it starts simulating). Additionally, when you simulate a scene in editor, you can save the result of a ragdoll simulation in a pose component: Place your ragdoll, make sure a pose component is attached, simulate the scene and then, while the simulation is still running, make sure your ragdoll is selected and press K (or Scene > Utilities > Keep Simulation Changes ). Once you stop the simulation, the pose component gets updated to mimic the pose that the ragdoll was in. You can then adjust individual limbs and repeat the process with a new starting pose for the ragdoll, until you are happy with the result. You can also remove the ragdoll component afterwards.","title":"Using Ragdolls for Posing"},{"location":"animation/skeletal-animation/skeleton-pose-component/#see-also","text":"Skeletal Animations Animated Mesh Component Ragdoll Component (TODO)","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-blackboard/","text":"Blackboard Nodes The animation controller provides nodes to read and write values from and to a blackboard . For this, the game object on which the animation controller component is attached, also needs to hold a blackboard component. Note: If no blackboad is available, these nodes will output a warning to the log . If a blackboard is available, but the desired entry is not (yet) in the blackboard, they may add the entry or assume a default value of zero. Set Blackboard Value Node ![[blackboardSet.png]] When activated or deactivated, this node writes a given value to the blackboard. Properties BlackboardEntry : The name of the blackboard entry (variable) to write to. SetOnActivation : If true, ActivationValue will be written to the blackboard whenever the Active input pin changes from disabled to enabled. ActivationValue : The value that shall be written to the blackboard, when the Active pin becomes enabled. SetOnDeactivation : If true, DeactivationValue will be written to the blackboard whenever the Active input pin changes from enabled to disabled. DeactivationValue : The value that shall be written to the blackboard, when the Active pin becomes disabled. Input Pins Active : The active state determines when either the ActivationValue or DeactivationValue shall be written to the blackbard. As long as this pin's state doesn't change, no value is written. Check Blackboard Value Node ![[blackboardCheck.png]] This node constantly monitors a blackboard value and compares it to a reference value. Whenever the comparison yields true , the Active output pin is enabled, otherwise disabled. Properties BlackboardEntry : The name of the blackboard entry (variable) to monitor. ReferenceValue : A reference value for the comparison. Comparison : The way the two values get compared. Output Pins Active : This output pin will be triggered whenever the comparison was successful. Get Blackboard Value Node ![[blackboardGet.png]] This node outputs the value of a specific blackboard entry. The number value can then be forwarded to other nodes. Properties BlackboardEntry : The name of the blackboard entry (variable) to read. Output Pins Number : The value of the blackboard entry. If the entry doesn't exist, the pin outputs zero. See Also Animation Controller Skeletal Animations","title":"Blackboard Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-blackboard/#blackboard-nodes","text":"The animation controller provides nodes to read and write values from and to a blackboard . For this, the game object on which the animation controller component is attached, also needs to hold a blackboard component. Note: If no blackboad is available, these nodes will output a warning to the log . If a blackboard is available, but the desired entry is not (yet) in the blackboard, they may add the entry or assume a default value of zero.","title":"Blackboard Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-blackboard/#set-blackboard-value-node","text":"![[blackboardSet.png]] When activated or deactivated, this node writes a given value to the blackboard.","title":"Set Blackboard Value Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-blackboard/#properties","text":"BlackboardEntry : The name of the blackboard entry (variable) to write to. SetOnActivation : If true, ActivationValue will be written to the blackboard whenever the Active input pin changes from disabled to enabled. ActivationValue : The value that shall be written to the blackboard, when the Active pin becomes enabled. SetOnDeactivation : If true, DeactivationValue will be written to the blackboard whenever the Active input pin changes from enabled to disabled. DeactivationValue : The value that shall be written to the blackboard, when the Active pin becomes disabled.","title":"Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-blackboard/#input-pins","text":"Active : The active state determines when either the ActivationValue or DeactivationValue shall be written to the blackbard. As long as this pin's state doesn't change, no value is written.","title":"Input Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-blackboard/#check-blackboard-value-node","text":"![[blackboardCheck.png]] This node constantly monitors a blackboard value and compares it to a reference value. Whenever the comparison yields true , the Active output pin is enabled, otherwise disabled.","title":"Check Blackboard Value Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-blackboard/#properties_1","text":"BlackboardEntry : The name of the blackboard entry (variable) to monitor. ReferenceValue : A reference value for the comparison. Comparison : The way the two values get compared.","title":"Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-blackboard/#output-pins","text":"Active : This output pin will be triggered whenever the comparison was successful.","title":"Output Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-blackboard/#get-blackboard-value-node","text":"![[blackboardGet.png]] This node outputs the value of a specific blackboard entry. The number value can then be forwarded to other nodes.","title":"Get Blackboard Value Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-blackboard/#properties_2","text":"BlackboardEntry : The name of the blackboard entry (variable) to read.","title":"Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-blackboard/#output-pins_1","text":"Number : The value of the blackboard entry. If the entry doesn't exist, the pin outputs zero.","title":"Output Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-blackboard/#see-also","text":"Animation Controller Skeletal Animations","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-bone-weights/","text":"Bone Weight Nodes Bone weight nodes are used to generate a weight mask. The mask defines how strongly an animation clip will influence different parts of the skeleton. This is frequently used to apply an animation only to certain parts of a character, for example only the upper or lower body, or even only the left or right arm. For example it is common to play a walking animation only on the bones below the hip, whereas on the spine and upwards one would want to play an attack animation. Since animations are often authored for the entire skeleton, it is therefore necessary to mask out unwanted parts. Bone weights are often in the range of zero to one, with zero meaning that that bone is entirely unaffected by an animation and one means it is fully affected. However, for convenience, weights above one are allowed as well. The system simply normalizes the weights on every bone at the very end. This way, if one animation affects a bone with a weight of one, and another animation affects the same bone with a weight of nine, the first one will only have 10% influence and the second has 90% influence. That makes it easier to layer an important animation on top of a base animation. By simply setting a very large weight (10 or more) an animation can easily override a part of the body, without having to use an inverse mask to filter out the base animation. Important: Not all animations will work correctly when they are layered on top of each other. If one animation rotates a bone into one direction, and another animation rotates the same bone very differently, it is possible for the interpolation of the rotations to result in an invalid value. This will manifest as jerking or jumping bones at specific points in the animation. If that happens, you have to use an inverse bone mask to fully filter out the base animation, such that in the end only one of the animations really influences those bones. Bone weights are typically connected directly to an animation clip sampling node, and the information that this animation clip shall only influence a part of the skeleton is passed along until it reaches a combine poses node where the result is baked into one pose. Without such a node in the graph, the bone weights won't have an effect. Bone Weight Config Node ![[boneWeights.png]] This node creates a mask for every bone in the skeleton. By default, the mask is zero for every bone. You then add bones by name to the RootBones array. Every bone that is reachable from any of the root bones, will get a weight of one. You can specify multiple root bones, in case that an animation should for example affect both arms, but not the spine and head. Properties Weight : The overall weight for the mask. A higher weight means that animation clips that use this weight mask will have stronger influence on the final pose. RootBones : An array of bone names from where the weight mask should be set to one. Typically this only holds a single entry, for example the hip bone (to affect both legs) or a spine or shoulder bone (to affect the arms and head). Output Pins Weights : This represents the full bone mask and can be passed into animation clip sampling nodes, to make them only affect the desired bones. InverseWeights : If this pin is connected, the node generates the inverse mask as well. So for example, if the node would generate a mask that only affects the head, then the inverse mask will affect everything but the head. See Also Animation Controller Skeletal Animations","title":"Bone Weight Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-bone-weights/#bone-weight-nodes","text":"Bone weight nodes are used to generate a weight mask. The mask defines how strongly an animation clip will influence different parts of the skeleton. This is frequently used to apply an animation only to certain parts of a character, for example only the upper or lower body, or even only the left or right arm. For example it is common to play a walking animation only on the bones below the hip, whereas on the spine and upwards one would want to play an attack animation. Since animations are often authored for the entire skeleton, it is therefore necessary to mask out unwanted parts. Bone weights are often in the range of zero to one, with zero meaning that that bone is entirely unaffected by an animation and one means it is fully affected. However, for convenience, weights above one are allowed as well. The system simply normalizes the weights on every bone at the very end. This way, if one animation affects a bone with a weight of one, and another animation affects the same bone with a weight of nine, the first one will only have 10% influence and the second has 90% influence. That makes it easier to layer an important animation on top of a base animation. By simply setting a very large weight (10 or more) an animation can easily override a part of the body, without having to use an inverse mask to filter out the base animation. Important: Not all animations will work correctly when they are layered on top of each other. If one animation rotates a bone into one direction, and another animation rotates the same bone very differently, it is possible for the interpolation of the rotations to result in an invalid value. This will manifest as jerking or jumping bones at specific points in the animation. If that happens, you have to use an inverse bone mask to fully filter out the base animation, such that in the end only one of the animations really influences those bones. Bone weights are typically connected directly to an animation clip sampling node, and the information that this animation clip shall only influence a part of the skeleton is passed along until it reaches a combine poses node where the result is baked into one pose. Without such a node in the graph, the bone weights won't have an effect.","title":"Bone Weight Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-bone-weights/#bone-weight-config-node","text":"![[boneWeights.png]] This node creates a mask for every bone in the skeleton. By default, the mask is zero for every bone. You then add bones by name to the RootBones array. Every bone that is reachable from any of the root bones, will get a weight of one. You can specify multiple root bones, in case that an animation should for example affect both arms, but not the spine and head.","title":"Bone Weight Config Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-bone-weights/#properties","text":"Weight : The overall weight for the mask. A higher weight means that animation clips that use this weight mask will have stronger influence on the final pose. RootBones : An array of bone names from where the weight mask should be set to one. Typically this only holds a single entry, for example the hip bone (to affect both legs) or a spine or shoulder bone (to affect the arms and head).","title":"Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-bone-weights/#output-pins","text":"Weights : This represents the full bone mask and can be passed into animation clip sampling nodes, to make them only affect the desired bones. InverseWeights : If this pin is connected, the node generates the inverse mask as well. So for example, if the node would generate a mask that only affects the head, then the inverse mask will affect everything but the head.","title":"Output Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-bone-weights/#see-also","text":"Animation Controller Skeletal Animations","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-combine-poses/","text":"Combine Poses Nodes An animation controller typically samples more than one animation. For example you may want to play a walking animation on the lower part of a character, and an aim weapon animation on the upper part. These animations are generally independent of each other, but have to be combined at some point, to form the final single animation pose. Certain nodes in the animation controller graph allow you to accomplish this. Combine Poses Node ![[CombinePose.png]] Currently the Combine Poses node is the only available node to combine multiple poses in local space into one. Properties MaxPoses : The maximum number of active poses to blend. You can have more input pins connected to this node. This number just says how many of them will be blended, if more poses than this are actually active at any given time. If more poses are active, the ones with the lowest weight will be ignored. Example: Let's say this value is set to 2. You have three input poses connected. One for walking, one for aiming a gun and one as a general breathing / idle animation. If all three poses are active at the same time, one of them will be ignored. For example, if walking and aiming both have a weight of 1, but breathing has a weight of 0.5, the breathing animation will not be mixed into the final result. This is used to clamp the maximum performance cost of the animation blending. Input Pins LocalPoses : This is a single input pin that allows an unlimited number of connections. Each incoming pose carries not only the bone transformations, but also the bone weights . These are typically determined by the animation clip sampling nodes and the bone weight nodes . All poses are mixed together according to their overall weight. In practice that means that two animations that don't use custom bone weights will be blended 50:50. Output Pins LocalPose : The single combined pose in local space . It is common to pass this directly on to a Local To Model Pose node . Performance Considerations You can use multiple nodes to combine many poses in several steps. However, for best performance prefer to use only a single node to combine many poses and make use of bone weights to control each ones overall influence. See Also Animation Controller Skeletal Animations","title":"Combine Poses Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-combine-poses/#combine-poses-nodes","text":"An animation controller typically samples more than one animation. For example you may want to play a walking animation on the lower part of a character, and an aim weapon animation on the upper part. These animations are generally independent of each other, but have to be combined at some point, to form the final single animation pose. Certain nodes in the animation controller graph allow you to accomplish this.","title":"Combine Poses Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-combine-poses/#combine-poses-node","text":"![[CombinePose.png]] Currently the Combine Poses node is the only available node to combine multiple poses in local space into one.","title":"Combine Poses Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-combine-poses/#properties","text":"MaxPoses : The maximum number of active poses to blend. You can have more input pins connected to this node. This number just says how many of them will be blended, if more poses than this are actually active at any given time. If more poses are active, the ones with the lowest weight will be ignored. Example: Let's say this value is set to 2. You have three input poses connected. One for walking, one for aiming a gun and one as a general breathing / idle animation. If all three poses are active at the same time, one of them will be ignored. For example, if walking and aiming both have a weight of 1, but breathing has a weight of 0.5, the breathing animation will not be mixed into the final result. This is used to clamp the maximum performance cost of the animation blending.","title":"Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-combine-poses/#input-pins","text":"LocalPoses : This is a single input pin that allows an unlimited number of connections. Each incoming pose carries not only the bone transformations, but also the bone weights . These are typically determined by the animation clip sampling nodes and the bone weight nodes . All poses are mixed together according to their overall weight. In practice that means that two animations that don't use custom bone weights will be blended 50:50.","title":"Input Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-combine-poses/#output-pins","text":"LocalPose : The single combined pose in local space . It is common to pass this directly on to a Local To Model Pose node .","title":"Output Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-combine-poses/#performance-considerations","text":"You can use multiple nodes to combine many poses in several steps. However, for best performance prefer to use only a single node to combine many poses and make use of bone weights to control each ones overall influence.","title":"Performance Considerations"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-combine-poses/#see-also","text":"Animation Controller Skeletal Animations","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-debug/","text":"Debug Nodes These animation controller nodes can be used to find problems. Log Node ![[Log.png]] The log node simply prints a dev string to the log whenever it gets activated. Properties Text : The text to print. This may include placeholders for the input values. Use {0} to {3} to embed the values from Input0 to Input3 respectively. Input Pins Active : Every frame in which this pin is triggered, the node will log Text as a Dev message to the log . Input0 to Input3 : These pins allow you to pass in values that can be printed to the log. See Also Animation Controller Skeletal Animations","title":"Debug Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-debug/#debug-nodes","text":"These animation controller nodes can be used to find problems.","title":"Debug Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-debug/#log-node","text":"![[Log.png]] The log node simply prints a dev string to the log whenever it gets activated.","title":"Log Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-debug/#properties","text":"Text : The text to print. This may include placeholders for the input values. Use {0} to {3} to embed the values from Input0 to Input3 respectively.","title":"Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-debug/#input-pins","text":"Active : Every frame in which this pin is triggered, the node will log Text as a Dev message to the log . Input0 to Input3 : These pins allow you to pass in values that can be printed to the log.","title":"Input Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-debug/#see-also","text":"Animation Controller Skeletal Animations","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-events/","text":"Event Nodes Event nodes are used to broadcast event messages on the game object on which the animation controller is running. This allows other code to react at the right moment to things like an animation being finished. Event nodes allow you to broadcast custom events under exactly defined conditions. Additionally, every time an animation clip is played, and actively contributes to the final pose, events that are defined on that clip will automatically be broadcast on the associated game object. Note that the animation controller itself cannot react to events. For that purpose use custom code . Send Event Node ![[SendEvent.png]] When this node is triggered, it broadcasts an plMsgGenericEvent with Message set to the value of EventName . Properties EventName : The string that is used as the Message property of the plMsgGenericEvent that is broadcast. Input Pins Active : While this pin is triggered, the message is sent (once every frame). See Also Animation Controller Skeletal Animations","title":"Event Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-events/#event-nodes","text":"Event nodes are used to broadcast event messages on the game object on which the animation controller is running. This allows other code to react at the right moment to things like an animation being finished. Event nodes allow you to broadcast custom events under exactly defined conditions. Additionally, every time an animation clip is played, and actively contributes to the final pose, events that are defined on that clip will automatically be broadcast on the associated game object. Note that the animation controller itself cannot react to events. For that purpose use custom code .","title":"Event Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-events/#send-event-node","text":"![[SendEvent.png]] When this node is triggered, it broadcasts an plMsgGenericEvent with Message set to the value of EventName .","title":"Send Event Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-events/#properties","text":"EventName : The string that is used as the Message property of the plMsgGenericEvent that is broadcast.","title":"Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-events/#input-pins","text":"Active : While this pin is triggered, the message is sent (once every frame).","title":"Input Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-events/#see-also","text":"Animation Controller Skeletal Animations","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-input/","text":"Input Nodes Input nodes expose the state of input devices to the animation controller. Input nodes are mainly provided for convenience during prototyping, as they may circumvent key mappings and general game state (e.g. whether the player is even allowed to move a character at all, at the moment). For a proper game, it is better to use an input component to forward input state to custom code and then decide their which animation shall get played. Then you can forward that state to the animation controller, through a blackboard . The animation controller itself would retrieve what it shall do through the blackboard nodes . XBox Controller Input Node ![[Controller.png]] This node reads the raw state of the connected XBox controller 1. It then outputs the button states as trigger or number pins, depending on whether the respective button or stick provides an analog signal. This node completely ignores any kind of button mapping. It is purely meant for prototyping scenarios, where it can be very convenient. Output Pins This node has one output pin for every button and stick direction. If you need to turn an analog signal into a trigger value, use the Compare Number node . See Also Animation Controller Skeletal Animations","title":"Input Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-input/#input-nodes","text":"Input nodes expose the state of input devices to the animation controller. Input nodes are mainly provided for convenience during prototyping, as they may circumvent key mappings and general game state (e.g. whether the player is even allowed to move a character at all, at the moment). For a proper game, it is better to use an input component to forward input state to custom code and then decide their which animation shall get played. Then you can forward that state to the animation controller, through a blackboard . The animation controller itself would retrieve what it shall do through the blackboard nodes .","title":"Input Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-input/#xbox-controller-input-node","text":"![[Controller.png]] This node reads the raw state of the connected XBox controller 1. It then outputs the button states as trigger or number pins, depending on whether the respective button or stick provides an analog signal. This node completely ignores any kind of button mapping. It is purely meant for prototyping scenarios, where it can be very convenient.","title":"XBox Controller Input Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-input/#output-pins","text":"This node has one output pin for every button and stick direction. If you need to turn an analog signal into a trigger value, use the Compare Number node .","title":"Output Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-input/#see-also","text":"Animation Controller Skeletal Animations","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/","text":"Logic and Math Nodes The animation controller provides a couple of nodes to evaluate basic arithmatic and logic. This is meant for very simple use cases and for quick prototyping. Often animation logic requires much more complex rules than what would be feasible to express in the animation controller graph. Instead use custom code to decide which animation should run under which circumstances, and pass the result to the animation controller via a blackboard . The animation controller can then simply read the state for each animation using the blackboard nodes . Logic AND Node This node sets its output pin to triggered when all incoming connections on the input pin are triggered at the same time. Note that even though there is only a single input pin, it can be connected to multiple sources. Properties NegateResult : If set, the output value will be negated. This effectively turns it into a NAND logic node. Logic OR Node This node sets its output pin to triggered when any incoming connections on the input pin are triggered. Note that even though there is only a single input pin, it can be connected to multiple sources. Properties NegateResult : If set, the output value will be negated. This effectively turns it into a NOR logic node. Logic NOT Node This node sets its output pin to triggered when the input pin is not triggered and vice versa. As with the other nodes, you can connect multiple sources to this input pin. In this case, if any of those connections is triggered, the entire input pin is considered to be in the triggered state, and the output will be the opposite. Compare Value Node This node can be used to check whether a number value compares in a certain way against a reference value. For example whether some input value is larger than 0.5. If this is the case, the output pin will be triggered. Properties ReferenceValue : The reference value to compare the incoming value against. Comparison : The mathematical operation with which to compare the two values. Input Pins Number : The number to compare against the reference value. Output Pins Active : The output pin is triggered whenever the comparison is successful. Math Expression Node The math expression node takes up to four different numbers as its input, plugs them into a user provided expression and outputs the result. The expression must be syntactically correct, otherwise the node prints an error to the log . Properties Expression : The expression can use the following: Numbers in floating point format (e.g. 1 , 2.3 , -78 ) + , - , * , / , % (modulo) Parenthesis ( and ) to specify precedence The variables a , b , c and d representing the input pin values The functions abs and sqrt Examples: a * 0.5 - b abs(a) + abs(b) (a + 1) % 2 Input Pins a , b , c and d : Input values to the mathematical expression. Unconnected pins are treated as having the value zero. Output Pins Result : Outputs the result of the evaluated expression. See Also Animation Controller Skeletal Animations","title":"Logic and Math Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/#logic-and-math-nodes","text":"The animation controller provides a couple of nodes to evaluate basic arithmatic and logic. This is meant for very simple use cases and for quick prototyping. Often animation logic requires much more complex rules than what would be feasible to express in the animation controller graph. Instead use custom code to decide which animation should run under which circumstances, and pass the result to the animation controller via a blackboard . The animation controller can then simply read the state for each animation using the blackboard nodes .","title":"Logic and Math Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/#logic-and-node","text":"This node sets its output pin to triggered when all incoming connections on the input pin are triggered at the same time. Note that even though there is only a single input pin, it can be connected to multiple sources.","title":"Logic AND Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/#properties","text":"NegateResult : If set, the output value will be negated. This effectively turns it into a NAND logic node.","title":"Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/#logic-or-node","text":"This node sets its output pin to triggered when any incoming connections on the input pin are triggered. Note that even though there is only a single input pin, it can be connected to multiple sources.","title":"Logic OR Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/#properties_1","text":"NegateResult : If set, the output value will be negated. This effectively turns it into a NOR logic node.","title":"Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/#logic-not-node","text":"This node sets its output pin to triggered when the input pin is not triggered and vice versa. As with the other nodes, you can connect multiple sources to this input pin. In this case, if any of those connections is triggered, the entire input pin is considered to be in the triggered state, and the output will be the opposite.","title":"Logic NOT Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/#compare-value-node","text":"This node can be used to check whether a number value compares in a certain way against a reference value. For example whether some input value is larger than 0.5. If this is the case, the output pin will be triggered.","title":"Compare Value Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/#properties_2","text":"ReferenceValue : The reference value to compare the incoming value against. Comparison : The mathematical operation with which to compare the two values.","title":"Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/#input-pins","text":"Number : The number to compare against the reference value.","title":"Input Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/#output-pins","text":"Active : The output pin is triggered whenever the comparison is successful.","title":"Output Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/#math-expression-node","text":"The math expression node takes up to four different numbers as its input, plugs them into a user provided expression and outputs the result. The expression must be syntactically correct, otherwise the node prints an error to the log .","title":"Math Expression Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/#properties_3","text":"Expression : The expression can use the following: Numbers in floating point format (e.g. 1 , 2.3 , -78 ) + , - , * , / , % (modulo) Parenthesis ( and ) to specify precedence The variables a , b , c and d representing the input pin values The functions abs and sqrt Examples: a * 0.5 - b abs(a) + abs(b) (a + 1) % 2","title":"Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/#input-pins_1","text":"a , b , c and d : Input values to the mathematical expression. Unconnected pins are treated as having the value zero.","title":"Input Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/#output-pins_1","text":"Result : Outputs the result of the evaluated expression.","title":"Output Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-logic-math/#see-also","text":"Animation Controller Skeletal Animations","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-mix1d/","text":"Mix Clips 1D Node The Mix Clips 1D animation controller node is used to linearly interpolate between a fixed set of animations. ![[anim-mix1d.gif]] Every animation clip has a position value in 1D space. The lerp input pin value determines how to interpolate those clips. The output pose will be either exactly one of those clips, or a mix between two clips, but never more than that. So if one clip is placed at position 0 and another at position 1 , you can fade from the first clip to the second by passing in a lerp value between 0 and 1 . The length of each clip may be different, however, the lookup positions across all clips are synchronized. That means if two clips are being mixed, and the first clip is sampled right at its middle, then the second clip will also be sampled at its middle, even if this is a completely different time offset (say 1 second versus 1.5 seconds). At which speed to move the sample position forwards, is determined by the length of the two animation clips that the lerp value is closest to. This node is useful if you have an action that can be done at different speeds and you want to cover all possible speeds with just a few different animation clips. The most intuitive example is a walk/run motion. You only need two animation clips, one for slow walking and one for fast running, and this node allows you to generate any speed in between through interpolation. For this to work, all animation clips have to follow the rule that they do the same motion at the same relative time offset. So in the case of a walk/run motion, both clips have to start with the same foot forwards, then move the other foot and finally move the first foot again, such that the animation is looped. The clips can have different lengths, though, so the run clip might be shorter than the walk clip (and therefore faster). In the video above you can see such a transition in action. The lerp input value is varyied to demonstrate how the resulting interpolated animation looks. Here the node also has an idle and a walk backward clip, so it can interpolate between even more states. Properties See common properties . Clips : A list of animation clips between which this animation node will interpolate. The node will only ever sample the two clips whose Position values are closest the the value provided through the Lerp input pin. Input Pins See common input pins . Lerp : This value determines which animation clips get mixed together. If the lerp value is in between two Position values of two clips, the output pose will be the linear interpolation of those two clips. If the lerp value is lower than the lowest Position value or higher than the highest, the output will be exactly that animation clip (there will be no extrapolation). Output Pins See common output pins . See Also Animation Controller Skeletal Animations Mix Clips 2D Node Play Single Clip Nodes","title":"Mix Clips 1D Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-mix1d/#mix-clips-1d-node","text":"The Mix Clips 1D animation controller node is used to linearly interpolate between a fixed set of animations. ![[anim-mix1d.gif]] Every animation clip has a position value in 1D space. The lerp input pin value determines how to interpolate those clips. The output pose will be either exactly one of those clips, or a mix between two clips, but never more than that. So if one clip is placed at position 0 and another at position 1 , you can fade from the first clip to the second by passing in a lerp value between 0 and 1 . The length of each clip may be different, however, the lookup positions across all clips are synchronized. That means if two clips are being mixed, and the first clip is sampled right at its middle, then the second clip will also be sampled at its middle, even if this is a completely different time offset (say 1 second versus 1.5 seconds). At which speed to move the sample position forwards, is determined by the length of the two animation clips that the lerp value is closest to. This node is useful if you have an action that can be done at different speeds and you want to cover all possible speeds with just a few different animation clips. The most intuitive example is a walk/run motion. You only need two animation clips, one for slow walking and one for fast running, and this node allows you to generate any speed in between through interpolation. For this to work, all animation clips have to follow the rule that they do the same motion at the same relative time offset. So in the case of a walk/run motion, both clips have to start with the same foot forwards, then move the other foot and finally move the first foot again, such that the animation is looped. The clips can have different lengths, though, so the run clip might be shorter than the walk clip (and therefore faster). In the video above you can see such a transition in action. The lerp input value is varyied to demonstrate how the resulting interpolated animation looks. Here the node also has an idle and a walk backward clip, so it can interpolate between even more states.","title":"Mix Clips 1D Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-mix1d/#properties","text":"See common properties . Clips : A list of animation clips between which this animation node will interpolate. The node will only ever sample the two clips whose Position values are closest the the value provided through the Lerp input pin.","title":"Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-mix1d/#input-pins","text":"See common input pins . Lerp : This value determines which animation clips get mixed together. If the lerp value is in between two Position values of two clips, the output pose will be the linear interpolation of those two clips. If the lerp value is lower than the lowest Position value or higher than the highest, the output will be exactly that animation clip (there will be no extrapolation).","title":"Input Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-mix1d/#output-pins","text":"See common output pins .","title":"Output Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-mix1d/#see-also","text":"Animation Controller Skeletal Animations Mix Clips 2D Node Play Single Clip Nodes","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-mix2d/","text":"Mix Clips 2D Node The Mix Clips 2D animation controller node is used to interpolate between a set of animations which are positioned in 2D space. You give it a 2D coordinate, and it will determine which animation clips are relevant and mix them together with proper weights depending on how close the coordinate is to each clip. The purpose of this node is to generate a continuous animation space from just a few discrete clips. This is often used for locomotion, where you only have animation clips for walking into a fixed number of directions and at certain speeds, but you'd like to be able to move a character into any direction and at any speed in between. ![[anim-mix2d.gif]] This node can generally be used to combine animations that can be thought of as having a position on a 2D plane. For example if you have animations for aiming forwards, to the left, right, up and down, you can use the mix 2D node to generate any pose in between. Be aware that the poses will be combined linearly, though. If the poses from two clips are too different, the result may not look very good. In this case it is best to create additional clips with in-between poses. How To Use You add multiple animation clips and give each clip a position ( X and Y ). As with the mix clips 1D node , the playback of all clips is synchronized, meaning that the length of each clip may differ, but they will be played back such that they start and end in unison. That means your clips must be authored accordingly, so for example for locomotion all clips should start with the left foot forwards, then move the right foot forwards, then the left again. From that point on the clips will be looped. What the coordinates represent is up to you. For locomotion you could say that X represents left/right movement and Y forwards/backwards. You would then position a walk left clip at (-1, 0) a walk right clip at (+1, 0) a walk forward clip at (0, +1) and a run forward clip at (0, +2) . Through the X and Y input pins you provide a 2D coordinate. During testing you may hook this up directly to an input node , though later you'll probably need more control. The node will then take that input coordinate to decide which clips should be used with what influence, and mix them together to a single output pose. Properties See common properties . InputResponse : A time duration over which changes to the X and Y input values are applied. This prevents sudden extreme changes. For example when X and Y are connected to physical buttons, which are just turned on or off , the final animation would jerk between those extremes. In a finished game you may want to smooth out the input yourself, but for starters this node can do a basic smoothing of the input values for you. Thus, if an input value switches from 1 to 0 , an InputResponse of 50ms means that the used value will transition smoothly towards 0 over that amount of time and thus the output pose will also transition smoothly. CenterClip : An optional clip for the position (0, 0) . This clip is always played at its own speed and not synchronized to the other clips. It is meant for idle state animations. It may be much longer and contain many subtle motions for variation. If such behavior is not desired and instead you want the center clip to be synchronized with the rest, you can instead place a clip at position (0, 0) as well. Clips : The various clips. Each clip must have a unique 2D position assigned. Input Pins See common input pins . X , Y : The input coordinate to select how to blend the Clips . It directly relates to the clips` positions. Output Pins See common output pins . See Also Animation Controller Skeletal Animations Mix Clips 1D Node Play Single Clip Nodes","title":"Mix Clips 2D Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-mix2d/#mix-clips-2d-node","text":"The Mix Clips 2D animation controller node is used to interpolate between a set of animations which are positioned in 2D space. You give it a 2D coordinate, and it will determine which animation clips are relevant and mix them together with proper weights depending on how close the coordinate is to each clip. The purpose of this node is to generate a continuous animation space from just a few discrete clips. This is often used for locomotion, where you only have animation clips for walking into a fixed number of directions and at certain speeds, but you'd like to be able to move a character into any direction and at any speed in between. ![[anim-mix2d.gif]] This node can generally be used to combine animations that can be thought of as having a position on a 2D plane. For example if you have animations for aiming forwards, to the left, right, up and down, you can use the mix 2D node to generate any pose in between. Be aware that the poses will be combined linearly, though. If the poses from two clips are too different, the result may not look very good. In this case it is best to create additional clips with in-between poses.","title":"Mix Clips 2D Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-mix2d/#how-to-use","text":"You add multiple animation clips and give each clip a position ( X and Y ). As with the mix clips 1D node , the playback of all clips is synchronized, meaning that the length of each clip may differ, but they will be played back such that they start and end in unison. That means your clips must be authored accordingly, so for example for locomotion all clips should start with the left foot forwards, then move the right foot forwards, then the left again. From that point on the clips will be looped. What the coordinates represent is up to you. For locomotion you could say that X represents left/right movement and Y forwards/backwards. You would then position a walk left clip at (-1, 0) a walk right clip at (+1, 0) a walk forward clip at (0, +1) and a run forward clip at (0, +2) . Through the X and Y input pins you provide a 2D coordinate. During testing you may hook this up directly to an input node , though later you'll probably need more control. The node will then take that input coordinate to decide which clips should be used with what influence, and mix them together to a single output pose.","title":"How To Use"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-mix2d/#properties","text":"See common properties . InputResponse : A time duration over which changes to the X and Y input values are applied. This prevents sudden extreme changes. For example when X and Y are connected to physical buttons, which are just turned on or off , the final animation would jerk between those extremes. In a finished game you may want to smooth out the input yourself, but for starters this node can do a basic smoothing of the input values for you. Thus, if an input value switches from 1 to 0 , an InputResponse of 50ms means that the used value will transition smoothly towards 0 over that amount of time and thus the output pose will also transition smoothly. CenterClip : An optional clip for the position (0, 0) . This clip is always played at its own speed and not synchronized to the other clips. It is meant for idle state animations. It may be much longer and contain many subtle motions for variation. If such behavior is not desired and instead you want the center clip to be synchronized with the rest, you can instead place a clip at position (0, 0) as well. Clips : The various clips. Each clip must have a unique 2D position assigned.","title":"Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-mix2d/#input-pins","text":"See common input pins . X , Y : The input coordinate to select how to blend the Clips . It directly relates to the clips` positions.","title":"Input Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-mix2d/#output-pins","text":"See common output pins .","title":"Output Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-mix2d/#see-also","text":"Animation Controller Skeletal Animations Mix Clips 1D Node Play Single Clip Nodes","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-modelspace/","text":"Local To Model Pose Node The Local To Model Pose node is used to convert a pose from local space to model space . Animation clips store animation data in so called local space . That means that every bone stores its transformation (rotation, position and scale) relative to its parent bone. Therefore the actual full position of any given bone is not readily available, but instead has to be computed by concatenating the transformations of all ancestor bones. Having the data in this representation is advantageous for mixing multiple animations together, which is why this is done in local space. However, to apply a pose to a mesh, the final position and rotation has to be known for every bone. Additionally, operations like inverse kinematics (IK) also have to be done in model space. Therefore this node is necessary to convert a pose from one representation to the other, before it can be passed to an output node . Input Pins LocalPose : A single pose in local space. Output Pins ModelPose : The converted pose in model space. See Also Animation Controller Skeletal Animations","title":"Local To Model Pose Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-modelspace/#local-to-model-pose-node","text":"The Local To Model Pose node is used to convert a pose from local space to model space . Animation clips store animation data in so called local space . That means that every bone stores its transformation (rotation, position and scale) relative to its parent bone. Therefore the actual full position of any given bone is not readily available, but instead has to be computed by concatenating the transformations of all ancestor bones. Having the data in this representation is advantageous for mixing multiple animations together, which is why this is done in local space. However, to apply a pose to a mesh, the final position and rotation has to be known for every bone. Additionally, operations like inverse kinematics (IK) also have to be done in model space. Therefore this node is necessary to convert a pose from one representation to the other, before it can be passed to an output node .","title":"Local To Model Pose Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-modelspace/#input-pins","text":"LocalPose : A single pose in local space.","title":"Input Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-modelspace/#output-pins","text":"ModelPose : The converted pose in model space.","title":"Output Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-modelspace/#see-also","text":"Animation Controller Skeletal Animations","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-output/","text":"Output Nodes Every animation controller must have exactly one output node. Only animation poses that are ultimately connected to an output node will affect the animated mesh . This allows you to quickly deactivate an entire part of the graph simply by removing the connection to the output node. Nodes that are not connected to the output are not evaluated at runtime and therefore don't cost any performance. Model Pose Output Node The Model Pose Output node is currently the only available type of output node. Input Pins ModelPose : The one pose to use as the output. This pose has to be in model space. It is thefore quite common that the input is directly connected to a Local To Model Pose node . RotationZ : This number value adds angular root motion to the final pose. This enables the animation to change the rotation of the game object on which it is played. This is mainly convenient for simpler use cases and for prototyping. In more complex scenarios you may prefer control the object's orientation with custom code . See Also Animation Controller Skeletal Animations","title":"Output Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-output/#output-nodes","text":"Every animation controller must have exactly one output node. Only animation poses that are ultimately connected to an output node will affect the animated mesh . This allows you to quickly deactivate an entire part of the graph simply by removing the connection to the output node. Nodes that are not connected to the output are not evaluated at runtime and therefore don't cost any performance.","title":"Output Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-output/#model-pose-output-node","text":"The Model Pose Output node is currently the only available type of output node.","title":"Model Pose Output Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-output/#input-pins","text":"ModelPose : The one pose to use as the output. This pose has to be in model space. It is thefore quite common that the input is directly connected to a Local To Model Pose node . RotationZ : This number value adds angular root motion to the final pose. This enables the animation to change the rotation of the game object on which it is played. This is mainly convenient for simpler use cases and for prototyping. In more complex scenarios you may prefer control the object's orientation with custom code .","title":"Input Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-output/#see-also","text":"Animation Controller Skeletal Animations","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-playclip/","text":"Play Single Clip Nodes Animation controller graphs provide multiple node types that are used for sampling animation clips. The play single clip node is the most basic one. This node samples an animation clip and outputs the respective animation pose. It is thus used for typical playback of a single animation either once or in a loop. For added variety you may reference multiple animation clips, in which case the node will randomly pick one of them for playback. ![[skeletal-anim.gif]] Common Properties All animation controller nodes that sample animation clips share these properties: Loop : If enabled the node will loop playback while its Active pin is triggered. Which clip exactly gets looped depends on the node. The play single clip node for example just loops playback in general, but randomly picks a different clip in every iteration. The play clip sequence node on the other hand will loop its middle clip instead. ApplyRootMotion : If enabled, the node will compute the root motion from the sampled clips and forward this to the animation controller component , which may apply this to the game object's position. PlaybackSpeed : Adjusts the speed with which the animations are sampled. FadeIn , FadeOut : The time (in seconds) that it should take to fade an animation in and out. If this is set to 0 , the animation will immediately switch on or off when the node gets (de-)activated. If the time is non-zero, the animation will gradually fade in or out over this duration. These values are very important to cross-fade from one animation to another. For example, if a character is playing an idle animation and then should transition to walk , the walk animation should have a short fade in duration and the idle animation should have a fade out duration. The shorter of the two durations determines how long the two animations are being cross-faded to transition smoothly from one state to the other. Additionally, if the fade in and out values are longer than the animation itself, the animation will be forced to play back for that amount of time, typically by extending the use of the last keyframe. This can be very useful if you use an animation that only contains a static pose, for example a pose for aiming. With a FadeIn of 200ms this single pose will be held for 200ms but gradually faded in. That leads to a character slowly raising their hand, instead of immediately having the hand raised. The same is true for the FadeOut property, which again can be used to slowly lower the hand by fading out the aim pose over a longer duration. ImmediateFadeIn , ImmediateFadeOut : The playback starts when the Active pin is triggered. If ImmediateFadeOut is off, the clip will be played back to its very end before it is allowed to fade out. If ImmediateFadeOut is on, however, the animation will be faded out right away when the Active pin stops being triggered. If ImmediateFadeIn is off, once a node starts fading out, it will continue fading out until it is fully off, no matter what the Active pin state is. If ImmediateFadeIn is on, a node that has started fading out may immediately fade in again if the Active pin gets triggered again. These values determine how responsive animation playback is in regards to input changes. Immediately fading in and out can drastically reduce delay between input and a visual reaction, but may also only work well with certain animations. Properties Clips : One or multiple animation clips to play. If more than one is added, the node will pick one at random in every loop iteration. Common Input Pins Many animation controller nodes have some or all of these input pins: Active : This pin determines whether the node samples its animation clips at all . Once it gets triggered in a frame, the node starts to sample its animation clips, fades them in etc. If Loop is enabled, the playback will repeat as long as the Active pin is triggered. Once the pin is not triggered anymore, the node will start to fade out its animations. Either right away ( ImmediateFadeOut on) or when it reaches the end of the currently playing clip ( ImmediateFadeOut off). Weights : If this pin is connected to a bone weight node , then the sampled animation clip is only applied to that part of the character. This is used to limit playback of an animation to selected body parts. Speed : This pin adjusts the overall playback speed. Input Pins ClipIndex : If the node has multiple Clips set, exactly which one will be played back can be controlled through this pin. With ClipIndex set to 0 the first clip is used exclusively, with ClipIndex set to 1 only the second clip is used, and so on. If ClipIndex is not connected or set to a negative value, a random clip is used. Common Output Pins Many animation controller nodes have some or all of these output pins: LocalPose : The final pose from the sampled animation clips is output through this pin. This has to be passed to a combine poses node or a local to model pose node . OnFadeOut : This pin gets triggered for a single frame once the node changes its internal state to fade out the animation (affected by ImmediateFadeOut and FadeOut ). This is typically a good time to start fading in another animation to take over. This pin is guaranteed to get triggered, even if the FadeOut time is zero. See Also Animation Controller Skeletal Animations Bone Weight Nodes Play Clip Sequence Node Mix Clips 2D Node","title":"Play Single Clip Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-playclip/#play-single-clip-nodes","text":"Animation controller graphs provide multiple node types that are used for sampling animation clips. The play single clip node is the most basic one. This node samples an animation clip and outputs the respective animation pose. It is thus used for typical playback of a single animation either once or in a loop. For added variety you may reference multiple animation clips, in which case the node will randomly pick one of them for playback. ![[skeletal-anim.gif]]","title":"Play Single Clip Nodes"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-playclip/#common-properties","text":"All animation controller nodes that sample animation clips share these properties: Loop : If enabled the node will loop playback while its Active pin is triggered. Which clip exactly gets looped depends on the node. The play single clip node for example just loops playback in general, but randomly picks a different clip in every iteration. The play clip sequence node on the other hand will loop its middle clip instead. ApplyRootMotion : If enabled, the node will compute the root motion from the sampled clips and forward this to the animation controller component , which may apply this to the game object's position. PlaybackSpeed : Adjusts the speed with which the animations are sampled. FadeIn , FadeOut : The time (in seconds) that it should take to fade an animation in and out. If this is set to 0 , the animation will immediately switch on or off when the node gets (de-)activated. If the time is non-zero, the animation will gradually fade in or out over this duration. These values are very important to cross-fade from one animation to another. For example, if a character is playing an idle animation and then should transition to walk , the walk animation should have a short fade in duration and the idle animation should have a fade out duration. The shorter of the two durations determines how long the two animations are being cross-faded to transition smoothly from one state to the other. Additionally, if the fade in and out values are longer than the animation itself, the animation will be forced to play back for that amount of time, typically by extending the use of the last keyframe. This can be very useful if you use an animation that only contains a static pose, for example a pose for aiming. With a FadeIn of 200ms this single pose will be held for 200ms but gradually faded in. That leads to a character slowly raising their hand, instead of immediately having the hand raised. The same is true for the FadeOut property, which again can be used to slowly lower the hand by fading out the aim pose over a longer duration. ImmediateFadeIn , ImmediateFadeOut : The playback starts when the Active pin is triggered. If ImmediateFadeOut is off, the clip will be played back to its very end before it is allowed to fade out. If ImmediateFadeOut is on, however, the animation will be faded out right away when the Active pin stops being triggered. If ImmediateFadeIn is off, once a node starts fading out, it will continue fading out until it is fully off, no matter what the Active pin state is. If ImmediateFadeIn is on, a node that has started fading out may immediately fade in again if the Active pin gets triggered again. These values determine how responsive animation playback is in regards to input changes. Immediately fading in and out can drastically reduce delay between input and a visual reaction, but may also only work well with certain animations.","title":"Common Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-playclip/#properties","text":"Clips : One or multiple animation clips to play. If more than one is added, the node will pick one at random in every loop iteration.","title":"Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-playclip/#common-input-pins","text":"Many animation controller nodes have some or all of these input pins: Active : This pin determines whether the node samples its animation clips at all . Once it gets triggered in a frame, the node starts to sample its animation clips, fades them in etc. If Loop is enabled, the playback will repeat as long as the Active pin is triggered. Once the pin is not triggered anymore, the node will start to fade out its animations. Either right away ( ImmediateFadeOut on) or when it reaches the end of the currently playing clip ( ImmediateFadeOut off). Weights : If this pin is connected to a bone weight node , then the sampled animation clip is only applied to that part of the character. This is used to limit playback of an animation to selected body parts. Speed : This pin adjusts the overall playback speed.","title":"Common Input Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-playclip/#input-pins","text":"ClipIndex : If the node has multiple Clips set, exactly which one will be played back can be controlled through this pin. With ClipIndex set to 0 the first clip is used exclusively, with ClipIndex set to 1 only the second clip is used, and so on. If ClipIndex is not connected or set to a negative value, a random clip is used.","title":"Input Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-playclip/#common-output-pins","text":"Many animation controller nodes have some or all of these output pins: LocalPose : The final pose from the sampled animation clips is output through this pin. This has to be passed to a combine poses node or a local to model pose node . OnFadeOut : This pin gets triggered for a single frame once the node changes its internal state to fade out the animation (affected by ImmediateFadeOut and FadeOut ). This is typically a good time to start fading in another animation to take over. This pin is guaranteed to get triggered, even if the FadeOut time is zero.","title":"Common Output Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-playclip/#see-also","text":"Animation Controller Skeletal Animations Bone Weight Nodes Play Clip Sequence Node Mix Clips 2D Node","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-sequence/","text":"Play Clip Sequence Node The play clip sequence node is similar to the play single clip node , however, instead of playing just a single clip (looped), the sequence node plays at least three clips in a row. One to enter an animation state, one (looped) animation while it stays active, and one to exit the animation state. Such sequences are common for actions such as jumping or climbing a ladder. The start clip transitions the character from a start state, such as idle or walking into the new state, such as jumping . The middle clip is then played as long as the jumping state needs to continue, and once the character hits the ground again, the end clip is played to transition back. Properties See common properties . StartClip : The animation clip to start with. This clip should end on a keyframe from where the MiddleClips can continue seemlessly. MiddleClips : One or multiple animation clips to play after the StartClip . Typically these will get looped as long as the node is Active . If more than one clip is set, which one to play can be selected using the MiddleClipIndex pin. Otherwise a random one will be selected on every iteration. EndClip : The clip to play after the middle clips are finished. If the node is looped this will only happen once the Active pin is not triggered anymore. Input Pins See common input pins . MiddleClipIndex : This pin can be used to select which of the MiddleClips to play next. In the video above this is used to select whether the gun should get fired or not. Output Pins See common output pins . OnNextClip : This pin will get triggered every time a clip finishes and the next middle or end clip starts. This can be used to know for example when the start phase has finished. PlayingClipIndex : This pin outputs the index of the currently playing clip. This value is 0 to N-1 for any of the N middle clips. It will be -1 when the start clip is playing and -2 when the end clip is playing. See Also Animation Controller Skeletal Animations","title":"Play Clip Sequence Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-sequence/#play-clip-sequence-node","text":"The play clip sequence node is similar to the play single clip node , however, instead of playing just a single clip (looped), the sequence node plays at least three clips in a row. One to enter an animation state, one (looped) animation while it stays active, and one to exit the animation state. Such sequences are common for actions such as jumping or climbing a ladder. The start clip transitions the character from a start state, such as idle or walking into the new state, such as jumping . The middle clip is then played as long as the jumping state needs to continue, and once the character hits the ground again, the end clip is played to transition back.","title":"Play Clip Sequence Node"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-sequence/#properties","text":"See common properties . StartClip : The animation clip to start with. This clip should end on a keyframe from where the MiddleClips can continue seemlessly. MiddleClips : One or multiple animation clips to play after the StartClip . Typically these will get looped as long as the node is Active . If more than one clip is set, which one to play can be selected using the MiddleClipIndex pin. Otherwise a random one will be selected on every iteration. EndClip : The clip to play after the middle clips are finished. If the node is looped this will only happen once the Active pin is not triggered anymore.","title":"Properties"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-sequence/#input-pins","text":"See common input pins . MiddleClipIndex : This pin can be used to select which of the MiddleClips to play next. In the video above this is used to select whether the gun should get fired or not.","title":"Input Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-sequence/#output-pins","text":"See common output pins . OnNextClip : This pin will get triggered every time a clip finishes and the next middle or end clip starts. This can be used to know for example when the start phase has finished. PlayingClipIndex : This pin outputs the index of the currently playing clip. This value is 0 to N-1 for any of the N middle clips. It will be -1 when the start clip is playing and -2 when the end clip is playing.","title":"Output Pins"},{"location":"animation/skeletal-animation/animation-controller/anim-nodes-sequence/#see-also","text":"Animation Controller Skeletal Animations","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-asset/","text":"Animation Controller Asset The animation controller asset is used to configure complex animations. See the animation controller chapter for a conceptiual description. Animation Controller Graph Animation controllers are set up as graph structures. Nodes represent actions. Data flows from left to right. Nodes have input pins and output pins which represent different kinds of data, such as trigger states ( on/off ), number values, animation poses and bone weights. The goal of an animation controller is to sample a number of animation clips , combine them together, and generate a final pose which can then be applied to an animated mesh . Creating Nodes Right click into the main area to open a context menu. Here you select which nodes to add to the graph. Every graph requires at least an output node , a model space conversion node and a node to sample a clip, for example a play single clip node . Once the graph samples two different clips you also need a combine poses node . Connecting Nodes You connect nodes through their pins. Just left click and drag from one output pin to another input pin. The UI will display which pins can be connected once you start dragging. Since every pin represents a certain data type, only compatible pin types may get connected. Some pins allow to connect to multiple other pins, or have multiple incoming connections. If a pin does not allow this, creating a new connection removes previous connections automatically. Nodes that are not ultimately connected to the output node, will not have any effect. Node Properties Nodes may additionally have properties . These are displayed in the property pane when a node is selected. See the documentation for the different node types for more detailed descriptions. Using Animation Controller Assets Once an animation controller asset is set up, it can be applied to an animated mesh by adding an animation controller component to the same game object. The component will evaluate the graph in every update, and send the final animation pose to the animated mesh. To control what the animation controller does, you may also need a blackboard for storing state, and potentially a script component to decide when which animation should be active. See Also Skeletal Animations Animation Controller Component Blackboards Simple Animation Component","title":"Animation Controller Asset"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-asset/#animation-controller-asset","text":"The animation controller asset is used to configure complex animations. See the animation controller chapter for a conceptiual description.","title":"Animation Controller Asset"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-asset/#animation-controller-graph","text":"Animation controllers are set up as graph structures. Nodes represent actions. Data flows from left to right. Nodes have input pins and output pins which represent different kinds of data, such as trigger states ( on/off ), number values, animation poses and bone weights. The goal of an animation controller is to sample a number of animation clips , combine them together, and generate a final pose which can then be applied to an animated mesh .","title":"Animation Controller Graph"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-asset/#creating-nodes","text":"Right click into the main area to open a context menu. Here you select which nodes to add to the graph. Every graph requires at least an output node , a model space conversion node and a node to sample a clip, for example a play single clip node . Once the graph samples two different clips you also need a combine poses node .","title":"Creating Nodes"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-asset/#connecting-nodes","text":"You connect nodes through their pins. Just left click and drag from one output pin to another input pin. The UI will display which pins can be connected once you start dragging. Since every pin represents a certain data type, only compatible pin types may get connected. Some pins allow to connect to multiple other pins, or have multiple incoming connections. If a pin does not allow this, creating a new connection removes previous connections automatically. Nodes that are not ultimately connected to the output node, will not have any effect.","title":"Connecting Nodes"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-asset/#node-properties","text":"Nodes may additionally have properties . These are displayed in the property pane when a node is selected. See the documentation for the different node types for more detailed descriptions.","title":"Node Properties"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-asset/#using-animation-controller-assets","text":"Once an animation controller asset is set up, it can be applied to an animated mesh by adding an animation controller component to the same game object. The component will evaluate the graph in every update, and send the final animation pose to the animated mesh. To control what the animation controller does, you may also need a blackboard for storing state, and potentially a script component to decide when which animation should be active.","title":"Using Animation Controller Assets"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-asset/#see-also","text":"Skeletal Animations Animation Controller Component Blackboards Simple Animation Component","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-component/","text":"Animation Controller Component The animation controller component is used to apply complex animation playback and blending functionality to an animated mesh . It is the big brother of the simple animation component . Instead of playing just a single animation clip, it uses an animation controller asset to determine the animation pose. The component itself doesn't do much, other than updating the animation pose and sending it to the animated mesh. For how to control the animation playback, please see the Animation Controller chapter. Component Properties AnimController : The animation controller to use. RootMotionMode : Selects how root motion is applied to the owning game object. See Also Skeletal Animations Simple Animation Component Animation Controller","title":"Animation Controller Component"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-component/#animation-controller-component","text":"The animation controller component is used to apply complex animation playback and blending functionality to an animated mesh . It is the big brother of the simple animation component . Instead of playing just a single animation clip, it uses an animation controller asset to determine the animation pose. The component itself doesn't do much, other than updating the animation pose and sending it to the animated mesh. For how to control the animation playback, please see the Animation Controller chapter.","title":"Animation Controller Component"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-component/#component-properties","text":"AnimController : The animation controller to use. RootMotionMode : Selects how root motion is applied to the owning game object.","title":"Component Properties"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-component/#see-also","text":"Skeletal Animations Simple Animation Component Animation Controller","title":"See Also"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-overview/","text":"Animation Controller Animating characters is a complex task. Even simple creatures typically already need tens of different animation clips for locomotion and basic actions. However, having the animation clips is not enough, they also need to be played on the animated mesh in such a way that animations blend over nicely and play perfectly in sequence. Additionally you may need to apply an animation only to a certain part of the body. Animations should fade in and out smoothly as they are activated and deactivated, and certain clips need to be synchronized to achieve the desired effect. An animation controller is used to configure how each animation clip of a character should behave when it is actively played on the mesh. Animation controllers are the basis for complex animation playback. They also provide some very basic functionality for logic and math, such that one can build simple state machines. For more complex decisions which animations should play when, use custom code . Creating and Using Animation Controllers Animation controllers are configured through the animation controller asset type . To apply the output pose of an animation controller to an animated mesh , add an animation controller component to the same game object . The controller will send the output pose to the mesh every frame, but only while the scene is simulated . To control what an animation controller will output, you typically also need a blackboard . The blackboard is used to store state. Scripts or other custom code decide which animations should be played and write that state to the blackboard. The animation controller in turn reads state from the blackboard and then activates the desired animation clip playback. The controller can also write back state to the blackboard, for example to communicate back that an animation clip has finished playing. Animation Controller Concept The following image shows a very basic animation controller: The flow of information is from left to right. On the far left side the two green nodes are used to read state from the blackboard . Here we read the blackboard values PlayIdle and PlayWave to see which animation clips should get played. The pink output pins are trigger pins , meaning they can be active ( triggered ) or inactive . When the read value is 1 (in this case) the pins are set to triggered which then activates the connected nodes to the right. The two light blue nodes are used to sample animation clips . There are different ways how animation clips can be played, but here we only use very basic (looped) playback. When the PlayIdle blackboard value is set, the Idle clip will be sampled. When the PlayWave blackboard value is set, the Wave clip will get sampled. Any combination is possible, so both clips can be played at the same time. The sampling nodes have a LocalPose output pin. This pin represents the animation pose that was determined. The pin also carries information about weighting the pose. That means when the clip playback was just started, the pose may still be fading and shouldn't immediately have full influence. Similarly, if a pose shall only be applied to a certain body part these bone weights are also included here and will be forwarded to any following node. In the middle of the graph the combine poses node is used to gather multiple poses and turn them into one. This node uses the aforementioned bone weights and overall pose weight to blend all the available poses together. Note that the blue nodes output local poses . That means the pose data is in a certain format. Data in this format can be used for certain operations, however, the data cannot be output in this format. Therefore the next step is to convert the pose from local space to model space . Once the data is in model space there are other operations that can be done with it. In this graph, though, the converted pose is simply forwarded to the output node . This is always the final step. Summary The animation controller uses a graph based workflow to let you visually configure how animation clips are combined. Nodes have inputs to control their behavior, and they output data or state that can (or must) be forwarded to the next step in the pipeline, until an animation pose ultimately reaches the output. The system is intelligent enough to optimize away operations that don't affect the output. You typically control which animations are played when through a blackboard . For quick prototyping you can also use the input nodes to get certain input data directly into the graph. Simple animation state machines can be built directly in the animation controller graph using the logic and math nodes as well as the blackboard nodes . For more complex logic you should use custom code . See Also Skeletal Animations Animation Controller Asset Animation Controller Component Animated Mesh Component","title":"Animation Controller"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-overview/#animation-controller","text":"Animating characters is a complex task. Even simple creatures typically already need tens of different animation clips for locomotion and basic actions. However, having the animation clips is not enough, they also need to be played on the animated mesh in such a way that animations blend over nicely and play perfectly in sequence. Additionally you may need to apply an animation only to a certain part of the body. Animations should fade in and out smoothly as they are activated and deactivated, and certain clips need to be synchronized to achieve the desired effect. An animation controller is used to configure how each animation clip of a character should behave when it is actively played on the mesh. Animation controllers are the basis for complex animation playback. They also provide some very basic functionality for logic and math, such that one can build simple state machines. For more complex decisions which animations should play when, use custom code .","title":"Animation Controller"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-overview/#creating-and-using-animation-controllers","text":"Animation controllers are configured through the animation controller asset type . To apply the output pose of an animation controller to an animated mesh , add an animation controller component to the same game object . The controller will send the output pose to the mesh every frame, but only while the scene is simulated . To control what an animation controller will output, you typically also need a blackboard . The blackboard is used to store state. Scripts or other custom code decide which animations should be played and write that state to the blackboard. The animation controller in turn reads state from the blackboard and then activates the desired animation clip playback. The controller can also write back state to the blackboard, for example to communicate back that an animation clip has finished playing.","title":"Creating and Using Animation Controllers"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-overview/#animation-controller-concept","text":"The following image shows a very basic animation controller: The flow of information is from left to right. On the far left side the two green nodes are used to read state from the blackboard . Here we read the blackboard values PlayIdle and PlayWave to see which animation clips should get played. The pink output pins are trigger pins , meaning they can be active ( triggered ) or inactive . When the read value is 1 (in this case) the pins are set to triggered which then activates the connected nodes to the right. The two light blue nodes are used to sample animation clips . There are different ways how animation clips can be played, but here we only use very basic (looped) playback. When the PlayIdle blackboard value is set, the Idle clip will be sampled. When the PlayWave blackboard value is set, the Wave clip will get sampled. Any combination is possible, so both clips can be played at the same time. The sampling nodes have a LocalPose output pin. This pin represents the animation pose that was determined. The pin also carries information about weighting the pose. That means when the clip playback was just started, the pose may still be fading and shouldn't immediately have full influence. Similarly, if a pose shall only be applied to a certain body part these bone weights are also included here and will be forwarded to any following node. In the middle of the graph the combine poses node is used to gather multiple poses and turn them into one. This node uses the aforementioned bone weights and overall pose weight to blend all the available poses together. Note that the blue nodes output local poses . That means the pose data is in a certain format. Data in this format can be used for certain operations, however, the data cannot be output in this format. Therefore the next step is to convert the pose from local space to model space . Once the data is in model space there are other operations that can be done with it. In this graph, though, the converted pose is simply forwarded to the output node . This is always the final step.","title":"Animation Controller Concept"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-overview/#summary","text":"The animation controller uses a graph based workflow to let you visually configure how animation clips are combined. Nodes have inputs to control their behavior, and they output data or state that can (or must) be forwarded to the next step in the pipeline, until an animation pose ultimately reaches the output. The system is intelligent enough to optimize away operations that don't affect the output. You typically control which animations are played when through a blackboard . For quick prototyping you can also use the input nodes to get certain input data directly into the graph. Simple animation state machines can be built directly in the animation controller graph using the logic and math nodes as well as the blackboard nodes . For more complex logic you should use custom code .","title":"Summary"},{"location":"animation/skeletal-animation/animation-controller/animation-controller-overview/#see-also","text":"Skeletal Animations Animation Controller Asset Animation Controller Component Animated Mesh Component","title":"See Also"},{"location":"appendix/color-spaces/","text":"Color Spaces See Also","title":"Color Spaces"},{"location":"appendix/color-spaces/#color-spaces","text":"","title":"Color Spaces"},{"location":"appendix/color-spaces/#see-also","text":"","title":"See Also"},{"location":"appendix/container-usage/","text":"Container Usage Guidelines Plasma Engine has the following container classes: plStaticArray plHybridArray plDynamicArray plStaticRingBuffer plDeque plList plMap plSet plHashTable plArrayMap The following containers store their data as contiguous arrays: plStaticArray plHybridArray plDynamicArray plStaticRingBuffer plArrayMap The following containers are built on top of plDeque and thus share some performance characteristics: plList plMap plSet When to use which Container Type Arrays General Guidelines For arrays, plHybridArray , plDynamicArray and plDeque are the most important containers. If you know or have a guess how much data you will need, always use this information in a call to Reserve to ensure that the containers can allocate data once (or at least much less), and do not need to reallocate several times. Never remove an element in between (using RemoveAt ), unless there is really no other way (and hopefully your array is small). Prefer RemoveAtAndSwap to replace the removed element by the last element in the array instead (this will destroy the order though). Similarly, never insert elements anywhere else than at the end. The only exception is plDeque , which is very efficient at insertion and removal of elements at both ends. plHybridArray plHybridArray uses an internal fixed size cache (which you can specify as a template argument). When you create an plHybridArray on the stack, that data is also allocated on the stack. This is the most important container for writing performance critical yet safe code. plHybridArray allows to implement many algorithms without the need to use dynamic allocations. Prefer plHybridArray when you have a use case where you typically have a small set of elements, but it might be larger in some situations. plHybridArray will give you the performance of an immediately available array on the stack, combined with the safety of a dynamically allocated array on the heap, as it will reallocate data dynamically, whenever its internal storage is insufficient. However, be careful not to make the internal buffer too large. When creating plHybridArrays on the stack, you should not use more than a few KB for the internal cache, as you increase the risk for stack overflows. You should usually try to stay below (2KB / sizeof(Type)) for the number of elements in the plHybridArray cache. Note: plHybridArray is derived from plDynamicArray , that means every function that takes an plDynamicArray (even for writing output to), can be given an plHybridArray . plDynamicArray plDynamicArray always allocates its data on the heap. The upside is, that it has a very low memory overhead, as long as it is empty, and it can handle any number of elements. Prefer plDynamicArray if your working set is generally larger and when you know how many elements it needs to hold before you fill it up. Use Reserve or SetCount BEFORE you start adding data to it, to prevent unnecessary reallocation later, as those are very costly. Also prefer plDynamicArray if the memory overhead in the empty state is of concern. plDeque plDeque stores its data as several chunks of contiguous arrays. An additional \"redirection array\" is used to know how to index into these chunks. plDeque requires one pointer indirection to make a lookup into its data. The deque is the only array container that does not store its data in one contiguous block of memory. Be aware of that, you cannot memcpy or memcmp data in a deque with more than one element, as the next element might be stored somewhere completely different. However a deque NEVER relocates an existing element in memory, either. That means once an element is inserted into a deque, you can safely store pointers to it, as it will not move around in memory (unless it is deleted, of course). Deques allocate their data in chunks. So whenever the memory is insufficient, a new chunk that can hold a fixed number of elements is allocated. One chunk is typically 4 KB in size. That means if you store floats in a deque, one chunk can hold 1000 values. Thus this is the 'minimum' memory usage of a deque, unless it is completely empty still. As such plDeques are very efficient for iteration (most of the data is contiguous) and they are very dynamic, as their size can grow dynamically without the need to reallocate and copy previous data, as the other array types would need to do. Therefore prefer plDeques whenever you need to have nearly the performance of an array, but have very dynamic data sets, that are difficult to predict in size, or that vary all the time. As deques are optimized for insertion and removal at both ends, plDeques are perfectly suited as fifo queues and dynamic ring-buffers. The memory overhead of plDeques is rather high, so do not use it for small data-sets (here plHybridArray is typically the best container). plDeques are also very well suited, whenever you have large objects to store that are very costly to construct or copy around, and you want to prevent those operations by all means, such that you do not want a container reallocation to trigger that. plStaticArray This is a container that only stores a static array internally and cannot resize itself to be larger than that. Use this in code that has a definite upper limit of elements and whenever you must prevent the usage of any allocator by all means (such as for global variables). Typically there should be no need for this container, as plHybridArray delivers the same performance advantages and the safety of reallocating to the required size dynamically. plStaticRingBuffer Use this when you need a ring-buffer that shall have a fixed size. Use plDeque if you need a dynamically resizing ring-buffer. Lists There is only one implementation of a doubly linked list: plList plList is internally built on top of a plDeque . As such the memory requirements are the same. plList is optimized for inserting and removing objects frequently. Iterating over its elements might be slow due to excessive cache misses. You should typically not use plList in code that is performance critical. Use it when you have data-sets of unpredictable size that need to be sorted or rearranged very dynamically. For example when you read a complex data set which then needs to be processed and sorted by different criteria, which might delete and insert elements at random positions, then prefer an plList . However, once that step is finished, you should copy your sorted data into some array container for faster access. Nodes in an plList are never relocated in memory, as such iterators stay valid as long as the element is still alive. Associative Containers plMap and plSet Both containers are basically the same, except that plMap stores a 'value' for each 'key', whereas a set only stores 'keys'. Use plMap whenever you need to be able to look up an entry with a key. Use plSet whenever you simply need to know whether some element is present or to merge data-sets down to all the unique elements. plMap and plSet are built on top of plDeque , similar to plList , that means they are quite heavy-weight in their memory consumption, however they can grow in size efficiently. As with plList , iterators (and pointers) to elements stay valid, as long as the element is alive, i.e. the data is never relocated. Insertion, lookup and removal are all O(log n) operations, since they are red-black trees internally and thus always perfectly balanced. plMap and plSet are well suited for very dynamic data sets (where a lot of insertions and removals are done, while also using it for lookup). If you have a use-case where you insert once and then lookup often, a sorted array, such as plArrayMap , or an plHashTable might be more efficient. plMap and plSet only require a simple comparer to be able to sort elements in a strictly weak ordering. As such they are well suited to handle objects that can be difficult to be hashed. Note that the nodes in the Map/Set each contain one element of their key/value type and those are stored in an plDeque . As such, when you put an plHybridArray (or an plString ) into an plMap , only one allocation is needed to allocate all the memory for a chunk (in the plDeque ) of data, which holds a large number of nodes, which already embed the data of their keys/values (e.g. plHybridArray ). Thus you can get away with very few memory allocations. If however you store an plDynamicArray in an plMap , each element still needs to allocate its own internal storage, which means you will get one additional allocation per element. plArrayMap This container provides similar functionality as plMap but should be more efficient in scenarios where elements are looked up more often than they are inserted or removed. The implementation simply uses an array that is kept sorted, such that lookups can be done in a more cache friendly manner. If all you need is an associative container and your use case consists of changing the container infrequently, but looking up elements frequently (which is very often the case), you should prefer this container. Note, however, that this container will rearrange elements in memory whenever it needs to be sorted. In contrast an plMap guarantees that elements never move in memory, allowing to store pointers to the memory locations. Likewise the iterators of an plMap stay valid as long as an elements resides in the map. For plArrayMap this is not true, the index at which an element is stored can change whenever any element is added or removed. plHashTable The plHashTable is optimized for very fast lookup, which should typically be a O(1) operation. Prefer the hash table whenever you will have a data set that is modified infrequently, but lookups will be done often and need to be as fast as possible. plHashTable may relocate its internal memory, which means iterators to its elements may not stay valid when the table is modified. Make sure to Reserve how much elements you probably will put into the plHashTable , to ensure fewer reallocations, but also to avoid hash-collisions and thus to improve overall performance. See Also String Usage Guidelines","title":"Container Usage Guidelines"},{"location":"appendix/container-usage/#container-usage-guidelines","text":"Plasma Engine has the following container classes: plStaticArray plHybridArray plDynamicArray plStaticRingBuffer plDeque plList plMap plSet plHashTable plArrayMap The following containers store their data as contiguous arrays: plStaticArray plHybridArray plDynamicArray plStaticRingBuffer plArrayMap The following containers are built on top of plDeque and thus share some performance characteristics: plList plMap plSet","title":"Container Usage Guidelines"},{"location":"appendix/container-usage/#when-to-use-which-container-type","text":"","title":"When to use which Container Type"},{"location":"appendix/container-usage/#arrays","text":"","title":"Arrays"},{"location":"appendix/container-usage/#general-guidelines","text":"For arrays, plHybridArray , plDynamicArray and plDeque are the most important containers. If you know or have a guess how much data you will need, always use this information in a call to Reserve to ensure that the containers can allocate data once (or at least much less), and do not need to reallocate several times. Never remove an element in between (using RemoveAt ), unless there is really no other way (and hopefully your array is small). Prefer RemoveAtAndSwap to replace the removed element by the last element in the array instead (this will destroy the order though). Similarly, never insert elements anywhere else than at the end. The only exception is plDeque , which is very efficient at insertion and removal of elements at both ends.","title":"General Guidelines"},{"location":"appendix/container-usage/#plhybridarray","text":"plHybridArray uses an internal fixed size cache (which you can specify as a template argument). When you create an plHybridArray on the stack, that data is also allocated on the stack. This is the most important container for writing performance critical yet safe code. plHybridArray allows to implement many algorithms without the need to use dynamic allocations. Prefer plHybridArray when you have a use case where you typically have a small set of elements, but it might be larger in some situations. plHybridArray will give you the performance of an immediately available array on the stack, combined with the safety of a dynamically allocated array on the heap, as it will reallocate data dynamically, whenever its internal storage is insufficient. However, be careful not to make the internal buffer too large. When creating plHybridArrays on the stack, you should not use more than a few KB for the internal cache, as you increase the risk for stack overflows. You should usually try to stay below (2KB / sizeof(Type)) for the number of elements in the plHybridArray cache. Note: plHybridArray is derived from plDynamicArray , that means every function that takes an plDynamicArray (even for writing output to), can be given an plHybridArray .","title":"plHybridArray"},{"location":"appendix/container-usage/#pldynamicarray","text":"plDynamicArray always allocates its data on the heap. The upside is, that it has a very low memory overhead, as long as it is empty, and it can handle any number of elements. Prefer plDynamicArray if your working set is generally larger and when you know how many elements it needs to hold before you fill it up. Use Reserve or SetCount BEFORE you start adding data to it, to prevent unnecessary reallocation later, as those are very costly. Also prefer plDynamicArray if the memory overhead in the empty state is of concern.","title":"plDynamicArray"},{"location":"appendix/container-usage/#pldeque","text":"plDeque stores its data as several chunks of contiguous arrays. An additional \"redirection array\" is used to know how to index into these chunks. plDeque requires one pointer indirection to make a lookup into its data. The deque is the only array container that does not store its data in one contiguous block of memory. Be aware of that, you cannot memcpy or memcmp data in a deque with more than one element, as the next element might be stored somewhere completely different. However a deque NEVER relocates an existing element in memory, either. That means once an element is inserted into a deque, you can safely store pointers to it, as it will not move around in memory (unless it is deleted, of course). Deques allocate their data in chunks. So whenever the memory is insufficient, a new chunk that can hold a fixed number of elements is allocated. One chunk is typically 4 KB in size. That means if you store floats in a deque, one chunk can hold 1000 values. Thus this is the 'minimum' memory usage of a deque, unless it is completely empty still. As such plDeques are very efficient for iteration (most of the data is contiguous) and they are very dynamic, as their size can grow dynamically without the need to reallocate and copy previous data, as the other array types would need to do. Therefore prefer plDeques whenever you need to have nearly the performance of an array, but have very dynamic data sets, that are difficult to predict in size, or that vary all the time. As deques are optimized for insertion and removal at both ends, plDeques are perfectly suited as fifo queues and dynamic ring-buffers. The memory overhead of plDeques is rather high, so do not use it for small data-sets (here plHybridArray is typically the best container). plDeques are also very well suited, whenever you have large objects to store that are very costly to construct or copy around, and you want to prevent those operations by all means, such that you do not want a container reallocation to trigger that.","title":"plDeque"},{"location":"appendix/container-usage/#plstaticarray","text":"This is a container that only stores a static array internally and cannot resize itself to be larger than that. Use this in code that has a definite upper limit of elements and whenever you must prevent the usage of any allocator by all means (such as for global variables). Typically there should be no need for this container, as plHybridArray delivers the same performance advantages and the safety of reallocating to the required size dynamically.","title":"plStaticArray"},{"location":"appendix/container-usage/#plstaticringbuffer","text":"Use this when you need a ring-buffer that shall have a fixed size. Use plDeque if you need a dynamically resizing ring-buffer.","title":"plStaticRingBuffer"},{"location":"appendix/container-usage/#lists","text":"There is only one implementation of a doubly linked list: plList plList is internally built on top of a plDeque . As such the memory requirements are the same. plList is optimized for inserting and removing objects frequently. Iterating over its elements might be slow due to excessive cache misses. You should typically not use plList in code that is performance critical. Use it when you have data-sets of unpredictable size that need to be sorted or rearranged very dynamically. For example when you read a complex data set which then needs to be processed and sorted by different criteria, which might delete and insert elements at random positions, then prefer an plList . However, once that step is finished, you should copy your sorted data into some array container for faster access. Nodes in an plList are never relocated in memory, as such iterators stay valid as long as the element is still alive.","title":"Lists"},{"location":"appendix/container-usage/#associative-containers","text":"","title":"Associative Containers"},{"location":"appendix/container-usage/#plmap-and-plset","text":"Both containers are basically the same, except that plMap stores a 'value' for each 'key', whereas a set only stores 'keys'. Use plMap whenever you need to be able to look up an entry with a key. Use plSet whenever you simply need to know whether some element is present or to merge data-sets down to all the unique elements. plMap and plSet are built on top of plDeque , similar to plList , that means they are quite heavy-weight in their memory consumption, however they can grow in size efficiently. As with plList , iterators (and pointers) to elements stay valid, as long as the element is alive, i.e. the data is never relocated. Insertion, lookup and removal are all O(log n) operations, since they are red-black trees internally and thus always perfectly balanced. plMap and plSet are well suited for very dynamic data sets (where a lot of insertions and removals are done, while also using it for lookup). If you have a use-case where you insert once and then lookup often, a sorted array, such as plArrayMap , or an plHashTable might be more efficient. plMap and plSet only require a simple comparer to be able to sort elements in a strictly weak ordering. As such they are well suited to handle objects that can be difficult to be hashed. Note that the nodes in the Map/Set each contain one element of their key/value type and those are stored in an plDeque . As such, when you put an plHybridArray (or an plString ) into an plMap , only one allocation is needed to allocate all the memory for a chunk (in the plDeque ) of data, which holds a large number of nodes, which already embed the data of their keys/values (e.g. plHybridArray ). Thus you can get away with very few memory allocations. If however you store an plDynamicArray in an plMap , each element still needs to allocate its own internal storage, which means you will get one additional allocation per element.","title":"plMap and plSet"},{"location":"appendix/container-usage/#plarraymap","text":"This container provides similar functionality as plMap but should be more efficient in scenarios where elements are looked up more often than they are inserted or removed. The implementation simply uses an array that is kept sorted, such that lookups can be done in a more cache friendly manner. If all you need is an associative container and your use case consists of changing the container infrequently, but looking up elements frequently (which is very often the case), you should prefer this container. Note, however, that this container will rearrange elements in memory whenever it needs to be sorted. In contrast an plMap guarantees that elements never move in memory, allowing to store pointers to the memory locations. Likewise the iterators of an plMap stay valid as long as an elements resides in the map. For plArrayMap this is not true, the index at which an element is stored can change whenever any element is added or removed.","title":"plArrayMap"},{"location":"appendix/container-usage/#plhashtable","text":"The plHashTable is optimized for very fast lookup, which should typically be a O(1) operation. Prefer the hash table whenever you will have a data set that is modified infrequently, but lookups will be done often and need to be as fast as possible. plHashTable may relocate its internal memory, which means iterators to its elements may not stay valid when the table is modified. Make sure to Reserve how much elements you probably will put into the plHashTable , to ensure fewer reallocations, but also to avoid hash-collisions and thus to improve overall performance.","title":"plHashTable"},{"location":"appendix/container-usage/#see-also","text":"String Usage Guidelines","title":"See Also"},{"location":"appendix/library-structure/","text":"Library Structure This document gives an overview of the functionality that Plasma Engine provides and how the most important libraries are structured. Plasma Engine is divided into multiple libraries that provide different functionality. The most basic and also largest library is Foundation . It contains all the basic functionality on which the rest of the engine is built. Foundation is meant to be application agnostic. All its features can be used in any kind of application. Most of the platform abstractions are implemented here. Foundation itself only depends on various third-party libraries. The Core library is built on top of Foundation. This library contains engine specific features, such as the Game Object system. Core is where most of the actual engine infrastructure is implemented. System is the library that is supposed to contain all the high-level platform specific code that might be difficult to abstract. Currently this mostly contains window creation code. The TestFramework library implements code to manage our unit-tests. You can ignore this entirely. GameEngine builds on top of all the other libraries, including the rendering code. It contains the most high-level code for a game engine, such as AI and Animation, plGameApplication (TODO) and plGameState , which are the basis for any game application built with Plasma, the prefab system, the visual scripting and much more. In general the most interesting libraries to look at are Foundation , Core and GameEngine . Library Overview: Foundation The Foundation library contains all the 'low-level' code that is used throughout the engine. In Foundation\\Containers you will find different types of container classes. These are heavily influenced by the STL, so a lot might look familiar. The most interesting class one should have a look at is plHybridArray . In Foundation\\Strings you will find all the string classes and utilities. Plasma Engine works with Utf-8 strings everywhere, which makes some things a bit more complicated, but these string classes make it pretty easy. There are utility classes to work with raw C strings and higher-level string classes to create and store strings efficiently. In Foundation\\Math you will find lots of math classes, e.g. classes to do vector arithmetic ( plVec3 , plMat4 , plQuat , plPlane , etc.), classes to work with colors ( plColor , plColorGammaUB , plColorLinear16f ), classes to work with bounding volumes ( plBoundingBox , plBoundingSphere ) and do culling ( plFrustum ), utility functions for intersection tests ( plIntersectionUtils ) and a class to work with angles efficiently ( plAngle ). There is even an implementation for a fixed point type ( plFixedPoint ). In Foundation\\Time you will find plTime , which handles simple time values. Using plTime::Now() you can access the current application time. To handle game time (e.g. for slow-motion, etc.) use plClock . For profiling or timing plStopwatch is available. And finally for system-independent timestamps, which might be useful when working with file modification times, plTimestamp and plDateTime are provided. In Foundation\\Threading you will find functionality that is useful for threading. plAtomicInteger provides lock-free integer values. plLock and plMutex implement the standard mechanisms for working with critical sections. plThread is a platform independent implementation for threads and plThreadSignal allows to send signals to other threads. plThreadUtils provides some of the low-level threading functions, such as plThreadUtils::Sleep() . However, before you go ahead and create your own threads, you should have a look at plTaskSystem , which is a thread-pool implementation that efficiently distributes tasks across multiple worker threads. It supports dependencies across tasks, different priorities, waiting/blocking for unfinished tasks, task canceling and load-balancing when tasks run over multiple frames. There should be only very few situations where a task is not good enough and a custom thread is necessary. In Foundation\\Logging you will find plLog , the central class to output log information. There are various ways logging information can be output. plLogWriter::HTML allows to write the data to an HTML file, plLogWriter::Console and plLogWriter::VisualStudio output the data to different console windows. Additionally the plInspector tool will display all logged data as well. The logging system is extremely useful to get an insight into what your application is doing and what might be going wrong, so we suggest setting this up early and using it to log most of what your application is doing. In Foundation\\Algorithm you will find some useful algorithms, mostly for sorting, searching and hashing. In Foundation\\Basics you can find a lot of platform specific code, most of which might not be very interesting. You will also find PLASMA_ASSERT which you should be using frequently. In Foundation\\Basics\\Types you will find some fundamental types that are used frequently in Plasma. plDelegate is often used for callbacks. plEnum is used for type-safe enum types and plBitflags is used for type-safe and easy to use bitflags. plArrayPtr is a 'fat pointer' that stores the start and length of an array. Finally plVariant is a type that can store different types of data (float, int, string, vector, etc.) and knows which type it has stored. It can do conversions between related types and is often used in message passing. In Foundation\\CodeUtils you can find utilities to work with code or text, such as plTokenizer . You will also find a full implementation of a C preprocessor ( plPreprocessor ). In Foundation\\Communication you can find functionality to communicate with other code. plEvent is a frequently used class to raise events and thus inform other code of changes. plMessage is used for message passing, which is quite often used with the game object system (which you will find in the Core library). plTelemetry is a system to broadcast information from the running application to other applications, usually tools for introspection, such as plInspector . In Foundation\\Configuration you will find tools to configure the application. plStartup is a system to (de-)initialize different subsystems in your code in the right order. plPlugin is used when you want to extend your application using plugin DLLs dynamically at runtime. plStartup helps with this as well. plCVar is a 'configuration variable' that allows to easily change the state of the running application. Its state can be stored on disk and it can be modified either through an plConsole or remotely through plInspector. This allows to have lots of 'debug modes' that can be enabled on demand without recompilation or complicated integration into the input handling. In Foundation\\Utilities you will find some utility functionality, such as plCommandLineUtils for command-line argument parsing, plConversionUtils for string / number conversions and plStats for broadcasting the state of some internal code, which is useful for debugging game code. In Foundation\\IO you will find lots of functionality for reading and writing data. plStreamReader and plStreamWriter are the interfaces for all IO operations. Derived from these classes you will find plMemoryStreamReader / plMemoryStreamWriter for working with data in-memory. plCompressedStreamReaderZstd and plCompressedStreamWriterZstd allow to zip and unzip data and plChunkStreamWriter / plChunkStreamReader implement a 'chunked' format that can be used for building file formats that another application may not fully understand. plOSFile is the low-level file abstraction, in most cases you should not need to work with this. Instead prefer plFileSystem which adds functionality for virtual file systems through mount points. For example a compressed file or a remote folder may be mounted as a read-only directory. plFileSystem is the central class to manage file accesses, but to actually read or write a file, use plFileReader and plFileWriter , which also implement the plStream* interface. To store data in a structured way, plJSONWriter and plOpenDdlWriter are provided. For convenient retrieval plJSONReader and plOpenDdlReader are available. For less convenient but more flexible and efficient JSON/ OpenDDL reading you can also use plJSONParser or plOpenDdlParser . In Foundation\\Reflection you will find the reflection system of Plasma Engine. This is used by the game objects and some other high-level code for object type identification and properties. This may be used for scripting, for setting up objects from configuration files and for editors. Most notable classes are plRTTI and plReflectedClass . Library Overview: Core In Core you will find the core engine infrastructure. In Core\\Application you can find code to more easily set up your application loop in a platform independent way. In Core\\Graphics you will find code commonly needed for doing graphics, such as plCamera for camera controls and plGeometry to create geometric objects. In Core\\Input you will find plInputManager which can be used for retrieving input from various different devices, e.g. mouse, keyboard, gamepad or virtual thumbstick. The system is easily extensible to include custom devices. In Core\\ResourceManager you will find the static class plResourceManager which is the central class for resource loading (e.g. textures, shaders, etc.). For implementing custom resource types you need to derive from plResource and for customizing the loading procedure you may need to implement a custom plResourceTypeLoader . All resources are referenced through plResourceHandle types, which implement reference counting. In Core\\Scripting you can find plLuaWrapper that allows to easily work with Lua scripts. In Core\\World you will find the game object system. plGameObject is the class to use to manage entities, plComponent is the base component class that allows to implement and attach components to your entities. All entities belong to an instance of plWorld , which represents your scene graph. Library Overview: GameEngine In GameEngine you will find all the high-level code needed in a game engine. GameEngine\\Console contains code for a Quake-like in-game console that can be used for changing the game configuration (through plCVar or custom functions) and to see the plLog output. GameEngine\\GameApplication contains plGameApplication (TODO) , which extends plApplication with higher-level, more game specific functionality. This is one of the most important high-level classes to look at and extend when writing your own, stand-alone game application (assuming you can't do so with plGameState alone). In GameEngine\\GameState you find plGameState , which is the most important class to extend when writing your own game code, especially if you want to be able to run your code within the editor. GameEngine\\Interfaces contains various interface definitions for instance for basic interactions with physics and audio engines. GameEngine\\Prefabs contains the code to work with prefabs. Library Overview: Texture In Texture you will find various things related to working with images and textures. In Texture\\Image you will find plImage which can be used to read, write and convert images from various formats. Library Overview: Utilities In Utilities you will find various different things that may be useful, but are not used by the general engine runtime. They may be used by some tool or by a sample game, though. Utilities\\DataStructures contains data structures that are too engine specific. Here you will find things such as octree and quadtree implementations ( plDynamicOctree ). Utilities\\GridAlgorithms contains functionality to rasterize circles and lines into grids. This can be extremely useful for 2D top down games, like strategy games, to do line-of-sight computations and such. Utilities\\PathFinding contains functionality to do path searches on graphs and simple nav-mesh generation for 2D grids. See Also Samples","title":"Library Structure"},{"location":"appendix/library-structure/#library-structure","text":"This document gives an overview of the functionality that Plasma Engine provides and how the most important libraries are structured. Plasma Engine is divided into multiple libraries that provide different functionality. The most basic and also largest library is Foundation . It contains all the basic functionality on which the rest of the engine is built. Foundation is meant to be application agnostic. All its features can be used in any kind of application. Most of the platform abstractions are implemented here. Foundation itself only depends on various third-party libraries. The Core library is built on top of Foundation. This library contains engine specific features, such as the Game Object system. Core is where most of the actual engine infrastructure is implemented. System is the library that is supposed to contain all the high-level platform specific code that might be difficult to abstract. Currently this mostly contains window creation code. The TestFramework library implements code to manage our unit-tests. You can ignore this entirely. GameEngine builds on top of all the other libraries, including the rendering code. It contains the most high-level code for a game engine, such as AI and Animation, plGameApplication (TODO) and plGameState , which are the basis for any game application built with Plasma, the prefab system, the visual scripting and much more. In general the most interesting libraries to look at are Foundation , Core and GameEngine .","title":"Library Structure"},{"location":"appendix/library-structure/#library-overview-foundation","text":"The Foundation library contains all the 'low-level' code that is used throughout the engine. In Foundation\\Containers you will find different types of container classes. These are heavily influenced by the STL, so a lot might look familiar. The most interesting class one should have a look at is plHybridArray . In Foundation\\Strings you will find all the string classes and utilities. Plasma Engine works with Utf-8 strings everywhere, which makes some things a bit more complicated, but these string classes make it pretty easy. There are utility classes to work with raw C strings and higher-level string classes to create and store strings efficiently. In Foundation\\Math you will find lots of math classes, e.g. classes to do vector arithmetic ( plVec3 , plMat4 , plQuat , plPlane , etc.), classes to work with colors ( plColor , plColorGammaUB , plColorLinear16f ), classes to work with bounding volumes ( plBoundingBox , plBoundingSphere ) and do culling ( plFrustum ), utility functions for intersection tests ( plIntersectionUtils ) and a class to work with angles efficiently ( plAngle ). There is even an implementation for a fixed point type ( plFixedPoint ). In Foundation\\Time you will find plTime , which handles simple time values. Using plTime::Now() you can access the current application time. To handle game time (e.g. for slow-motion, etc.) use plClock . For profiling or timing plStopwatch is available. And finally for system-independent timestamps, which might be useful when working with file modification times, plTimestamp and plDateTime are provided. In Foundation\\Threading you will find functionality that is useful for threading. plAtomicInteger provides lock-free integer values. plLock and plMutex implement the standard mechanisms for working with critical sections. plThread is a platform independent implementation for threads and plThreadSignal allows to send signals to other threads. plThreadUtils provides some of the low-level threading functions, such as plThreadUtils::Sleep() . However, before you go ahead and create your own threads, you should have a look at plTaskSystem , which is a thread-pool implementation that efficiently distributes tasks across multiple worker threads. It supports dependencies across tasks, different priorities, waiting/blocking for unfinished tasks, task canceling and load-balancing when tasks run over multiple frames. There should be only very few situations where a task is not good enough and a custom thread is necessary. In Foundation\\Logging you will find plLog , the central class to output log information. There are various ways logging information can be output. plLogWriter::HTML allows to write the data to an HTML file, plLogWriter::Console and plLogWriter::VisualStudio output the data to different console windows. Additionally the plInspector tool will display all logged data as well. The logging system is extremely useful to get an insight into what your application is doing and what might be going wrong, so we suggest setting this up early and using it to log most of what your application is doing. In Foundation\\Algorithm you will find some useful algorithms, mostly for sorting, searching and hashing. In Foundation\\Basics you can find a lot of platform specific code, most of which might not be very interesting. You will also find PLASMA_ASSERT which you should be using frequently. In Foundation\\Basics\\Types you will find some fundamental types that are used frequently in Plasma. plDelegate is often used for callbacks. plEnum is used for type-safe enum types and plBitflags is used for type-safe and easy to use bitflags. plArrayPtr is a 'fat pointer' that stores the start and length of an array. Finally plVariant is a type that can store different types of data (float, int, string, vector, etc.) and knows which type it has stored. It can do conversions between related types and is often used in message passing. In Foundation\\CodeUtils you can find utilities to work with code or text, such as plTokenizer . You will also find a full implementation of a C preprocessor ( plPreprocessor ). In Foundation\\Communication you can find functionality to communicate with other code. plEvent is a frequently used class to raise events and thus inform other code of changes. plMessage is used for message passing, which is quite often used with the game object system (which you will find in the Core library). plTelemetry is a system to broadcast information from the running application to other applications, usually tools for introspection, such as plInspector . In Foundation\\Configuration you will find tools to configure the application. plStartup is a system to (de-)initialize different subsystems in your code in the right order. plPlugin is used when you want to extend your application using plugin DLLs dynamically at runtime. plStartup helps with this as well. plCVar is a 'configuration variable' that allows to easily change the state of the running application. Its state can be stored on disk and it can be modified either through an plConsole or remotely through plInspector. This allows to have lots of 'debug modes' that can be enabled on demand without recompilation or complicated integration into the input handling. In Foundation\\Utilities you will find some utility functionality, such as plCommandLineUtils for command-line argument parsing, plConversionUtils for string / number conversions and plStats for broadcasting the state of some internal code, which is useful for debugging game code. In Foundation\\IO you will find lots of functionality for reading and writing data. plStreamReader and plStreamWriter are the interfaces for all IO operations. Derived from these classes you will find plMemoryStreamReader / plMemoryStreamWriter for working with data in-memory. plCompressedStreamReaderZstd and plCompressedStreamWriterZstd allow to zip and unzip data and plChunkStreamWriter / plChunkStreamReader implement a 'chunked' format that can be used for building file formats that another application may not fully understand. plOSFile is the low-level file abstraction, in most cases you should not need to work with this. Instead prefer plFileSystem which adds functionality for virtual file systems through mount points. For example a compressed file or a remote folder may be mounted as a read-only directory. plFileSystem is the central class to manage file accesses, but to actually read or write a file, use plFileReader and plFileWriter , which also implement the plStream* interface. To store data in a structured way, plJSONWriter and plOpenDdlWriter are provided. For convenient retrieval plJSONReader and plOpenDdlReader are available. For less convenient but more flexible and efficient JSON/ OpenDDL reading you can also use plJSONParser or plOpenDdlParser . In Foundation\\Reflection you will find the reflection system of Plasma Engine. This is used by the game objects and some other high-level code for object type identification and properties. This may be used for scripting, for setting up objects from configuration files and for editors. Most notable classes are plRTTI and plReflectedClass .","title":"Library Overview: Foundation"},{"location":"appendix/library-structure/#library-overview-core","text":"In Core you will find the core engine infrastructure. In Core\\Application you can find code to more easily set up your application loop in a platform independent way. In Core\\Graphics you will find code commonly needed for doing graphics, such as plCamera for camera controls and plGeometry to create geometric objects. In Core\\Input you will find plInputManager which can be used for retrieving input from various different devices, e.g. mouse, keyboard, gamepad or virtual thumbstick. The system is easily extensible to include custom devices. In Core\\ResourceManager you will find the static class plResourceManager which is the central class for resource loading (e.g. textures, shaders, etc.). For implementing custom resource types you need to derive from plResource and for customizing the loading procedure you may need to implement a custom plResourceTypeLoader . All resources are referenced through plResourceHandle types, which implement reference counting. In Core\\Scripting you can find plLuaWrapper that allows to easily work with Lua scripts. In Core\\World you will find the game object system. plGameObject is the class to use to manage entities, plComponent is the base component class that allows to implement and attach components to your entities. All entities belong to an instance of plWorld , which represents your scene graph.","title":"Library Overview: Core"},{"location":"appendix/library-structure/#library-overview-gameengine","text":"In GameEngine you will find all the high-level code needed in a game engine. GameEngine\\Console contains code for a Quake-like in-game console that can be used for changing the game configuration (through plCVar or custom functions) and to see the plLog output. GameEngine\\GameApplication contains plGameApplication (TODO) , which extends plApplication with higher-level, more game specific functionality. This is one of the most important high-level classes to look at and extend when writing your own, stand-alone game application (assuming you can't do so with plGameState alone). In GameEngine\\GameState you find plGameState , which is the most important class to extend when writing your own game code, especially if you want to be able to run your code within the editor. GameEngine\\Interfaces contains various interface definitions for instance for basic interactions with physics and audio engines. GameEngine\\Prefabs contains the code to work with prefabs.","title":"Library Overview: GameEngine"},{"location":"appendix/library-structure/#library-overview-texture","text":"In Texture you will find various things related to working with images and textures. In Texture\\Image you will find plImage which can be used to read, write and convert images from various formats.","title":"Library Overview: Texture"},{"location":"appendix/library-structure/#library-overview-utilities","text":"In Utilities you will find various different things that may be useful, but are not used by the general engine runtime. They may be used by some tool or by a sample game, though. Utilities\\DataStructures contains data structures that are too engine specific. Here you will find things such as octree and quadtree implementations ( plDynamicOctree ). Utilities\\GridAlgorithms contains functionality to rasterize circles and lines into grids. This can be extremely useful for 2D top down games, like strategy games, to do line-of-sight computations and such. Utilities\\PathFinding contains functionality to do path searches on graphs and simple nav-mesh generation for 2D grids.","title":"Library Overview: Utilities"},{"location":"appendix/library-structure/#see-also","text":"Samples","title":"See Also"},{"location":"appendix/string-formatting/","text":"String Formatting For formatting strings there are a couple of different options: plConversionUtils provides various ToString functions. These are useful for generic cases, where only individual variables need to be converted to a string representation and no control over the exact formatting is needed. plConversionUtils also provides functions to parse strings for numbers. plStringUtils::snprintf is a custom implementation of the infamous C printf function, with better security against buffer overruns and consistent behavior across platforms. It is available, but should generally be avoided, as it cannot provide any type safety. plFormatString is the preferred way to do string formatting. It is easy to use, fully type safe, extensible with custom formatters and optimized for performance by doing only very few allocation and delaying the formatting until it is needed, which enables functions to not pay the price for formatting an incoming string, if the function doesn't actually use the result. Using plFormatString as an Argument plFormatString is a class that can be easily used as a function parameter to accept formatted strings: void Print(const plFormatString& text); A function that takes just an plFormatString has to be called with the plFmt wrapper: Print(plFmt(\"Hello {}\", \"World\")); The plFmt function is a variadic template, that can take up to 10 arguments and wraps them all up into an plFormatString object. If you want your Print function to be a little bit more convenient, and not require the use of plFmt , you can add an overload that provides the variadic template functionality directly. template <typename... ARGS> void Print(const char* szFormat, ARGS&&... args) { Print(plFormatStringImpl<ARGS...>(szFormat, std::forward<ARGS>(args)...)); } Now Print can be called like this: Print(\"Hello {}\", \"World\"); Inside Print , all you need to do to get the formatted string is to call plFormatString::GetText() : void Print(const plFormatString& text) { plStringBuilder tmp; const char* szResult = text.GetText(tmp); // do something with szResult, do not use tmp, as it is not guaranteed to hold the result (meaning, it may not have been needed) } Using plFormatString Once a function takes an plFormatString (see above), you can use {} notation to indicate where an argument shall be inserted. Positional Arguments If a formatting string contains {} , every instance will use the next argument, as given to the function. You can also use {n} with n being 0 to 9 to insert the n-th argument. This allows you to skip, rearrange, or repeat arguments: Print(\"Arg1: {1}, Arg2: {2}, Arg1: {1}\", \"zero\", \"one\", \"two\"); Formatting Options Most types that can be formatted, can be passed in directly: Print(\"int value is {}\", 23); However, often you may want to specify exactly how to display the value. To do so, you need to wrap the incoming argument in an option struct. Each option struct can have arbitrary parameters to configure how it works. Print(\"HEX: 0x{}\", plArgU(value, 8, true, 16, true)); plArgU is an option struct for unsigned int values. Here we specify that the output should have a fixed width of 8 characters, should pad the output with zeros if necessary, use base 16 (hex) and upper case letters. There are many such option structs available, each with their own parameters. By convention, all formatting option structs are named plArgXYZ . Available Option Structs This is a not necessarily complete list of available option structs: plArgC - for single characters plArgF - for floating point values plArgI - for int values plArgU - for unsigned int values plArgP - for pointers plArgErrorCode - for Windows error codes plArgDateTime - for plDateTime plArgHumanReadable - for shortening numbers with suffixes (such as K (kilo) and M (mega)) plArgFileSize - for shortening file sizes and use suffixes (such as KB and MB ) Custom Argument Formatters You can easily write your own formatter. The formatting work is done by a free function called BuildString , overloaded for the type that it shall format. If you search for BuildString functions, you will find many overloads, each handling a different type. Please look at those functions to see how to write your own formatter. For it to work, all that is necessary, is that your code is #include 'd when it is used in a format string. If you try to use a type (such as MyType ) in a format string and your custom formatter is not known ( #include 'd) in that cpp file, you will get a compile time error with a very long message telling you that no overload of of BuildString is available to handle this type. The plArgXYZ structs are just used to wrap a type and store additional options. This is not required, for BuildString to work, but it does allow you to wrap the same type multiple times to select different formatters. For example, unsigned int is wrapped by plArgU for regular int formatting options, by plArgFileSize to print a value with B , KB , MB or GB suffixes and by plArgErrorCode to interpret it as a Windows error code and translate it to a readable string. If you have a custom type MyType and you do not need any formatting options, you can write a BuildString overload, that takes a MyType directly. See Also String Usage Guidelines","title":"String Formatting"},{"location":"appendix/string-formatting/#string-formatting","text":"For formatting strings there are a couple of different options: plConversionUtils provides various ToString functions. These are useful for generic cases, where only individual variables need to be converted to a string representation and no control over the exact formatting is needed. plConversionUtils also provides functions to parse strings for numbers. plStringUtils::snprintf is a custom implementation of the infamous C printf function, with better security against buffer overruns and consistent behavior across platforms. It is available, but should generally be avoided, as it cannot provide any type safety. plFormatString is the preferred way to do string formatting. It is easy to use, fully type safe, extensible with custom formatters and optimized for performance by doing only very few allocation and delaying the formatting until it is needed, which enables functions to not pay the price for formatting an incoming string, if the function doesn't actually use the result.","title":"String Formatting"},{"location":"appendix/string-formatting/#using-plformatstring-as-an-argument","text":"plFormatString is a class that can be easily used as a function parameter to accept formatted strings: void Print(const plFormatString& text); A function that takes just an plFormatString has to be called with the plFmt wrapper: Print(plFmt(\"Hello {}\", \"World\")); The plFmt function is a variadic template, that can take up to 10 arguments and wraps them all up into an plFormatString object. If you want your Print function to be a little bit more convenient, and not require the use of plFmt , you can add an overload that provides the variadic template functionality directly. template <typename... ARGS> void Print(const char* szFormat, ARGS&&... args) { Print(plFormatStringImpl<ARGS...>(szFormat, std::forward<ARGS>(args)...)); } Now Print can be called like this: Print(\"Hello {}\", \"World\"); Inside Print , all you need to do to get the formatted string is to call plFormatString::GetText() : void Print(const plFormatString& text) { plStringBuilder tmp; const char* szResult = text.GetText(tmp); // do something with szResult, do not use tmp, as it is not guaranteed to hold the result (meaning, it may not have been needed) }","title":"Using plFormatString as an Argument"},{"location":"appendix/string-formatting/#using-plformatstring","text":"Once a function takes an plFormatString (see above), you can use {} notation to indicate where an argument shall be inserted.","title":"Using plFormatString"},{"location":"appendix/string-formatting/#positional-arguments","text":"If a formatting string contains {} , every instance will use the next argument, as given to the function. You can also use {n} with n being 0 to 9 to insert the n-th argument. This allows you to skip, rearrange, or repeat arguments: Print(\"Arg1: {1}, Arg2: {2}, Arg1: {1}\", \"zero\", \"one\", \"two\");","title":"Positional Arguments"},{"location":"appendix/string-formatting/#formatting-options","text":"Most types that can be formatted, can be passed in directly: Print(\"int value is {}\", 23); However, often you may want to specify exactly how to display the value. To do so, you need to wrap the incoming argument in an option struct. Each option struct can have arbitrary parameters to configure how it works. Print(\"HEX: 0x{}\", plArgU(value, 8, true, 16, true)); plArgU is an option struct for unsigned int values. Here we specify that the output should have a fixed width of 8 characters, should pad the output with zeros if necessary, use base 16 (hex) and upper case letters. There are many such option structs available, each with their own parameters. By convention, all formatting option structs are named plArgXYZ .","title":"Formatting Options"},{"location":"appendix/string-formatting/#available-option-structs","text":"This is a not necessarily complete list of available option structs: plArgC - for single characters plArgF - for floating point values plArgI - for int values plArgU - for unsigned int values plArgP - for pointers plArgErrorCode - for Windows error codes plArgDateTime - for plDateTime plArgHumanReadable - for shortening numbers with suffixes (such as K (kilo) and M (mega)) plArgFileSize - for shortening file sizes and use suffixes (such as KB and MB )","title":"Available Option Structs"},{"location":"appendix/string-formatting/#custom-argument-formatters","text":"You can easily write your own formatter. The formatting work is done by a free function called BuildString , overloaded for the type that it shall format. If you search for BuildString functions, you will find many overloads, each handling a different type. Please look at those functions to see how to write your own formatter. For it to work, all that is necessary, is that your code is #include 'd when it is used in a format string. If you try to use a type (such as MyType ) in a format string and your custom formatter is not known ( #include 'd) in that cpp file, you will get a compile time error with a very long message telling you that no overload of of BuildString is available to handle this type. The plArgXYZ structs are just used to wrap a type and store additional options. This is not required, for BuildString to work, but it does allow you to wrap the same type multiple times to select different formatters. For example, unsigned int is wrapped by plArgU for regular int formatting options, by plArgFileSize to print a value with B , KB , MB or GB suffixes and by plArgErrorCode to interpret it as a Windows error code and translate it to a readable string. If you have a custom type MyType and you do not need any formatting options, you can write a BuildString overload, that takes a MyType directly.","title":"Custom Argument Formatters"},{"location":"appendix/string-formatting/#see-also","text":"String Usage Guidelines","title":"See Also"},{"location":"appendix/string-usage/","text":"String Usage Guidelines All strings in the Plasma Engine are Utf8 encoded. As such they are accessible as simple const char* arrays. However, one has to be very careful to distinguish between the \"Element Count\" (the number of bytes in the string) and the \"Character Count\". Utf8 is a variable length encoding, which means that all ASCII characters (the first 127 characters in the ANSI set) are encoded with one byte. Everything else can take between two and four bytes per character. Therefore it is very dangerous to apply an algorithm to every byte in a string, as it might actually need to be aware of the individual characters. Therefore all the string classes provide lots of high-level functionality for modifying the strings, which take care of these things internally. On the other hand, direct access to the char array is not granted, to ensure the encoding does not get corrupted. The following classes provide functionality to work on char* arrays: plUnicodeUtils plStringUtils plPathUtils These generally implement 'read-only' functionality. The actual string classes provide the same functions, but with more comfortable interfaces. Note that many of these functions take an 'end' pointer. This can generally be ignored, if you are working on zero terminated strings. It can however also be used to work on non zero-terminated sub-strings. How to access C strings Working on Utf8 strings is difficult, as you usually want to work on a per-character basis, which does not have a 1:1 mapping to bytes, when the data is Utf8 encoded. To make this possible, use plStringIterator . plStringIterator provides an interface on top of a char array, that allows to iterate forwards and backwards through a string on a per-character basis. You can access the current character and get it in Utf32 encoding. This can easily be compared with standard C++ char-constants like 'a', 'b', '\\', '\\n', '\\0' etc. If you want to work on a per-character basis on a char that might be Utf8 encoded, you should always wrap it inside an plStringIterator and then use the iterator instead of accessing the array directly. Note that it only provides read-access though, as you cannot modify a Utf8 string in place in all circumstances. Additionally there is plStringView , which allows to only work on sub-strings of other strings, which can be used to implement parsing functions very efficiently. How to create and modify strings When you need to modify or build strings, use plStringBuilder . plStringBuilder provides a large set of functions to easily modify strings. It includes a set of dedicated 'path functions' for working with paths (to prevent duplicate slashes, extract certain information etc.). plStringBuilder uses an internal cache of 128 bytes, which is allocated on the stack. That means working with local plStringBuilders is very efficient, as usually no memory allocations are required, unless you build very long strings. That in turn means, that you should never use plStringBuilders for storage, as they will waste a lot of memory. plStringBuilders should nearly always be local variables with a short life span. How to store strings For storing strings, use plString . plString is a typedef for plHybridString<32> . That means it has an internal cache for storing up to 32 bytes without the need for memory allocations. That usually covers more than 90% of all strings, which means that memory allocations are rare, but the amount of wasted memory is also relatively low. plString does not provide functions to modify the string (other than completely replacing it), use plStringBuilder in such cases. If you know that you are storing strings of certain lengths, e.g. filename extensions, you can use other predefined plString s, such as plString8 or plString48 to tune the internal cache size to be more fitting. How to convert strings between Utf8, Utf16, Utf32 and wchar_t Plasma Engine provides a few classes to enable converting to and from all the Utf encodings and wchar_t encoding (which is simply Utf16 one some platforms and Utf32 on others). The following classes take strings encoded in any encoding and convert them into their target encoding: plStringWChar plStringUtf8 plStringUtf16 plStringUtf32 Ie. an instance of plStringWChar will always encode a string to (the platform specific) wchar_t encoding and instances of plStringUtf8 will convert strings to Utf8 encoding, etc. Use it like this: plStringUtf8 MyUtf8 = L\"My wchar_t string\"; // The 'L' makes it a 'wide-string', ie. a wchar_t array printf(\"Output Utf8: %s\", MyUtf8.GetData()); or for interacting with win32 functions: plStringWChar FileNameW = MyUtf8FileName.GetData(); DeleteFileW(FileNameW.GetData()); This allows to quickly and easily convert back and forth between the different encodings. Just make sure that you convert your data to Utf8 when you store something inside the Engine, and to the platform-specific encoding (typically wchar_t ) when interacting with OS functions (unless they support Utf8 as well). See Also Container Usage Guidelines String Formatting","title":"String Usage Guidelines"},{"location":"appendix/string-usage/#string-usage-guidelines","text":"All strings in the Plasma Engine are Utf8 encoded. As such they are accessible as simple const char* arrays. However, one has to be very careful to distinguish between the \"Element Count\" (the number of bytes in the string) and the \"Character Count\". Utf8 is a variable length encoding, which means that all ASCII characters (the first 127 characters in the ANSI set) are encoded with one byte. Everything else can take between two and four bytes per character. Therefore it is very dangerous to apply an algorithm to every byte in a string, as it might actually need to be aware of the individual characters. Therefore all the string classes provide lots of high-level functionality for modifying the strings, which take care of these things internally. On the other hand, direct access to the char array is not granted, to ensure the encoding does not get corrupted. The following classes provide functionality to work on char* arrays: plUnicodeUtils plStringUtils plPathUtils These generally implement 'read-only' functionality. The actual string classes provide the same functions, but with more comfortable interfaces. Note that many of these functions take an 'end' pointer. This can generally be ignored, if you are working on zero terminated strings. It can however also be used to work on non zero-terminated sub-strings.","title":"String Usage Guidelines"},{"location":"appendix/string-usage/#how-to-access-c-strings","text":"Working on Utf8 strings is difficult, as you usually want to work on a per-character basis, which does not have a 1:1 mapping to bytes, when the data is Utf8 encoded. To make this possible, use plStringIterator . plStringIterator provides an interface on top of a char array, that allows to iterate forwards and backwards through a string on a per-character basis. You can access the current character and get it in Utf32 encoding. This can easily be compared with standard C++ char-constants like 'a', 'b', '\\', '\\n', '\\0' etc. If you want to work on a per-character basis on a char that might be Utf8 encoded, you should always wrap it inside an plStringIterator and then use the iterator instead of accessing the array directly. Note that it only provides read-access though, as you cannot modify a Utf8 string in place in all circumstances. Additionally there is plStringView , which allows to only work on sub-strings of other strings, which can be used to implement parsing functions very efficiently.","title":"How to access C strings"},{"location":"appendix/string-usage/#how-to-create-and-modify-strings","text":"When you need to modify or build strings, use plStringBuilder . plStringBuilder provides a large set of functions to easily modify strings. It includes a set of dedicated 'path functions' for working with paths (to prevent duplicate slashes, extract certain information etc.). plStringBuilder uses an internal cache of 128 bytes, which is allocated on the stack. That means working with local plStringBuilders is very efficient, as usually no memory allocations are required, unless you build very long strings. That in turn means, that you should never use plStringBuilders for storage, as they will waste a lot of memory. plStringBuilders should nearly always be local variables with a short life span.","title":"How to create and modify strings"},{"location":"appendix/string-usage/#how-to-store-strings","text":"For storing strings, use plString . plString is a typedef for plHybridString<32> . That means it has an internal cache for storing up to 32 bytes without the need for memory allocations. That usually covers more than 90% of all strings, which means that memory allocations are rare, but the amount of wasted memory is also relatively low. plString does not provide functions to modify the string (other than completely replacing it), use plStringBuilder in such cases. If you know that you are storing strings of certain lengths, e.g. filename extensions, you can use other predefined plString s, such as plString8 or plString48 to tune the internal cache size to be more fitting.","title":"How to store strings"},{"location":"appendix/string-usage/#how-to-convert-strings-between-utf8-utf16-utf32-and-wchar_t","text":"Plasma Engine provides a few classes to enable converting to and from all the Utf encodings and wchar_t encoding (which is simply Utf16 one some platforms and Utf32 on others). The following classes take strings encoded in any encoding and convert them into their target encoding: plStringWChar plStringUtf8 plStringUtf16 plStringUtf32 Ie. an instance of plStringWChar will always encode a string to (the platform specific) wchar_t encoding and instances of plStringUtf8 will convert strings to Utf8 encoding, etc. Use it like this: plStringUtf8 MyUtf8 = L\"My wchar_t string\"; // The 'L' makes it a 'wide-string', ie. a wchar_t array printf(\"Output Utf8: %s\", MyUtf8.GetData()); or for interacting with win32 functions: plStringWChar FileNameW = MyUtf8FileName.GetData(); DeleteFileW(FileNameW.GetData()); This allows to quickly and easily convert back and forth between the different encodings. Just make sure that you convert your data to Utf8 when you store something inside the Engine, and to the platform-specific encoding (typically wchar_t ) when interacting with OS functions (unless they support Utf8 as well).","title":"How to convert strings between Utf8, Utf16, Utf32 and wchar_t"},{"location":"appendix/string-usage/#see-also","text":"Container Usage Guidelines String Formatting","title":"See Also"},{"location":"appendix/third-party-code/","text":"ThirdParty Code and Data This page lists which third party code and data is used by Plasma. Important: Before you distribute any project, please check the licensing conditions for all used components. The list below tries to be exhaustive and up-to-date for components directly used by Plasma, but this is only provided for your convenience and we give no guarantee for correctness. It is still your responsibility to make absolutely certain that your project doesn't violate any licensing conditions from third-party components used directly or indirectly in your project. Assimp Link: http://www.assimp.org Compile switch: None (hard dependency for the asset processing) Open Asset Import Library, a portable Open Source library to import various well-known 3D model formats in a uniform manner. cc0Textures Link: https://cc0textures.com CC0 Textures offers a library containing hundreds of detailed PBR materials with displacement-, normal- and roughness maps for photorealistic rendering. All assets are available for free and without any restrictions. cgbookcase Link: https://cgbookcase.com cgbookcase provides hundreds of high quality, PBR textures. All textures on cgbookcase.com are licensed as CC0. Dear Imgui Link: https://github.com/ocornut/imgui Compile switch: PLASMA_3RDPARTY_IMGUI_SUPPORT A nice library for easily creating ingame GUIs. DirectXTex Link: https://github.com/Microsoft/DirectXTex Compile switch: Currently none Used by plImage and the plTexConv tool for GPU-enabled block compression. Duktape Link: https://duktape.org Compile switch: PLASMA_3RDPARTY_DUKTAPE_SUPPORT Duktape is an embeddable Javascript engine, with a focus on portability and compact footprint. It can be used directly or through plDuktapeWrapper. Non-essential for the engine, but scripting functionality (using TypeScript ) is built on top of it. Enet Link: http://enet.bespin.org Compile switch: PLASMA_3RDPARTY_ENET_SUPPORT An efficient and easy to use networking library, built on top of the UDP protocol. It is used by plTelemetry to interact with the plInspector, and it is also used to implement the file serving functionality. FleetOps Link: https://www.fleetops.net Some assets were kindly provided with permission to use and redistribute, by the awesome team behind the FleetOps project. Thanks so much guys! FMOD 2.x Link: https://www.fmod.com Compile switch: PLASMA_BUILD_FMOD Plasma has an integration for the FMOD sound system . Important: FMOD is a commercial product and you may need to buy a license to use it in your project. FreeSound Link: https://freesound.org Freesound is a collaborative database of Creative Commons Licensed sounds. jc_voronoi Link: https://github.com/JCash/voronoi/blob/dev/src/jc_voronoi.h Compile switch: None A fast single file 2D voronoi diagram generator. Used by plBreakableSheetComponent. Jolt Physics Link: https://github.com/jrouwe/JoltPhysics Compile switch: PLASMA_3RDPARTY_JOLT_SUPPORT Jolt Physics is a free and open source physics engine. It is used to provide collision detection, physics simulation, character controllers and other interactions. Kenney.nl Link: kenney.nl Kenney provides thousands of textures, 3D models and sound effects under a generous public domain license. Some of them are used in our sample projects. Lua Link: (http://www.lua.org Compile switch: PLASMA_3RDPARTY_LUA_SUPPORT The Lua scripting language. Can be used directly or through plLuaWrapper for easier access to common functionality. Non-essential for Plasma, only the ingame console interpreter would stop working without it. Mikktspace Link: http://mmikkelsen3d.blogspot.ie Compile switch: None (hard dependency for the asset processing) Tangent space generation code by Morten S. Mikkelsen. See https://wiki.blender.org/index.php/Dev:Shading/Tangent_Space_Normal_Maps for more information. It is used by plGeometry. Ozz Link: http://guillaumeblanc.github.io/ozz-animation Compile switch: None Used as the basis for skeletal animations. Both during asset import (to build an optimized skeleton structure) and at runtime for animation playback. Qt 5 Link: https://www.qt.io Compile switch: PLASMA_ENABLE_QT_SUPPORT Used for all desktop GUI code in the editor and tools. Important: Depending on how you use Qt, you may need to acquire (buy) a license for it. See https://www.qt.io/terms-conditions/ . Qt Advanced Docking System Link https://github.com/githubuser0xFFFF/Qt-Advanced-Docking-System Compile switch PLASMA_3RDPARTY_ADS_SUPPORT A docking system for Qt similiar to the one in visual studio. Used by the editor and inspector applications. Recast Compile switch: PLASMA_3RDPARTY_RECAST_SUPPORT Link: https://github.com/recastnavigation/recastnavigation A library to generate navigation meshes from arbitrary 3D geometry. RenderDoc Link: https://renderdoc.org Compile switch: PLASMA_3RDPARTY_RENDERDOC_SUPPORT RenderDoc is a free MIT licensed stand-alone graphics debugger. The plRenderDocPlugin enables full control over taking RenderDoc snapshots from within the engine. RmlUi Link: https://mikke89.github.io/RmlUiDoc/ RmlUi is the C++ user interface package based on the HTML and CSS standards, designed as a complete solution for any project's interface needs. It is a fork of the libRocket project, introducing new features, bug fixes, and performance improvements. SFML Link: http://www.sfml-dev.org Compile switch: currently none (TODO) This library provides a simple and portable interface for window creation, input handling and more. Used by plWindow and plStandardInputDevice on non-Windows platforms (Mac, Linux). Silk Icons Link: http://www.famfamfam.com/lab/icons/silk Icons from this set are used by the tools. Sonniss Link: https://sonniss.com Sonniss is a premium distribution platform for high-quality sound effects libraries. Sounds distributed from Sonnis are taken from the GameAudioGDC bundles. See Data\\Content\\Sound\\FmodProject\\Assets\\Sonnis\\Licensing.pdf for details. stb Link: https://github.com/nothings/stb Compile switch: None Public domain licensed code by Sean Barrett. Used by plImage to read and write some of the supported formats like PNG and JPEG. UTF8-CPP Link: https://github.com/nemtrif/utfcpp Compile switch: None A library that provides Unicode related functionality. Integrated directly into plFoundation. V-HACD Link: https://github.com/kmammou/v-hacd Compile switch: PLASMA_3RDPARTY_VHACD_SUPPORT The \"Volumetric Hierarchical Approximate Convex Decomposition\" library is used to decompose a concave triangle mesh into multiple convex pieces. This allows you to generate complex collision meshes which can be used as the shapes of dynamic actors . xxHash Link: https://github.com/Cyan4973/xxHash Compile switch: None A very fast hash algorithm. Integrated directly into plFoundation. zLib Link: http://www.zlib.net Compile switch: PLASMA_3RDPARTY_ZLIB_SUPPORT Provides algorithms for zip compression and decompression. It is used by plCompressedStreamReaderZlib and plCompressedStreamWriterZlib in plFoundation. zstd Link: https://facebook.github.io/zstd Compile switch: PLASMA_3RDPARTY_ZSTD_SUPPORT A very fast lossless compression library. It is used by plCompressedStreamReaderZstd and plCompressedStreamWriterZstd and also by plArchive.","title":"ThirdParty Code and Data"},{"location":"appendix/third-party-code/#thirdparty-code-and-data","text":"This page lists which third party code and data is used by Plasma. Important: Before you distribute any project, please check the licensing conditions for all used components. The list below tries to be exhaustive and up-to-date for components directly used by Plasma, but this is only provided for your convenience and we give no guarantee for correctness. It is still your responsibility to make absolutely certain that your project doesn't violate any licensing conditions from third-party components used directly or indirectly in your project.","title":"ThirdParty Code and Data"},{"location":"appendix/third-party-code/#assimp","text":"Link: http://www.assimp.org Compile switch: None (hard dependency for the asset processing) Open Asset Import Library, a portable Open Source library to import various well-known 3D model formats in a uniform manner.","title":"Assimp"},{"location":"appendix/third-party-code/#cc0textures","text":"Link: https://cc0textures.com CC0 Textures offers a library containing hundreds of detailed PBR materials with displacement-, normal- and roughness maps for photorealistic rendering. All assets are available for free and without any restrictions.","title":"cc0Textures"},{"location":"appendix/third-party-code/#cgbookcase","text":"Link: https://cgbookcase.com cgbookcase provides hundreds of high quality, PBR textures. All textures on cgbookcase.com are licensed as CC0.","title":"cgbookcase"},{"location":"appendix/third-party-code/#dear-imgui","text":"Link: https://github.com/ocornut/imgui Compile switch: PLASMA_3RDPARTY_IMGUI_SUPPORT A nice library for easily creating ingame GUIs.","title":"Dear Imgui"},{"location":"appendix/third-party-code/#directxtex","text":"Link: https://github.com/Microsoft/DirectXTex Compile switch: Currently none Used by plImage and the plTexConv tool for GPU-enabled block compression.","title":"DirectXTex"},{"location":"appendix/third-party-code/#duktape","text":"Link: https://duktape.org Compile switch: PLASMA_3RDPARTY_DUKTAPE_SUPPORT Duktape is an embeddable Javascript engine, with a focus on portability and compact footprint. It can be used directly or through plDuktapeWrapper. Non-essential for the engine, but scripting functionality (using TypeScript ) is built on top of it.","title":"Duktape"},{"location":"appendix/third-party-code/#enet","text":"Link: http://enet.bespin.org Compile switch: PLASMA_3RDPARTY_ENET_SUPPORT An efficient and easy to use networking library, built on top of the UDP protocol. It is used by plTelemetry to interact with the plInspector, and it is also used to implement the file serving functionality.","title":"Enet"},{"location":"appendix/third-party-code/#fleetops","text":"Link: https://www.fleetops.net Some assets were kindly provided with permission to use and redistribute, by the awesome team behind the FleetOps project. Thanks so much guys!","title":"FleetOps"},{"location":"appendix/third-party-code/#fmod-2x","text":"Link: https://www.fmod.com Compile switch: PLASMA_BUILD_FMOD Plasma has an integration for the FMOD sound system . Important: FMOD is a commercial product and you may need to buy a license to use it in your project.","title":"FMOD 2.x"},{"location":"appendix/third-party-code/#freesound","text":"Link: https://freesound.org Freesound is a collaborative database of Creative Commons Licensed sounds.","title":"FreeSound"},{"location":"appendix/third-party-code/#jc_voronoi","text":"Link: https://github.com/JCash/voronoi/blob/dev/src/jc_voronoi.h Compile switch: None A fast single file 2D voronoi diagram generator. Used by plBreakableSheetComponent.","title":"jc_voronoi"},{"location":"appendix/third-party-code/#jolt-physics","text":"Link: https://github.com/jrouwe/JoltPhysics Compile switch: PLASMA_3RDPARTY_JOLT_SUPPORT Jolt Physics is a free and open source physics engine. It is used to provide collision detection, physics simulation, character controllers and other interactions.","title":"Jolt Physics"},{"location":"appendix/third-party-code/#kenneynl","text":"Link: kenney.nl Kenney provides thousands of textures, 3D models and sound effects under a generous public domain license. Some of them are used in our sample projects.","title":"Kenney.nl"},{"location":"appendix/third-party-code/#lua","text":"Link: (http://www.lua.org Compile switch: PLASMA_3RDPARTY_LUA_SUPPORT The Lua scripting language. Can be used directly or through plLuaWrapper for easier access to common functionality. Non-essential for Plasma, only the ingame console interpreter would stop working without it.","title":"Lua"},{"location":"appendix/third-party-code/#mikktspace","text":"Link: http://mmikkelsen3d.blogspot.ie Compile switch: None (hard dependency for the asset processing) Tangent space generation code by Morten S. Mikkelsen. See https://wiki.blender.org/index.php/Dev:Shading/Tangent_Space_Normal_Maps for more information. It is used by plGeometry.","title":"Mikktspace"},{"location":"appendix/third-party-code/#ozz","text":"Link: http://guillaumeblanc.github.io/ozz-animation Compile switch: None Used as the basis for skeletal animations. Both during asset import (to build an optimized skeleton structure) and at runtime for animation playback.","title":"Ozz"},{"location":"appendix/third-party-code/#qt-5","text":"Link: https://www.qt.io Compile switch: PLASMA_ENABLE_QT_SUPPORT Used for all desktop GUI code in the editor and tools. Important: Depending on how you use Qt, you may need to acquire (buy) a license for it. See https://www.qt.io/terms-conditions/ .","title":"Qt 5"},{"location":"appendix/third-party-code/#qt-advanced-docking-system","text":"Link https://github.com/githubuser0xFFFF/Qt-Advanced-Docking-System Compile switch PLASMA_3RDPARTY_ADS_SUPPORT A docking system for Qt similiar to the one in visual studio. Used by the editor and inspector applications.","title":"Qt Advanced Docking System"},{"location":"appendix/third-party-code/#recast","text":"Compile switch: PLASMA_3RDPARTY_RECAST_SUPPORT Link: https://github.com/recastnavigation/recastnavigation A library to generate navigation meshes from arbitrary 3D geometry.","title":"Recast"},{"location":"appendix/third-party-code/#renderdoc","text":"Link: https://renderdoc.org Compile switch: PLASMA_3RDPARTY_RENDERDOC_SUPPORT RenderDoc is a free MIT licensed stand-alone graphics debugger. The plRenderDocPlugin enables full control over taking RenderDoc snapshots from within the engine.","title":"RenderDoc"},{"location":"appendix/third-party-code/#rmlui","text":"Link: https://mikke89.github.io/RmlUiDoc/ RmlUi is the C++ user interface package based on the HTML and CSS standards, designed as a complete solution for any project's interface needs. It is a fork of the libRocket project, introducing new features, bug fixes, and performance improvements.","title":"RmlUi"},{"location":"appendix/third-party-code/#sfml","text":"Link: http://www.sfml-dev.org Compile switch: currently none (TODO) This library provides a simple and portable interface for window creation, input handling and more. Used by plWindow and plStandardInputDevice on non-Windows platforms (Mac, Linux).","title":"SFML"},{"location":"appendix/third-party-code/#silk-icons","text":"Link: http://www.famfamfam.com/lab/icons/silk Icons from this set are used by the tools.","title":"Silk Icons"},{"location":"appendix/third-party-code/#sonniss","text":"Link: https://sonniss.com Sonniss is a premium distribution platform for high-quality sound effects libraries. Sounds distributed from Sonnis are taken from the GameAudioGDC bundles. See Data\\Content\\Sound\\FmodProject\\Assets\\Sonnis\\Licensing.pdf for details.","title":"Sonniss"},{"location":"appendix/third-party-code/#stb","text":"Link: https://github.com/nothings/stb Compile switch: None Public domain licensed code by Sean Barrett. Used by plImage to read and write some of the supported formats like PNG and JPEG.","title":"stb"},{"location":"appendix/third-party-code/#utf8-cpp","text":"Link: https://github.com/nemtrif/utfcpp Compile switch: None A library that provides Unicode related functionality. Integrated directly into plFoundation.","title":"UTF8-CPP"},{"location":"appendix/third-party-code/#v-hacd","text":"Link: https://github.com/kmammou/v-hacd Compile switch: PLASMA_3RDPARTY_VHACD_SUPPORT The \"Volumetric Hierarchical Approximate Convex Decomposition\" library is used to decompose a concave triangle mesh into multiple convex pieces. This allows you to generate complex collision meshes which can be used as the shapes of dynamic actors .","title":"V-HACD"},{"location":"appendix/third-party-code/#xxhash","text":"Link: https://github.com/Cyan4973/xxHash Compile switch: None A very fast hash algorithm. Integrated directly into plFoundation.","title":"xxHash"},{"location":"appendix/third-party-code/#zlib","text":"Link: http://www.zlib.net Compile switch: PLASMA_3RDPARTY_ZLIB_SUPPORT Provides algorithms for zip compression and decompression. It is used by plCompressedStreamReaderZlib and plCompressedStreamWriterZlib in plFoundation.","title":"zLib"},{"location":"appendix/third-party-code/#zstd","text":"Link: https://facebook.github.io/zstd Compile switch: PLASMA_3RDPARTY_ZSTD_SUPPORT A very fast lossless compression library. It is used by plCompressedStreamReaderZstd and plCompressedStreamWriterZstd and also by plArchive.","title":"zstd"},{"location":"assets/asset-browser/","text":"Asset Browser - NEEDS UPDATE The asset browser is used for selecting and opening asset documents . Assets can be filtered by type and path or searched for by name. If the asset browser panel is not visible, select Panels > Asset Browser . You will notice that the asset browser will show up in two modes. The panel is constantly visible and can be interacted with at all times, to search for assets and open them via double click, or instantiate them with drag and drop. Additionally, when an object has an asset reference property , choosing Select Asset from the button menu right next to it will open another asset browser in file picker mode . Several other options are available, for example, Open Asset allows you to open the referenced asset document, which can be useful to follow a chain of asset references. Search The Search field in the top-left corner allows you to search for assets by name, path and GUID. The search by path or name is case insensitive. For paths, both slashes and backslashes are allowed. You can also input the GUID of an asset (for example { 1c47ee4c-0379-4280-85f5-b8cda61941d2 } ). Advanced Search The Search field supports special keywords to find assets by certain criteria. Find Asset References ( ref and ref-all ) It can be very useful to know in which assets one particular asset is used. By typing ref:{asset-guid} into the search field, the asset browser will display all assets that directly reference that particular asset themselves. Using ref-all:{asset-guid} will display also all assets that are indirectly dependent on that asset. However, it is much more convenient to just right-click an asset and select Find all references to this asset . This will fill out the search field accordingly: Filter by Asset Type Below the search field, all available asset types are listed with checkboxes. Click the checkboxes to only display assets of those types. Click \\<All> or uncheck all asset types to display assets of all types. Filter by Folder On the bottom left the asset browser displays all data directories . When you select a folder in this tree view, the asset browser will only display assets located below that folder. Filter to this Path You can right-click on any asset and select Filter to this Path to set the folder filter to the path in which the selected asset resides. This is useful when you already see an asset in the browser but are interested in an asset that you know is located next to that asset (same folder or sub-folder). Create Asset Documents You can create new asset documents by right clicking a folder on the bottom left, or an asset on the right and selecting New > Asset Type . The advantage over creating a document via Editor > Create Document... is that the create file dialog opens directly in the location of the selected asset or folder, which makes it easier to create a new asset next to an existing asset. Display Assets in Recently Used Order The editor remembers which assets you used recently. The asset browser can list recently used assets at the top. This option can be toggled from the context menu on the right hand side of the asset browser. Note that the state of this option is remembered separately for the asset browser panel and for the asset browser when used as a file picker . In panel mode, it is typically disabled and all assets are sorted alphabetically, in file picker mode, it typically sorts by recently used time. Copy Asset Guid In rare cases you may need the internal GUID (Globally Unique Identifier) of an asset. You can easily copy it to the clipboard by right clicking an asset and selecting Copy Asset Guid . Transform Assets The asset browser allows you to quickly transform assets in multiple ways: Transform All: In the toolbar of the asset browser there is a button of a white box with a red arrow. When you press this button all assets that are not up-to-date get transformed (ie. their runtime data is created from the source input data). Transform Selected: Select one or multiple assets in the asset browser, right click and choose Transform to update only the selected assets. Transform Single: You can also quickly transform just a single asset by clicking the icon overlay at the bottom right of an asset's thumbnail (usually a checkmark or a gear). Resave all Asset Documents The rightmost button in the asset browser's toolbar triggers an action to open each and every document, save it and close it again. This can be used to migrate all documents to the very latest version. Since document versioning is very robust, there is little practical use for this operation, though. Check Filesystem The leftmost button in the asset browser's toolbar makes the editor check the filesystem for changes that were missed by the automatic filesystem watcher. This may be useful when you added or removed assets on disk and the changes are not reflected in the editor. Background Processing and Transform State At the bottom right of the asset browser there is a widget with a colored progressbar and a play button. The play/pause button is for switching background processing on and off. If enabled, outdated assets are automatically transformed in the background. The progressbar displays how many assets need updating and whether there were any errors. For details about processed assets and potential errors, check out the asset curator . If you do not want assets to be transformed automatically, disable background processing with the pause button. Drag and Drop You can drag assets from the asset browser into other documents, such as scenes . For mesh and prefab assets this will instantiate the asset (ie. create a new node that references the asset). For materials this may assign the material to the object that you drag it onto. Not all asset types support drag and drop. Also dragging an asset into the 3D viewport can have a different effect than dragging it into the scene tree. See Also Asset Curator Editor Documents Asset Import","title":"Asset Browser - NEEDS UPDATE"},{"location":"assets/asset-browser/#asset-browser-needs-update","text":"The asset browser is used for selecting and opening asset documents . Assets can be filtered by type and path or searched for by name. If the asset browser panel is not visible, select Panels > Asset Browser . You will notice that the asset browser will show up in two modes. The panel is constantly visible and can be interacted with at all times, to search for assets and open them via double click, or instantiate them with drag and drop. Additionally, when an object has an asset reference property , choosing Select Asset from the button menu right next to it will open another asset browser in file picker mode . Several other options are available, for example, Open Asset allows you to open the referenced asset document, which can be useful to follow a chain of asset references.","title":"Asset Browser - NEEDS UPDATE"},{"location":"assets/asset-browser/#search","text":"The Search field in the top-left corner allows you to search for assets by name, path and GUID. The search by path or name is case insensitive. For paths, both slashes and backslashes are allowed. You can also input the GUID of an asset (for example { 1c47ee4c-0379-4280-85f5-b8cda61941d2 } ).","title":"Search"},{"location":"assets/asset-browser/#advanced-search","text":"The Search field supports special keywords to find assets by certain criteria.","title":"Advanced Search"},{"location":"assets/asset-browser/#find-asset-references-ref-and-ref-all","text":"It can be very useful to know in which assets one particular asset is used. By typing ref:{asset-guid} into the search field, the asset browser will display all assets that directly reference that particular asset themselves. Using ref-all:{asset-guid} will display also all assets that are indirectly dependent on that asset. However, it is much more convenient to just right-click an asset and select Find all references to this asset . This will fill out the search field accordingly:","title":"Find Asset References (ref and ref-all)"},{"location":"assets/asset-browser/#filter-by-asset-type","text":"Below the search field, all available asset types are listed with checkboxes. Click the checkboxes to only display assets of those types. Click \\<All> or uncheck all asset types to display assets of all types.","title":"Filter by Asset Type"},{"location":"assets/asset-browser/#filter-by-folder","text":"On the bottom left the asset browser displays all data directories . When you select a folder in this tree view, the asset browser will only display assets located below that folder.","title":"Filter by Folder"},{"location":"assets/asset-browser/#filter-to-this-path","text":"You can right-click on any asset and select Filter to this Path to set the folder filter to the path in which the selected asset resides. This is useful when you already see an asset in the browser but are interested in an asset that you know is located next to that asset (same folder or sub-folder).","title":"Filter to this Path"},{"location":"assets/asset-browser/#create-asset-documents","text":"You can create new asset documents by right clicking a folder on the bottom left, or an asset on the right and selecting New > Asset Type . The advantage over creating a document via Editor > Create Document... is that the create file dialog opens directly in the location of the selected asset or folder, which makes it easier to create a new asset next to an existing asset.","title":"Create Asset Documents"},{"location":"assets/asset-browser/#display-assets-in-recently-used-order","text":"The editor remembers which assets you used recently. The asset browser can list recently used assets at the top. This option can be toggled from the context menu on the right hand side of the asset browser. Note that the state of this option is remembered separately for the asset browser panel and for the asset browser when used as a file picker . In panel mode, it is typically disabled and all assets are sorted alphabetically, in file picker mode, it typically sorts by recently used time.","title":"Display Assets in Recently Used Order"},{"location":"assets/asset-browser/#copy-asset-guid","text":"In rare cases you may need the internal GUID (Globally Unique Identifier) of an asset. You can easily copy it to the clipboard by right clicking an asset and selecting Copy Asset Guid .","title":"Copy Asset Guid"},{"location":"assets/asset-browser/#transform-assets","text":"The asset browser allows you to quickly transform assets in multiple ways: Transform All: In the toolbar of the asset browser there is a button of a white box with a red arrow. When you press this button all assets that are not up-to-date get transformed (ie. their runtime data is created from the source input data). Transform Selected: Select one or multiple assets in the asset browser, right click and choose Transform to update only the selected assets. Transform Single: You can also quickly transform just a single asset by clicking the icon overlay at the bottom right of an asset's thumbnail (usually a checkmark or a gear).","title":"Transform Assets"},{"location":"assets/asset-browser/#resave-all-asset-documents","text":"The rightmost button in the asset browser's toolbar triggers an action to open each and every document, save it and close it again. This can be used to migrate all documents to the very latest version. Since document versioning is very robust, there is little practical use for this operation, though.","title":"Resave all Asset Documents"},{"location":"assets/asset-browser/#check-filesystem","text":"The leftmost button in the asset browser's toolbar makes the editor check the filesystem for changes that were missed by the automatic filesystem watcher. This may be useful when you added or removed assets on disk and the changes are not reflected in the editor.","title":"Check Filesystem"},{"location":"assets/asset-browser/#background-processing-and-transform-state","text":"At the bottom right of the asset browser there is a widget with a colored progressbar and a play button. The play/pause button is for switching background processing on and off. If enabled, outdated assets are automatically transformed in the background. The progressbar displays how many assets need updating and whether there were any errors. For details about processed assets and potential errors, check out the asset curator . If you do not want assets to be transformed automatically, disable background processing with the pause button.","title":"Background Processing and Transform State"},{"location":"assets/asset-browser/#drag-and-drop","text":"You can drag assets from the asset browser into other documents, such as scenes . For mesh and prefab assets this will instantiate the asset (ie. create a new node that references the asset). For materials this may assign the material to the object that you drag it onto. Not all asset types support drag and drop. Also dragging an asset into the 3D viewport can have a different effect than dragging it into the scene tree.","title":"Drag and Drop"},{"location":"assets/asset-browser/#see-also","text":"Asset Curator Editor Documents Asset Import","title":"See Also"},{"location":"assets/asset-curator/","text":"Asset Curator - MERGED TO ASSET BROWSER The asset curator panel is a tool to help find and fix problems with assets . If the panel is not visible, use Panels > Asset Curator to open it. Activity At its top the asset curator panel displays an activity log. If background transform is active, this shows which assets have been transformed recently. Whether background transform is enabled can be seen from the progress bar at the bottom: Transform Issues If an asset fails to transform for some reason, it will be listed in the view to the bottom left. The most common issue is a missing file reference. For example when a texture source file has been moved or renamed, the texture asset can't find it anymore and thus fails to transform. When you select an asset from that list, the log at the bottom right will display any error message from the failed transform. Double click the asset to directly open the document. If Show Indirect Issues is disabled (the default), only assets that have problems finding their source files are displayed. Otherwise all assets which failed to transform are displayed, however, most of them will be follow-up issues due to other assets being incomplete. Once you fix an asset and make sure it is transformed, the asset curator will no longer show it in the issues list. See Also Asset Browser Assets","title":"Asset Curator - MERGED TO ASSET BROWSER"},{"location":"assets/asset-curator/#asset-curator-merged-to-asset-browser","text":"The asset curator panel is a tool to help find and fix problems with assets . If the panel is not visible, use Panels > Asset Curator to open it.","title":"Asset Curator - MERGED TO ASSET BROWSER"},{"location":"assets/asset-curator/#activity","text":"At its top the asset curator panel displays an activity log. If background transform is active, this shows which assets have been transformed recently. Whether background transform is enabled can be seen from the progress bar at the bottom:","title":"Activity"},{"location":"assets/asset-curator/#transform-issues","text":"If an asset fails to transform for some reason, it will be listed in the view to the bottom left. The most common issue is a missing file reference. For example when a texture source file has been moved or renamed, the texture asset can't find it anymore and thus fails to transform. When you select an asset from that list, the log at the bottom right will display any error message from the failed transform. Double click the asset to directly open the document. If Show Indirect Issues is disabled (the default), only assets that have problems finding their source files are displayed. Otherwise all assets which failed to transform are displayed, however, most of them will be follow-up issues due to other assets being incomplete. Once you fix an asset and make sure it is transformed, the asset curator will no longer show it in the issues list.","title":"Transform Issues"},{"location":"assets/asset-curator/#see-also","text":"Asset Browser Assets","title":"See Also"},{"location":"assets/asset-profiles/","text":"Asset Profiles Asset profiles are fully functional, but currently undocumented. See Also","title":"Asset Profiles"},{"location":"assets/asset-profiles/#asset-profiles","text":"Asset profiles are fully functional, but currently undocumented.","title":"Asset Profiles"},{"location":"assets/asset-profiles/#see-also","text":"","title":"See Also"},{"location":"assets/assets-overview/","text":"Assets Assets are the most common type of documents in the editor. Assets represent data that usually comes from some source file (like an image or a model) and must be processed into an optimized format for the engine to use at runtime. This processing step is called asset transformation . A good example are textures . Textures come in many different source formats, such as TGA, JPG, PNG, and so on. Texture data in these formats is not suited to be loaded directly by the engine. Instead, it must be encoded and compressed in formats that GPUs can decode efficiently. This step can be very time consuming and should therefore be done up front. Additionally, textures should contain mipmaps and may need to be downscaled for different platforms. Exactly how a texture should be transformed is something that you may want to have full control over, so you need some way to configure this. Therefore, instead of loading a texture directly into the engine, you need to create a texture asset in the editor. This document will reference one or multiple source files and allow you to configure the asset transform. When the asset gets transformed it will output an optimized file that the engine can then load and use efficiently. Types of Assets One can distinguish between two types of assets: Assets that mostly exist to transform existing data from a source format into an optimized format, and assets that represent entirely new data, authored in the editor. Examples for the former are textures , meshes , collision meshes , sounds and so on. Their purpose is to ensure that the engine does not need to handle all sorts of source formats, but only optimized runtime formats. Instead, the editor and other tools will deal with the source formats, and allow the user to configure this conversion step. Examples for the latter are scenes , prefabs , materials , property animations (TODO) , curves and so on. Their data does not come from some other file on disk, but is instead built entirely in the editor. However, they still need to be transformed to provide the engine with an optimized format. Creating Assets The straight forward way to create an asset document is through the menu Editor > Create Document... . This gives you a blank asset and you can (and must) fill out all properties manually. For common types of assets there is a more convenient way to quickly fill out the common properties. See the asset import documentation for details. Asset GUID The editor references assets not by file path, but by GUID ( G lobally U nique ID entifier). Each asset is assigned a GUID upon creation and the GUID never changes. That means an asset document can be renamed and moved to a different location on disk, and the editor will continue to find it. Similarly, the engine runtime will also locate the transformed asset files through the asset GUID (the file system takes care of translating a GUID to an actual path). This makes reorganizing the file structure easy and resilient to errors. For the rare case that you need to know the actual GUID of an asset, you can right click any asset document and select Copy Asset GUID . The only caveat is, that you should never duplicate an asset by actually copy and pasting an asset file, as this would result in two assets with the same GUID. The editor will try to detect and fix such cases, but it may not work out the way you planned. Instead use the File > Save as... functionality to create a copy of an asset with a different name. This will assign a new asset GUID to the new asset document. Asset Browser All assets are listed in the asset browser . Asset Transform Asset documents must be transformed to produce the actual runtime data that the engine uses. In an open asset document you can either press CTRL+E or click the green arrow button in the toolbar to export (transform) the asset. To transform all assets in a project, open the asset browser and click the Transform All button there. Optionally, you can also enable background asset processing . Asset Errors An asset can be in an error state. The most common reason for this is, that it references files or other assets that don't exist (anymore). In this case the asset cannot be transformed correctly and will therefore not produce any new output. All erroneous assets are listed in the asset curator . The curator panel will also show error log output for those assets. A common problem is, when you moved an asset document to a new location, you may also need to adjust the path to input files, such as the source texture or model data. Another problem are deleted assets, or missing assets because of a different data directory setup. Asset Profiles Assets can produce different, platform specific output, depending on which platform they are being transformed for. That means a texture may, for example, generate a runtime file that contains full resolution 4K textures for PC, but only limited 1K resolution textures for mobile devices. Such platform specific options can be configured through asset profiles (TODO) . For some types of assets, such platform specific settings may also be handled externally, for example FMOD already deals with platform specific audio encoding on its own. Assets and Resources The term asset mostly refers to the editor side and the editor documents . When an asset gets transformed, it generates the data representation for the runtime side. Inside the engine this data will be read into a resource by the resource manager (TODO) . Assets and resources are conceptually two different things. Assets always live on the editor side, resources always on the runtime side. Their code is strictly separate. Resources can be loaded from files or procedurally generated at runtime. The files that they load can come from anywhere and there is no requirement that those files are created through assets. However, assets are the most common and most convenient way to generate the runtime data. You could replace the entire asset management system with a custom system, though. The editor may be the most convenient way to transform assets from source format to runtime format for most scenarios, but if you have a special use case, you could built a completely custom asset processing pipeline and ignore the editor entirely, there is no 'secret sauce' in the editor that is required to make the runtime work. Consequently, when the documentation mentions 'assets', it always refers the data and behavior in the editor, and when it mentions 'resources' it always refers to data that is used by the runtime side (the renderer, the physics engine, the game logic, etc). When you work with the editor, the two code paths are even separated into different processes: Editor.exe and EditorEngineProcess.exe See Also Editor Documents Asset Browser Asset Curator Asset Profiles (TODO) Asset Import","title":"Assets"},{"location":"assets/assets-overview/#assets","text":"Assets are the most common type of documents in the editor. Assets represent data that usually comes from some source file (like an image or a model) and must be processed into an optimized format for the engine to use at runtime. This processing step is called asset transformation . A good example are textures . Textures come in many different source formats, such as TGA, JPG, PNG, and so on. Texture data in these formats is not suited to be loaded directly by the engine. Instead, it must be encoded and compressed in formats that GPUs can decode efficiently. This step can be very time consuming and should therefore be done up front. Additionally, textures should contain mipmaps and may need to be downscaled for different platforms. Exactly how a texture should be transformed is something that you may want to have full control over, so you need some way to configure this. Therefore, instead of loading a texture directly into the engine, you need to create a texture asset in the editor. This document will reference one or multiple source files and allow you to configure the asset transform. When the asset gets transformed it will output an optimized file that the engine can then load and use efficiently.","title":"Assets"},{"location":"assets/assets-overview/#types-of-assets","text":"One can distinguish between two types of assets: Assets that mostly exist to transform existing data from a source format into an optimized format, and assets that represent entirely new data, authored in the editor. Examples for the former are textures , meshes , collision meshes , sounds and so on. Their purpose is to ensure that the engine does not need to handle all sorts of source formats, but only optimized runtime formats. Instead, the editor and other tools will deal with the source formats, and allow the user to configure this conversion step. Examples for the latter are scenes , prefabs , materials , property animations (TODO) , curves and so on. Their data does not come from some other file on disk, but is instead built entirely in the editor. However, they still need to be transformed to provide the engine with an optimized format.","title":"Types of Assets"},{"location":"assets/assets-overview/#creating-assets","text":"The straight forward way to create an asset document is through the menu Editor > Create Document... . This gives you a blank asset and you can (and must) fill out all properties manually. For common types of assets there is a more convenient way to quickly fill out the common properties. See the asset import documentation for details.","title":"Creating Assets"},{"location":"assets/assets-overview/#asset-guid","text":"The editor references assets not by file path, but by GUID ( G lobally U nique ID entifier). Each asset is assigned a GUID upon creation and the GUID never changes. That means an asset document can be renamed and moved to a different location on disk, and the editor will continue to find it. Similarly, the engine runtime will also locate the transformed asset files through the asset GUID (the file system takes care of translating a GUID to an actual path). This makes reorganizing the file structure easy and resilient to errors. For the rare case that you need to know the actual GUID of an asset, you can right click any asset document and select Copy Asset GUID . The only caveat is, that you should never duplicate an asset by actually copy and pasting an asset file, as this would result in two assets with the same GUID. The editor will try to detect and fix such cases, but it may not work out the way you planned. Instead use the File > Save as... functionality to create a copy of an asset with a different name. This will assign a new asset GUID to the new asset document.","title":"Asset GUID"},{"location":"assets/assets-overview/#asset-browser","text":"All assets are listed in the asset browser .","title":"Asset Browser"},{"location":"assets/assets-overview/#asset-transform","text":"Asset documents must be transformed to produce the actual runtime data that the engine uses. In an open asset document you can either press CTRL+E or click the green arrow button in the toolbar to export (transform) the asset. To transform all assets in a project, open the asset browser and click the Transform All button there. Optionally, you can also enable background asset processing .","title":"Asset Transform"},{"location":"assets/assets-overview/#asset-errors","text":"An asset can be in an error state. The most common reason for this is, that it references files or other assets that don't exist (anymore). In this case the asset cannot be transformed correctly and will therefore not produce any new output. All erroneous assets are listed in the asset curator . The curator panel will also show error log output for those assets. A common problem is, when you moved an asset document to a new location, you may also need to adjust the path to input files, such as the source texture or model data. Another problem are deleted assets, or missing assets because of a different data directory setup.","title":"Asset Errors"},{"location":"assets/assets-overview/#asset-profiles","text":"Assets can produce different, platform specific output, depending on which platform they are being transformed for. That means a texture may, for example, generate a runtime file that contains full resolution 4K textures for PC, but only limited 1K resolution textures for mobile devices. Such platform specific options can be configured through asset profiles (TODO) . For some types of assets, such platform specific settings may also be handled externally, for example FMOD already deals with platform specific audio encoding on its own.","title":"Asset Profiles"},{"location":"assets/assets-overview/#assets-and-resources","text":"The term asset mostly refers to the editor side and the editor documents . When an asset gets transformed, it generates the data representation for the runtime side. Inside the engine this data will be read into a resource by the resource manager (TODO) . Assets and resources are conceptually two different things. Assets always live on the editor side, resources always on the runtime side. Their code is strictly separate. Resources can be loaded from files or procedurally generated at runtime. The files that they load can come from anywhere and there is no requirement that those files are created through assets. However, assets are the most common and most convenient way to generate the runtime data. You could replace the entire asset management system with a custom system, though. The editor may be the most convenient way to transform assets from source format to runtime format for most scenarios, but if you have a special use case, you could built a completely custom asset processing pipeline and ignore the editor entirely, there is no 'secret sauce' in the editor that is required to make the runtime work. Consequently, when the documentation mentions 'assets', it always refers the data and behavior in the editor, and when it mentions 'resources' it always refers to data that is used by the runtime side (the renderer, the physics engine, the game logic, etc). When you work with the editor, the two code paths are even separated into different processes: Editor.exe and EditorEngineProcess.exe","title":"Assets and Resources"},{"location":"assets/assets-overview/#see-also","text":"Editor Documents Asset Browser Asset Curator Asset Profiles (TODO) Asset Import","title":"See Also"},{"location":"assets/import-assets/","text":"Asset Import All assets are represented by documents . That means to get a texture into the engine, you need a texture document which describes which source files (png, jpg, etc) are used to create the texture and how they shall be imported. This is where you configure such things as, whether to use compression, whether an alpha channel should be present and so on. Other asset types of course have other options for importing. To create these documents you have two options: Manual or automatic import. Create Documents Manually You can manually create documents via Editor > Create Document... . In the file creation dialog, choose the file extension for the desired asset type and specify where to store the document. The newly created document will be in a blank state. You will need to fill out all the properties, including where the source files are located. This method always works for all asset types and for some types it is the only way. Since this method always involves multiple, mostly simple steps, it can become tedious. Therefore, some asset types provide a way to automate much of this process. Create Documents Automatically For asset types that are mostly defined by a single source file (e.g. textures and meshes ), the editor often provides an importing method that automates most of the trivial setup. Select Editor > Import Assets... or press CTRL+I to open a file browse dialog. Navigate to the file(s) that you want to import and select them. If you want to know which asset types are currently supported for automatic import, you can open the dropdown with the allowed file extensions here. Dragging and dropping an asset will skip the browser dialog. After you selected the source files, you will be presented with a table of options how to import them. Here we selected four files for import. One .obj file and three .jpg files. The automatic import uses heuristics to make an educated guess how to import certain source files. Here it already suggests to import the \"_col.jpg\" file as a diffuse texture, the \"_nrm.jpg\" file as a normal map and so on. If the heuristic is incorrect, you can use the dropdown on the left to fix it. It also suggests the target document file name. You can either click Browse or double click into the text field to change the target file name. Some source files can be imported in multiple ways. For example the .obj file could be imported as a mesh for rendering or as a mesh for physics. Often you want to import the same mesh for both, so you want two asset documents (a Mesh and a Collision Mesh ) which reference the same input file. Therefore this table lists the .obj file twice but with different import options in the dropdown box. If, for example, you do not want a mesh to be imported as collision mesh, at all, you can just select No Import from the respective dropdown. Once you click Import the asset documents are generated and you can then open them. If background asset processing is enabled, the editor will already start transforming the asset data. Otherwise open each document and click the button of a white box with the green arrow to manually trigger the asset transform. The automatic import creates the documents using a set of rules to fill out its properties, depending on the template that you selected for it. So for example an image imported as a \"diffuse texture\" and one imported as a \"normal map\" are mostly the same, except that a few options are already configured in a certain way for you. You should review all options for correctness afterwards. See Also Assets Asset Browser","title":"Asset Import"},{"location":"assets/import-assets/#asset-import","text":"All assets are represented by documents . That means to get a texture into the engine, you need a texture document which describes which source files (png, jpg, etc) are used to create the texture and how they shall be imported. This is where you configure such things as, whether to use compression, whether an alpha channel should be present and so on. Other asset types of course have other options for importing. To create these documents you have two options: Manual or automatic import.","title":"Asset Import"},{"location":"assets/import-assets/#create-documents-manually","text":"You can manually create documents via Editor > Create Document... . In the file creation dialog, choose the file extension for the desired asset type and specify where to store the document. The newly created document will be in a blank state. You will need to fill out all the properties, including where the source files are located. This method always works for all asset types and for some types it is the only way. Since this method always involves multiple, mostly simple steps, it can become tedious. Therefore, some asset types provide a way to automate much of this process.","title":"Create Documents Manually"},{"location":"assets/import-assets/#create-documents-automatically","text":"For asset types that are mostly defined by a single source file (e.g. textures and meshes ), the editor often provides an importing method that automates most of the trivial setup. Select Editor > Import Assets... or press CTRL+I to open a file browse dialog. Navigate to the file(s) that you want to import and select them. If you want to know which asset types are currently supported for automatic import, you can open the dropdown with the allowed file extensions here. Dragging and dropping an asset will skip the browser dialog. After you selected the source files, you will be presented with a table of options how to import them. Here we selected four files for import. One .obj file and three .jpg files. The automatic import uses heuristics to make an educated guess how to import certain source files. Here it already suggests to import the \"_col.jpg\" file as a diffuse texture, the \"_nrm.jpg\" file as a normal map and so on. If the heuristic is incorrect, you can use the dropdown on the left to fix it. It also suggests the target document file name. You can either click Browse or double click into the text field to change the target file name. Some source files can be imported in multiple ways. For example the .obj file could be imported as a mesh for rendering or as a mesh for physics. Often you want to import the same mesh for both, so you want two asset documents (a Mesh and a Collision Mesh ) which reference the same input file. Therefore this table lists the .obj file twice but with different import options in the dropdown box. If, for example, you do not want a mesh to be imported as collision mesh, at all, you can just select No Import from the respective dropdown. Once you click Import the asset documents are generated and you can then open them. If background asset processing is enabled, the editor will already start transforming the asset data. Otherwise open each document and click the button of a white box with the green arrow to manually trigger the asset transform. The automatic import creates the documents using a set of rules to fill out its properties, depending on the template that you selected for it. So for example an image imported as a \"diffuse texture\" and one imported as a \"normal map\" are mostly the same, except that a few options are already configured in a certain way for you. You should review all options for correctness afterwards.","title":"Create Documents Automatically"},{"location":"assets/import-assets/#see-also","text":"Assets Asset Browser","title":"See Also"},{"location":"build/build-android/","text":"Building for Android Prerequisites You need the following to build for Android: Android SDK Platform 10.0 (\"Q\") API-Level 29 Android NDK 26.1 or higher (older should work as well, but untested) Android SDK Build Tools Android SDK Platform-Tools Android Emulator (optional) Java (JRE) Cmake Vulkan SDK 1.3.275 or newer Ninja Powershell 7 (for debugging in VSCode or running convenience scripts) Visual Studio 2022 or Linux Ninja is a build generator used by CMake and needs to be added to the PATH environment variable. The easiest way to install the Android components is to download Android Studio and then to select these from the SDK Manager . Alternatively, you can also install these via the command line tool located in C:\\Users\\[USER]\\AppData\\Local\\Android\\Sdk\\cmdline-tools\\latest\\bin : # From the root of the PL checkout, run: .\\Utilities\\Android\\AndroidEmulator.ps1 -installBuildDependencies # Or run these manually from the ...\\cmdline-tools\\latest\\bin folder: ./sdkmanager \"build-tools;34.0.0\" ./sdkmanager \"cmdline-tools;latest\" ./sdkmanager \"ndk;26.1.10909125\" ./sdkmanager \"platform-tools\" ./sdkmanager \"platforms;android-29\" # Optional for emulator: ./sdkmanager \"emulator\" ./sdkmanager \"extras;google;Android_Emulator_Hypervisor_Driver\" ./sdkmanager \"system-images;android-29;google_apis;x86_64\" # Accept licenses for the above ./sdkmanager --licenses Once installed, the following environment variables need to be set: Change the version to reflect the one you are using. ANDROID_NDK_HOME needs to point to your installed version, by default this is: C:\\Users\\[USERNAME]\\AppData\\Local\\Android\\Sdk\\ndk\\[VERSION] ANDROID_HOME needs to point to your installed version, by default this is: C:\\Users\\[USERNAME]\\AppData\\Local\\Android\\Sdk JAVA_HOME needs to point to a java runtime. Android Studio has its own version so there is no need to download it separately: C:\\Program Files\\Android\\Android Studio\\jbr ANDROID_STUDIO (Optional for debugging). Needs to point to the root of Android Studio, e.g. C:\\Program Files\\Android\\Android Studio . We currently rely on the lldb-server that ships with Android Studio. Alternatively, you can also debug any app with Android Studio once at which point the required files are on the device and this env var is no longer needed. Visual Studio / VSCode / CLion While you can manually run CMake to use the ninja generator, a more convenient solution is to use CMake's CMakePresets.json which is already configured for Android arm64 and x64 builds. * Visual Studio : Use Visual Studio's open folder functionality. Go to File > Open > Folder... and select the root of the repository. If all environment variables were set correctly VS should automatically configure CMake. Once done, a drop down appears in the VS toolbar, allowing you to select the configuration, e.g. android-arm64-debug . Once changed, VS will start to configure CMake again for the new configuration. Next, select a build target, e.g. libFoundationTest.so which are the foundation unit tests. Note that you can only select applications, not all libraries here. * VSCode : Make sure you have the C/C++ , C/C++ Extension Pack , CMake and CMake Tools plugins installed. Select File > Open Folder... and select the root of the repository. Execute CMake: Select Configure Preset to select the config you wish to use. Make sure CMake runs through without errors. On failure fix any errors and execute CMake: Configure until successful. Finally, execute CMake: Build Target to build the project you want. * CLion : Open settings, go to Build, Execution, Deployment > CMake , select the profile you wish to use and enable it. Make sure CMake configure runs through without errors. Finally, select the build target of choice in the toolbar and press the build button next to it. Setting up an Emulator AVD You can either use the Android Studio GUI or the command line to setup the emulator. For the command line option, the avdmanager is usually located in the C:\\[USERNAME]\\admin\\AppData\\Local\\Android\\Sdk\\cmdline-tools\\latest\\bin folder. The following powershell command will create a device that can run PL generated apks: # From the root of the PL checkout, run: ./Utilities/Android/AndroidEmulator.ps1 -installEmulator # Or run this manually from the ...\\cmdline-tools\\latest\\bin folder: ./avdmanager create avd --force --name \"Pixel7\" --abi \"google_apis/x86_64\" --package \"system-images;android-29;google_apis;x86_64\" --device \"pixel_7\" Starting the Emulator The emulator can be comfortably started from within Android Studio or via the emulator application located in C:\\[USERNAME]\\admin\\AppData\\Local\\Android\\Sdk\\emulator via these powershell commands: # From the root of the PL checkout, run: ./Utilities/Android/AndroidEmulator.ps1 -startEmulator # Or run this manually from the ...\\Sdk\\emulator folder: ./emulator -avd \"Pixel7\" -wipe-data -no-snapshot -no-audio -port 5555 -gpu swiftshader_indirect For better performance, the -gpu host option can be used but it may cause crashes or graphical artifacts. For more information on the available options, see the official emulator hardware acceleration page. The options -wipe-data -no-snapshot -no-audio are not strictly necessary but will provide the same environment our unit tests are run under. To use the GUI instead, open Android Studio, go to Configure>AVD Manager and select Create Virtual Device . Select the Pixel 7 hardware profile. Next, select x86 Images , then select Q (API 29), x86_64 . NOTE: If the emulator hangs on start, go to the AVD's Advanced Settings -> Emulated Performance and select Cold boot or reset the image to factory defaults. Debugging Code You can use Android Studio by using the Profile or Debug APK option and selecting your APK. Before you can start debugging, open the Project Structure... dialog and make the following changes: 1. Project -> SDK: Select one of the installed Android SDKs. 2. Modules -> Dependencies Tab -> Module SDK: Select one of the installed Android SDKs. Afterwards, just select your target device and press Debug . If you want to use VSCode instead, you can follow the rest of the guide. Otherwise this section can be skipped. Before debugging it should be ensured that you have an emulator set up or a device connected. There should only be one device or emulator. Otherwise debugging is going to fail because it's unknown which target to use. $ adb devices List of devices attached ce11171b5298cc120c device If adb is not available in the command line, %ANDROID_HOME%\\platform-tools needs to be added to the PATH environment variable. You will need to install the CodeLLDB VSCode extension and then create or modify the .vscode/launch.json file in your checkout and add a launch config. There are a few examples in the launch.json file in the root of the repo. What you need to change is the following: 1. PackageName : This is the package name of the app you want to run, e.g. com.plengine.RendererTest . 1. apk (optional) : If set, the app will be installed first before starting. This should point to the output directory of your configuration. E.g. ${workspaceFolder}/Output/Bin/[BUILD_CONFIG]/RendererTest.apk . { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Android RendererTest\", \"type\": \"lldb\", \"request\": \"custom\", \"targetCreateCommands\": [ \"shell pwsh \\\"${workspaceFolder}/Utilities/DbgAndroidLldb.ps1\\\" -PackageName \\\"com.plengine.RendererTest\\\" -debugTemp \\\"${workspaceFolder}/debugtemp\\\" -apk \\\"${workspaceFolder}/Output/Bin/AndroidNinjaClangDebug64/RendererTest.apk\\\" -MessageBoxOnError\", \"command source ${workspaceFolder}/debugtemp/lldb.setup\" ], \"processCreateCommands\": [ \"shell pwsh \\\"${workspaceFolder}/Utilities/DbgAndroidJdb.ps1\\\" &\", \"continue\" ] } ] } In VSCode you can now select the debug config from the Run and Debug menu and start debugging. Note, if something goes wrong on Windows, a message box will pop up which might not be in the foreground. On Linux, inspect the LLDB log. You can also follow the detailed debugging guide in the FAQ section below. See plEngine log output To see the plEngine log output the following logcat filter can be used. adb logcat plEngine:D *:S FAQ No debugging visualizers loaded You need to ensure that the .lldbinit in the root of the repo can be loaded. Under Linux, add or change the following line in the home ~/.lldbinit file: settings set target.load-cwd-lldbinit true VSCode Debugger does not start / command line debugging If debugging doesn't work or debugging from the command line is preferred, the command line debugger can be started. It gives detailed output. Step 1 : The debugging script is located in Utilities/DbgAndroidLldb.ps1 . To run it manually you will need to run the following in powershell: ./home/[USERNAME]/Code/plEngine/Utilities/DbgAndroidLldb.ps1 -PackageName \"com.plengine.ShaderExplorer\" -debugTemp \"/home/[USERNAME]/Code/plEngine/debugtemp\" -apk \"/home/[USERNAME]/Code/plEngine/Output/Bin/AndroidNinjaClangDebugArm64/ShaderExplorer.apk\" Of course, adjust the paths so they match your local plEngine checkout location and Android build config. This will either fail with an error message or should start the apk on your device, showing the wait for debugger prompt. Step 2 : Start the lldb shell from any LLDB installation of your choosing and run the following commands in the shell: command source /home/[USERNAME]/Code/plEngine/debugtemp/lldb.setup shell pwsh \"/home/[USERNAME]/Code/plEngine/Utilities/DbgAndroidJdb.ps1\" & continue Run each line at a time. No errors should show up. command source is written by DbgAndroidLldb.ps1 and contains the commands to connect to the device and running process. DbgAndroidJdb.ps1 connects the java debugger in a fire and forget fashion to close the Waiting for debugger prompt on the device. Finally continue will start running the paused application. See Also Building plEngine","title":"Building for Android"},{"location":"build/build-android/#building-for-android","text":"","title":"Building for Android"},{"location":"build/build-android/#prerequisites","text":"You need the following to build for Android: Android SDK Platform 10.0 (\"Q\") API-Level 29 Android NDK 26.1 or higher (older should work as well, but untested) Android SDK Build Tools Android SDK Platform-Tools Android Emulator (optional) Java (JRE) Cmake Vulkan SDK 1.3.275 or newer Ninja Powershell 7 (for debugging in VSCode or running convenience scripts) Visual Studio 2022 or Linux Ninja is a build generator used by CMake and needs to be added to the PATH environment variable. The easiest way to install the Android components is to download Android Studio and then to select these from the SDK Manager . Alternatively, you can also install these via the command line tool located in C:\\Users\\[USER]\\AppData\\Local\\Android\\Sdk\\cmdline-tools\\latest\\bin : # From the root of the PL checkout, run: .\\Utilities\\Android\\AndroidEmulator.ps1 -installBuildDependencies # Or run these manually from the ...\\cmdline-tools\\latest\\bin folder: ./sdkmanager \"build-tools;34.0.0\" ./sdkmanager \"cmdline-tools;latest\" ./sdkmanager \"ndk;26.1.10909125\" ./sdkmanager \"platform-tools\" ./sdkmanager \"platforms;android-29\" # Optional for emulator: ./sdkmanager \"emulator\" ./sdkmanager \"extras;google;Android_Emulator_Hypervisor_Driver\" ./sdkmanager \"system-images;android-29;google_apis;x86_64\" # Accept licenses for the above ./sdkmanager --licenses Once installed, the following environment variables need to be set: Change the version to reflect the one you are using. ANDROID_NDK_HOME needs to point to your installed version, by default this is: C:\\Users\\[USERNAME]\\AppData\\Local\\Android\\Sdk\\ndk\\[VERSION] ANDROID_HOME needs to point to your installed version, by default this is: C:\\Users\\[USERNAME]\\AppData\\Local\\Android\\Sdk JAVA_HOME needs to point to a java runtime. Android Studio has its own version so there is no need to download it separately: C:\\Program Files\\Android\\Android Studio\\jbr ANDROID_STUDIO (Optional for debugging). Needs to point to the root of Android Studio, e.g. C:\\Program Files\\Android\\Android Studio . We currently rely on the lldb-server that ships with Android Studio. Alternatively, you can also debug any app with Android Studio once at which point the required files are on the device and this env var is no longer needed.","title":"Prerequisites"},{"location":"build/build-android/#visual-studio-vscode-clion","text":"While you can manually run CMake to use the ninja generator, a more convenient solution is to use CMake's CMakePresets.json which is already configured for Android arm64 and x64 builds. * Visual Studio : Use Visual Studio's open folder functionality. Go to File > Open > Folder... and select the root of the repository. If all environment variables were set correctly VS should automatically configure CMake. Once done, a drop down appears in the VS toolbar, allowing you to select the configuration, e.g. android-arm64-debug . Once changed, VS will start to configure CMake again for the new configuration. Next, select a build target, e.g. libFoundationTest.so which are the foundation unit tests. Note that you can only select applications, not all libraries here. * VSCode : Make sure you have the C/C++ , C/C++ Extension Pack , CMake and CMake Tools plugins installed. Select File > Open Folder... and select the root of the repository. Execute CMake: Select Configure Preset to select the config you wish to use. Make sure CMake runs through without errors. On failure fix any errors and execute CMake: Configure until successful. Finally, execute CMake: Build Target to build the project you want. * CLion : Open settings, go to Build, Execution, Deployment > CMake , select the profile you wish to use and enable it. Make sure CMake configure runs through without errors. Finally, select the build target of choice in the toolbar and press the build button next to it.","title":"Visual Studio / VSCode / CLion"},{"location":"build/build-android/#setting-up-an-emulator-avd","text":"You can either use the Android Studio GUI or the command line to setup the emulator. For the command line option, the avdmanager is usually located in the C:\\[USERNAME]\\admin\\AppData\\Local\\Android\\Sdk\\cmdline-tools\\latest\\bin folder. The following powershell command will create a device that can run PL generated apks: # From the root of the PL checkout, run: ./Utilities/Android/AndroidEmulator.ps1 -installEmulator # Or run this manually from the ...\\cmdline-tools\\latest\\bin folder: ./avdmanager create avd --force --name \"Pixel7\" --abi \"google_apis/x86_64\" --package \"system-images;android-29;google_apis;x86_64\" --device \"pixel_7\"","title":"Setting up an Emulator AVD"},{"location":"build/build-android/#starting-the-emulator","text":"The emulator can be comfortably started from within Android Studio or via the emulator application located in C:\\[USERNAME]\\admin\\AppData\\Local\\Android\\Sdk\\emulator via these powershell commands: # From the root of the PL checkout, run: ./Utilities/Android/AndroidEmulator.ps1 -startEmulator # Or run this manually from the ...\\Sdk\\emulator folder: ./emulator -avd \"Pixel7\" -wipe-data -no-snapshot -no-audio -port 5555 -gpu swiftshader_indirect For better performance, the -gpu host option can be used but it may cause crashes or graphical artifacts. For more information on the available options, see the official emulator hardware acceleration page. The options -wipe-data -no-snapshot -no-audio are not strictly necessary but will provide the same environment our unit tests are run under. To use the GUI instead, open Android Studio, go to Configure>AVD Manager and select Create Virtual Device . Select the Pixel 7 hardware profile. Next, select x86 Images , then select Q (API 29), x86_64 . NOTE: If the emulator hangs on start, go to the AVD's Advanced Settings -> Emulated Performance and select Cold boot or reset the image to factory defaults.","title":"Starting the Emulator"},{"location":"build/build-android/#debugging-code","text":"You can use Android Studio by using the Profile or Debug APK option and selecting your APK. Before you can start debugging, open the Project Structure... dialog and make the following changes: 1. Project -> SDK: Select one of the installed Android SDKs. 2. Modules -> Dependencies Tab -> Module SDK: Select one of the installed Android SDKs. Afterwards, just select your target device and press Debug . If you want to use VSCode instead, you can follow the rest of the guide. Otherwise this section can be skipped. Before debugging it should be ensured that you have an emulator set up or a device connected. There should only be one device or emulator. Otherwise debugging is going to fail because it's unknown which target to use. $ adb devices List of devices attached ce11171b5298cc120c device If adb is not available in the command line, %ANDROID_HOME%\\platform-tools needs to be added to the PATH environment variable. You will need to install the CodeLLDB VSCode extension and then create or modify the .vscode/launch.json file in your checkout and add a launch config. There are a few examples in the launch.json file in the root of the repo. What you need to change is the following: 1. PackageName : This is the package name of the app you want to run, e.g. com.plengine.RendererTest . 1. apk (optional) : If set, the app will be installed first before starting. This should point to the output directory of your configuration. E.g. ${workspaceFolder}/Output/Bin/[BUILD_CONFIG]/RendererTest.apk . { \"version\": \"0.2.0\", \"configurations\": [ { \"name\": \"Android RendererTest\", \"type\": \"lldb\", \"request\": \"custom\", \"targetCreateCommands\": [ \"shell pwsh \\\"${workspaceFolder}/Utilities/DbgAndroidLldb.ps1\\\" -PackageName \\\"com.plengine.RendererTest\\\" -debugTemp \\\"${workspaceFolder}/debugtemp\\\" -apk \\\"${workspaceFolder}/Output/Bin/AndroidNinjaClangDebug64/RendererTest.apk\\\" -MessageBoxOnError\", \"command source ${workspaceFolder}/debugtemp/lldb.setup\" ], \"processCreateCommands\": [ \"shell pwsh \\\"${workspaceFolder}/Utilities/DbgAndroidJdb.ps1\\\" &\", \"continue\" ] } ] } In VSCode you can now select the debug config from the Run and Debug menu and start debugging. Note, if something goes wrong on Windows, a message box will pop up which might not be in the foreground. On Linux, inspect the LLDB log. You can also follow the detailed debugging guide in the FAQ section below.","title":"Debugging Code"},{"location":"build/build-android/#see-plengine-log-output","text":"To see the plEngine log output the following logcat filter can be used. adb logcat plEngine:D *:S","title":"See plEngine log output"},{"location":"build/build-android/#faq","text":"","title":"FAQ"},{"location":"build/build-android/#no-debugging-visualizers-loaded","text":"You need to ensure that the .lldbinit in the root of the repo can be loaded. Under Linux, add or change the following line in the home ~/.lldbinit file: settings set target.load-cwd-lldbinit true","title":"No debugging visualizers loaded"},{"location":"build/build-android/#vscode-debugger-does-not-start-command-line-debugging","text":"If debugging doesn't work or debugging from the command line is preferred, the command line debugger can be started. It gives detailed output. Step 1 : The debugging script is located in Utilities/DbgAndroidLldb.ps1 . To run it manually you will need to run the following in powershell: ./home/[USERNAME]/Code/plEngine/Utilities/DbgAndroidLldb.ps1 -PackageName \"com.plengine.ShaderExplorer\" -debugTemp \"/home/[USERNAME]/Code/plEngine/debugtemp\" -apk \"/home/[USERNAME]/Code/plEngine/Output/Bin/AndroidNinjaClangDebugArm64/ShaderExplorer.apk\" Of course, adjust the paths so they match your local plEngine checkout location and Android build config. This will either fail with an error message or should start the apk on your device, showing the wait for debugger prompt. Step 2 : Start the lldb shell from any LLDB installation of your choosing and run the following commands in the shell: command source /home/[USERNAME]/Code/plEngine/debugtemp/lldb.setup shell pwsh \"/home/[USERNAME]/Code/plEngine/Utilities/DbgAndroidJdb.ps1\" & continue Run each line at a time. No errors should show up. command source is written by DbgAndroidLldb.ps1 and contains the commands to connect to the device and running process. DbgAndroidJdb.ps1 connects the java debugger in a fire and forget fashion to close the Waiting for debugger prompt on the device. Finally continue will start running the paused application.","title":"VSCode Debugger does not start / command line debugging"},{"location":"build/build-android/#see-also","text":"Building plEngine","title":"See Also"},{"location":"build/build-linux/","text":"Building for Linux Linux support for plEngine is currently in development and still to be considered experimental and incomplete. You can try it, but don't expect to be able to work productively with it. For rendering the new Vulkan backend is used, which itself is also very much in development yet. We welcome help finding and fixing issues. Supported Compilers / Make Systems The plEngine CMake scripts support the following compilers when building for Linux: GCC Clang C++17 support is required, so make sure that your respective compiler supports it. These generators are currently supported for Linux: Unix Makefiles Ninja Automatic Setup The RunCMake.sh script in the root folder of plEngine can be used to automatically install all required packages and run CMake, so that you can start building right away. This script currently supports these distributions: Ubuntu 22 Linux Mint 21 We welcome contributions to add support for more distributions. :warning: If the scripts prints a warning about Qt 6.3.0 or newer not being present in your package manager, you will have to install Qt 6.3.0 or newer manually. See Installing Qt 6 Manually GCC When running the script the first time, execute: ./RunCMake.sh --setup This will install all required packages for your distribution and then generate the make files required to build the Dev version of plEngine. To build the Dev build, execute: ninja -C build-Dev This build command is also given by RunCMake.sh as the final output. If you change any CMake files or add new source files it is sufficient to run: ./RunCMake.sh This only invokes CMake, without checking for missing packages. To build a different build type then Dev , pass the additional --build-type argument: ./RunCMake.sh --build-type Debug Clang If you would like to use Clang instead of GCC, simply add --clang to all invocations of RunCMake.sh : ./RunCMake.sh --setup --clang ./RunCMake.sh --clang ./RunCMake.sh --build-type Debug --clang Installing Qt 6 Manually Some distributions provide quite outdated versions of Qt 6 and the plEngine Editor requires at least Qt 6.3.0 due to a bug that exists in previous versions of Qt and prevents the 3D viewport in the Editor from working correctly. You have the following options: Install through aqtinstall Install Qt 6 through the official installer Build from source Once you have obtained a recent version of Qt, you have two options so that the plEngine cmake scripts find it: 1) Add the install location permantently to your PATH environment variable 2) Specify the install location when calling RunCMake.sh like this: ```bash PATH=/path/to/qt6/install:$PATH ./RunCMake.sh ``` Manual Setup If you want to setup things manually or your distribution is not supported by the RunCMake.sh script, you will most likely need all of the following packages: C++17 compliant compiler (GCC or Clang) CMake 3.20 or newer uuid-dev Qt6 (version 6.3 or newer) ninja or gnu-make libxrandr libxinerama libxcursor libxi libfreetype libtinfo5 Then invoke CMake with the following arguments: Option Explanation -B build Path to the build directory. -S . Path to the plEngine root. -G Ninja Choose to generate Ninja makefiles. Optional, if not provided gnu-make will be used. -DCMAKE_CXX_COMPILER=g++-12 Specify the C++ compiler to use. Optional, if not provided the system default will be used. -DCMAKE_C_COMPILER=gcc-12 Specify the C compiler to use. Optional, if not provided the system default will be used. -Dpl_EXPERIMENTAL_EDITOR_ON_LINUX=ON Build the plEngine editor on Linux. This is currently experimental and might have significant bugs. -Dpl_BUILD_EXPERIMENTAL_VULKAN=ON Build the Vulkan renderer. This is currently experimental and might have significant bugs. -DCMAKE_BUILD_TYPE=Dev Specify the build type to use. -DCMAKE_EXPORT_COMPILE_COMMANDS=ON Generate a compile_commands.json file to be used for code completion in editors like Visual Studio Code. -Dpl_QT_DIR=/path/to/qt6 Manually specify the path cmake should look for Qt 6 in. -Dpl_ENABLE_FOLDER_UNITY_FILES=OFF Disable unity builds. This increases compile times but might help certain editors to provide better code completion. Example usage: mkdir build cmake -B build -S . -G Ninja -DCMAKE_CXX_COMPILER=g++-12 -DCMAKE_C_COMPILER=gcc-12 -Dpl_EXPERIMENTAL_EDITOR_ON_LINUX=ON -Dpl_BUILD_EXPERIMENTAL_VULKAN=ON -DCMAKE_BUILD_TYPE=Dev -DCMAKE_EXPORT_COMPILE_COMMANDS=ON Using Qt Creator The root of the repository can also be opened in Qt Creator, which will generally do a good job at finding the Qt location on its own. See Also Building plEngine","title":"Building for Linux"},{"location":"build/build-linux/#building-for-linux","text":"Linux support for plEngine is currently in development and still to be considered experimental and incomplete. You can try it, but don't expect to be able to work productively with it. For rendering the new Vulkan backend is used, which itself is also very much in development yet. We welcome help finding and fixing issues.","title":"Building for Linux"},{"location":"build/build-linux/#supported-compilers-make-systems","text":"The plEngine CMake scripts support the following compilers when building for Linux: GCC Clang C++17 support is required, so make sure that your respective compiler supports it. These generators are currently supported for Linux: Unix Makefiles Ninja","title":"Supported Compilers / Make Systems"},{"location":"build/build-linux/#automatic-setup","text":"The RunCMake.sh script in the root folder of plEngine can be used to automatically install all required packages and run CMake, so that you can start building right away. This script currently supports these distributions: Ubuntu 22 Linux Mint 21 We welcome contributions to add support for more distributions. :warning: If the scripts prints a warning about Qt 6.3.0 or newer not being present in your package manager, you will have to install Qt 6.3.0 or newer manually. See Installing Qt 6 Manually","title":"Automatic Setup"},{"location":"build/build-linux/#gcc","text":"When running the script the first time, execute: ./RunCMake.sh --setup This will install all required packages for your distribution and then generate the make files required to build the Dev version of plEngine. To build the Dev build, execute: ninja -C build-Dev This build command is also given by RunCMake.sh as the final output. If you change any CMake files or add new source files it is sufficient to run: ./RunCMake.sh This only invokes CMake, without checking for missing packages. To build a different build type then Dev , pass the additional --build-type argument: ./RunCMake.sh --build-type Debug","title":"GCC"},{"location":"build/build-linux/#clang","text":"If you would like to use Clang instead of GCC, simply add --clang to all invocations of RunCMake.sh : ./RunCMake.sh --setup --clang ./RunCMake.sh --clang ./RunCMake.sh --build-type Debug --clang","title":"Clang"},{"location":"build/build-linux/#installing-qt-6-manually","text":"Some distributions provide quite outdated versions of Qt 6 and the plEngine Editor requires at least Qt 6.3.0 due to a bug that exists in previous versions of Qt and prevents the 3D viewport in the Editor from working correctly. You have the following options: Install through aqtinstall Install Qt 6 through the official installer Build from source Once you have obtained a recent version of Qt, you have two options so that the plEngine cmake scripts find it: 1) Add the install location permantently to your PATH environment variable 2) Specify the install location when calling RunCMake.sh like this: ```bash PATH=/path/to/qt6/install:$PATH ./RunCMake.sh ```","title":"Installing Qt 6 Manually"},{"location":"build/build-linux/#manual-setup","text":"If you want to setup things manually or your distribution is not supported by the RunCMake.sh script, you will most likely need all of the following packages: C++17 compliant compiler (GCC or Clang) CMake 3.20 or newer uuid-dev Qt6 (version 6.3 or newer) ninja or gnu-make libxrandr libxinerama libxcursor libxi libfreetype libtinfo5 Then invoke CMake with the following arguments: Option Explanation -B build Path to the build directory. -S . Path to the plEngine root. -G Ninja Choose to generate Ninja makefiles. Optional, if not provided gnu-make will be used. -DCMAKE_CXX_COMPILER=g++-12 Specify the C++ compiler to use. Optional, if not provided the system default will be used. -DCMAKE_C_COMPILER=gcc-12 Specify the C compiler to use. Optional, if not provided the system default will be used. -Dpl_EXPERIMENTAL_EDITOR_ON_LINUX=ON Build the plEngine editor on Linux. This is currently experimental and might have significant bugs. -Dpl_BUILD_EXPERIMENTAL_VULKAN=ON Build the Vulkan renderer. This is currently experimental and might have significant bugs. -DCMAKE_BUILD_TYPE=Dev Specify the build type to use. -DCMAKE_EXPORT_COMPILE_COMMANDS=ON Generate a compile_commands.json file to be used for code completion in editors like Visual Studio Code. -Dpl_QT_DIR=/path/to/qt6 Manually specify the path cmake should look for Qt 6 in. -Dpl_ENABLE_FOLDER_UNITY_FILES=OFF Disable unity builds. This increases compile times but might help certain editors to provide better code completion. Example usage: mkdir build cmake -B build -S . -G Ninja -DCMAKE_CXX_COMPILER=g++-12 -DCMAKE_C_COMPILER=gcc-12 -Dpl_EXPERIMENTAL_EDITOR_ON_LINUX=ON -Dpl_BUILD_EXPERIMENTAL_VULKAN=ON -DCMAKE_BUILD_TYPE=Dev -DCMAKE_EXPORT_COMPILE_COMMANDS=ON","title":"Manual Setup"},{"location":"build/build-linux/#using-qt-creator","text":"The root of the repository can also be opened in Qt Creator, which will generally do a good job at finding the Qt location on its own.","title":"Using Qt Creator"},{"location":"build/build-linux/#see-also","text":"Building plEngine","title":"See Also"},{"location":"build/build-macos/","text":"MacOS Builds Prerequisites Supported Compilers You can compile Plasma through one of these methods: XCode 5.1.1 or higher (GCC / Clang) 64 Bit Makefiles 64 Bit Dependencies You need to install these libraries: XQuartz 2.7.5 SFML-2.5.1 Qt 5.11 (optional) A good way to do so is via homebrew : brew update brew install Caskroom/cask/xquartz brew install qt6 brew install sfml Using the command line Run CMake with CMAKE_PREFIX_PATH pointing to the dependencies listed above. In this example, a build folder is created under the root of the repo and cmake is executed in it: cmake -DCMAKE_PREFIX_PATH=/usr/local/Cellar/qt/5.13.1/;/usr/local/Cellar/sfml/2.5.1/ -DPLASMA_ENABLE_QT_SUPPORT=1 -DCMAKE_BUILD_TYPE=RelWithDebInfo -DPLASMA_ENABLE_FOLDER_UNITY_FILES=$(unityfiles) -G \"Xcode\" ../ Afterwards the generated solution can be opened in XCode. See Also Building PlasmaEngine","title":"MacOS Builds"},{"location":"build/build-macos/#macos-builds","text":"","title":"MacOS Builds"},{"location":"build/build-macos/#prerequisites","text":"","title":"Prerequisites"},{"location":"build/build-macos/#supported-compilers","text":"You can compile Plasma through one of these methods: XCode 5.1.1 or higher (GCC / Clang) 64 Bit Makefiles 64 Bit","title":"Supported Compilers"},{"location":"build/build-macos/#dependencies","text":"You need to install these libraries: XQuartz 2.7.5 SFML-2.5.1 Qt 5.11 (optional) A good way to do so is via homebrew : brew update brew install Caskroom/cask/xquartz brew install qt6 brew install sfml","title":"Dependencies"},{"location":"build/build-macos/#using-the-command-line","text":"Run CMake with CMAKE_PREFIX_PATH pointing to the dependencies listed above. In this example, a build folder is created under the root of the repo and cmake is executed in it: cmake -DCMAKE_PREFIX_PATH=/usr/local/Cellar/qt/5.13.1/;/usr/local/Cellar/sfml/2.5.1/ -DPLASMA_ENABLE_QT_SUPPORT=1 -DCMAKE_BUILD_TYPE=RelWithDebInfo -DPLASMA_ENABLE_FOLDER_UNITY_FILES=$(unityfiles) -G \"Xcode\" ../ Afterwards the generated solution can be opened in XCode.","title":"Using the command line"},{"location":"build/build-macos/#see-also","text":"Building PlasmaEngine","title":"See Also"},{"location":"build/build-uwp/","text":"UWP Builds This page describes how to build Plasma for the Universal Windows Platform (UWP). For desktop builds, see this page . Note that only a subset of Plasma's functionality is officially maintained and supported on UWP. In general UWP support is not a priority for us. Prerequisites Install the desktop Windows prerequisites . Microsoft Visual Studio In Visual Studio, install these additional workloads: Universal Windows Platform Development CMake CMake is used as the build system. For UWP you need to have a custom installation. Generate the Solution To generate a solution for UWP, you need to pass a toolchain file to CMake. The file is located in the Plasma repository under Code/BuildSystem/CMake/toolchain-winstore.cmake . Using the CMake GUI Start the CMake GUI application. Create a new solution by pointing Where to build the binaries to a new location. Press Configure once, a dialog will show up to choose the generator. Choose the desired Visual Studio generator at the top. Depending on your target device, choose the platform. For instance, for HoloLens 1 select Win32 . At the bottom select Specify toolchain file for cross-compiling . On the next screen set the toolchain file PathToEzRepository /Code/BuildSystem/CMake/toolchain-winstore.cmake Using the command line Run CMake with this argument: -DCMAKE_TOOLCHAIN_FILE= PathToEzRepository /Code/BuildSystem/CMake/toolchain-winstore.cmake Building the Code Open the generated solution with Visual Studio and build everything. See Also Windows Builds","title":"UWP Builds"},{"location":"build/build-uwp/#uwp-builds","text":"This page describes how to build Plasma for the Universal Windows Platform (UWP). For desktop builds, see this page . Note that only a subset of Plasma's functionality is officially maintained and supported on UWP. In general UWP support is not a priority for us.","title":"UWP Builds"},{"location":"build/build-uwp/#prerequisites","text":"Install the desktop Windows prerequisites .","title":"Prerequisites"},{"location":"build/build-uwp/#microsoft-visual-studio","text":"In Visual Studio, install these additional workloads: Universal Windows Platform Development","title":"Microsoft Visual Studio"},{"location":"build/build-uwp/#cmake","text":"CMake is used as the build system. For UWP you need to have a custom installation.","title":"CMake"},{"location":"build/build-uwp/#generate-the-solution","text":"To generate a solution for UWP, you need to pass a toolchain file to CMake. The file is located in the Plasma repository under Code/BuildSystem/CMake/toolchain-winstore.cmake .","title":"Generate the Solution"},{"location":"build/build-uwp/#using-the-cmake-gui","text":"Start the CMake GUI application. Create a new solution by pointing Where to build the binaries to a new location. Press Configure once, a dialog will show up to choose the generator. Choose the desired Visual Studio generator at the top. Depending on your target device, choose the platform. For instance, for HoloLens 1 select Win32 . At the bottom select Specify toolchain file for cross-compiling . On the next screen set the toolchain file PathToEzRepository /Code/BuildSystem/CMake/toolchain-winstore.cmake","title":"Using the CMake GUI"},{"location":"build/build-uwp/#using-the-command-line","text":"Run CMake with this argument: -DCMAKE_TOOLCHAIN_FILE= PathToEzRepository /Code/BuildSystem/CMake/toolchain-winstore.cmake","title":"Using the command line"},{"location":"build/build-uwp/#building-the-code","text":"Open the generated solution with Visual Studio and build everything.","title":"Building the Code"},{"location":"build/build-uwp/#see-also","text":"Windows Builds","title":"See Also"},{"location":"build/build-windows/","text":"Windows Builds This page describes how to build Plasma for desktop Windows. For UWP builds, see this page . Prerequisites This software has to be installed manually. Microsoft Visual Studio Visual Studio is the only supported IDE on Windows. It is sufficient to install the free Community edition. These versions are currently supported: Visual Studio 2019 64 Bit Visual Studio 2022 64 Bit These workloads have to be installed: Desktop Development with C++ Game Development with C++ .Net Desktop Development CMake (optional) CMake is used as the build system. On Windows you only need to install CMake if you want to use the CMake GUI to choose custom CMake configurations . If you only use the provided GenerateXYZ.bat scripts, those will use a cmake.exe that comes with the Plasma repository. Generate the Solution Using the Generate Scripts In the root folder of the Plasma repository you will find a couple of .bat files, such as: GenerateWin64vs2019.bat GenerateWin64vs2022.bat Run one of them to generate a Visual Studio solution for your preferred compiler. If these scripts fail, you most likely don't have all the prerequisites installed. They also sometimes fail, if Visual Studio recently installed an update and you haven't rebooted your PC since. Usually when this script fails it is due to common issues with CMake or the MSVC installation. Read the error messages carefully and search the internet, you'll usually find a solution quickly. Once the script finished successfully, there will be a Workspace folder in the Plasma root folder. You fill find a Plasma_***.sln file in the respective folder for the Visual Studio version that you chose. Manually Running CMake This step requires that you install CMake yourself. Run the CMake GUI and configure the build options . Building the Code Open the generated solution with Visual Studio and build everything. See Also Building PlasmaEngine Building with Clang on Windows","title":"Windows Builds"},{"location":"build/build-windows/#windows-builds","text":"This page describes how to build Plasma for desktop Windows. For UWP builds, see this page .","title":"Windows Builds"},{"location":"build/build-windows/#prerequisites","text":"This software has to be installed manually.","title":"Prerequisites"},{"location":"build/build-windows/#microsoft-visual-studio","text":"Visual Studio is the only supported IDE on Windows. It is sufficient to install the free Community edition. These versions are currently supported: Visual Studio 2019 64 Bit Visual Studio 2022 64 Bit These workloads have to be installed: Desktop Development with C++ Game Development with C++ .Net Desktop Development","title":"Microsoft Visual Studio"},{"location":"build/build-windows/#cmake-optional","text":"CMake is used as the build system. On Windows you only need to install CMake if you want to use the CMake GUI to choose custom CMake configurations . If you only use the provided GenerateXYZ.bat scripts, those will use a cmake.exe that comes with the Plasma repository.","title":"CMake (optional)"},{"location":"build/build-windows/#generate-the-solution","text":"","title":"Generate the Solution"},{"location":"build/build-windows/#using-the-generate-scripts","text":"In the root folder of the Plasma repository you will find a couple of .bat files, such as: GenerateWin64vs2019.bat GenerateWin64vs2022.bat Run one of them to generate a Visual Studio solution for your preferred compiler. If these scripts fail, you most likely don't have all the prerequisites installed. They also sometimes fail, if Visual Studio recently installed an update and you haven't rebooted your PC since. Usually when this script fails it is due to common issues with CMake or the MSVC installation. Read the error messages carefully and search the internet, you'll usually find a solution quickly. Once the script finished successfully, there will be a Workspace folder in the Plasma root folder. You fill find a Plasma_***.sln file in the respective folder for the Visual Studio version that you chose.","title":"Using the Generate Scripts"},{"location":"build/build-windows/#manually-running-cmake","text":"This step requires that you install CMake yourself. Run the CMake GUI and configure the build options .","title":"Manually Running CMake"},{"location":"build/build-windows/#building-the-code","text":"Open the generated solution with Visual Studio and build everything.","title":"Building the Code"},{"location":"build/build-windows/#see-also","text":"Building PlasmaEngine Building with Clang on Windows","title":"See Also"},{"location":"build/building-plasma/","text":"Building Plasma Engine This article describes how you can build the engine yourself, which enables you to extend the engine with custom functionality. Platform Builds Windows Builds UWP Builds Linux Builds MacOS Builds Android Builds Building with Clang on Windows Build Types PLASMA sets up three build configuration types: Debug Dev Shipping While developing your game you should either use a Debug or a Dev build. The Debug build is best when you want to use a C++ debugger to investigate problems. It includes the necessary debug symbols and has many optimizations disabled, which makes it much easier to step through the code. Debug builds are significantly slower than the other build types. The Dev build has most of the optimizations enabled, yet still includes debug symbols . The Dev build is 3x to 10x faster than a Debug build in most cases and is very close to the speed of a Shipping build. Stepping through the C++ code with a debugger is possible, though it often behaves erratic due to the optimizations (inlining and such). For most developers the Dev build should be the main configuration to use. The Shipping build has all optimizations enabled. It does not include debug symbols anymore and it also has all the developer features disabled. That means things like plInspector won't work here. Similarly, features like allocation tracking (for detecting memory leaks) and profiling features are disabled as well. See Also Supported Platforms Plasma Engine as a Submodule","title":"Building Plasma Engine"},{"location":"build/building-plasma/#building-plasma-engine","text":"This article describes how you can build the engine yourself, which enables you to extend the engine with custom functionality.","title":"Building Plasma Engine"},{"location":"build/building-plasma/#platform-builds","text":"Windows Builds UWP Builds Linux Builds MacOS Builds Android Builds Building with Clang on Windows","title":"Platform Builds"},{"location":"build/building-plasma/#build-types","text":"PLASMA sets up three build configuration types: Debug Dev Shipping While developing your game you should either use a Debug or a Dev build. The Debug build is best when you want to use a C++ debugger to investigate problems. It includes the necessary debug symbols and has many optimizations disabled, which makes it much easier to step through the code. Debug builds are significantly slower than the other build types. The Dev build has most of the optimizations enabled, yet still includes debug symbols . The Dev build is 3x to 10x faster than a Debug build in most cases and is very close to the speed of a Shipping build. Stepping through the C++ code with a debugger is possible, though it often behaves erratic due to the optimizations (inlining and such). For most developers the Dev build should be the main configuration to use. The Shipping build has all optimizations enabled. It does not include debug symbols anymore and it also has all the developer features disabled. That means things like plInspector won't work here. Similarly, features like allocation tracking (for detecting memory leaks) and profiling features are disabled as well.","title":"Build Types"},{"location":"build/building-plasma/#see-also","text":"Supported Platforms Plasma Engine as a Submodule","title":"See Also"},{"location":"build/clang-on-windows/","text":"Building with Clang on Windows You can build Plasma Engine using Clang on Windows. This can be useful to find and fix compilation errors and warnings, that do not happen with MSVC. However, as Clang support on Windows is still experimental, you may not be able to build a working executable. Using Clang/LLVM with the CMake GUI Get the latest clang windows distribution: https://releases.llvm.org/download.html (the 64-bit version is recommended) Create a new solution for the Clang build by pointing Where to build the binaries to a new location. Press Configure once, a dialog will show up. Choose Ninja as the generator. (Note: Get ninja from https://ninja-build.org and put it in your PATH ) Choose Specify native compilers then hit Finish . Specify the C and C++ compiler. When using the default paths they are located at: C: C:/Program Files/LLVM/bin/clang.exe C++: C:/Program Files/LLVM/bin/clang++.exe Hit Finish You will now get an error from cmake No CMAKE_RC_COMPILER could be found . Check the Advanced checkbox to show additional options and point CMAKE_RC_COMPILER to C:\\Program Files (x86)\\Windows Kits\\10\\bin\\<windows-sdk-version>\\x64\\rc.exe (for example C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64\\rc.exe ). Hit Configure Hit Generate cd into the build location and run ninja to build. Using the Clang frontend for Visual Studio with the CMake GUI The clang frontend for the Visual Studio Compiler is no longer in development. Using the official LLVM clang is recommended. Create a new solution for the Clang build by pointing Where to build the binaries to a new location. Press Configure once, a dialog will show up. Choose the desired Visual Studio generator at the top. In the field Optional toolset to use (argument to -T) type v140_clang_c2 Finish the dialog and run 'Configure'. Check the Advanced checkbox to show additional options. Disable PLASMA_USE_PCH , as the Clang build will not work with precompiled headers. You may also want to disable PLASMA_ENABLE_FOLDER_UNITY_FILES as that makes it easier to see from which file a compilation error originated. Finish the CMake configuration , open the solution in Visual Studio and start compiling. See Also Windows Builds","title":"Building with Clang on Windows"},{"location":"build/clang-on-windows/#building-with-clang-on-windows","text":"You can build Plasma Engine using Clang on Windows. This can be useful to find and fix compilation errors and warnings, that do not happen with MSVC. However, as Clang support on Windows is still experimental, you may not be able to build a working executable.","title":"Building with Clang on Windows"},{"location":"build/clang-on-windows/#using-clangllvm-with-the-cmake-gui","text":"Get the latest clang windows distribution: https://releases.llvm.org/download.html (the 64-bit version is recommended) Create a new solution for the Clang build by pointing Where to build the binaries to a new location. Press Configure once, a dialog will show up. Choose Ninja as the generator. (Note: Get ninja from https://ninja-build.org and put it in your PATH ) Choose Specify native compilers then hit Finish . Specify the C and C++ compiler. When using the default paths they are located at: C: C:/Program Files/LLVM/bin/clang.exe C++: C:/Program Files/LLVM/bin/clang++.exe Hit Finish You will now get an error from cmake No CMAKE_RC_COMPILER could be found . Check the Advanced checkbox to show additional options and point CMAKE_RC_COMPILER to C:\\Program Files (x86)\\Windows Kits\\10\\bin\\<windows-sdk-version>\\x64\\rc.exe (for example C:\\Program Files (x86)\\Windows Kits\\10\\bin\\10.0.19041.0\\x64\\rc.exe ). Hit Configure Hit Generate cd into the build location and run ninja to build.","title":"Using Clang/LLVM with the CMake GUI"},{"location":"build/clang-on-windows/#using-the-clang-frontend-for-visual-studio-with-the-cmake-gui","text":"The clang frontend for the Visual Studio Compiler is no longer in development. Using the official LLVM clang is recommended. Create a new solution for the Clang build by pointing Where to build the binaries to a new location. Press Configure once, a dialog will show up. Choose the desired Visual Studio generator at the top. In the field Optional toolset to use (argument to -T) type v140_clang_c2 Finish the dialog and run 'Configure'. Check the Advanced checkbox to show additional options. Disable PLASMA_USE_PCH , as the Clang build will not work with precompiled headers. You may also want to disable PLASMA_ENABLE_FOLDER_UNITY_FILES as that makes it easier to see from which file a compilation error originated. Finish the CMake configuration , open the solution in Visual Studio and start compiling.","title":"Using the Clang frontend for Visual Studio with the CMake GUI"},{"location":"build/clang-on-windows/#see-also","text":"Windows Builds","title":"See Also"},{"location":"build/cmake-config/","text":"CMake Setup To generate a solution, run the CMake GUI. Specify Where is the source code and Where to build the binaries , then run Configure . As a generator, pick Visual Studio 2022 x64 (or one of the other supported platforms ). PLASMA_ENABLE_QT_SUPPORT Disable this setting, if you want to compile Plasma without Qt. This will remove all editor code and several tools from the final solution. The default is on . When possible the Plasma CMake scripts will automatically download Qt libraries and set everything up for you. On configurations for which we do not support fully automatic setup, you need to install Qt manually and then set set PLASMA_QT_DIR to its installation folder. PLASMA_BUILD_FMOD Enable this, if you want to FMOD sound support in your build. One Windows and Linux the default is on . PLASMA_BUILD_PHYSX Enable this, if you want to add NVIDIA PhysX support to your build. Once enabled, the next run of \"Configure\" will automatically download PhysX binaries (Windows only) and set the PLASMA_PHYSX_SDK variable accordingly. The default is off . Note that PhysX support has been superseded by the Jolt Physics integration . PLASMA_BUILD_RMLUI Enable this, if you want to add support for RmlUi to your build. The default is on . Once you have configured everything, run Generate and then Open Project . Adding a Custom Project The easiest way to get started with a custom project, is to use the C++ project generation . Another method is to copy an existing sample, such as the Sample Game Plugin . For starters, just create it in the same location, within the Plasma source tree. If you want to move it into your own repository, you can then reference its location as an external project (see below). External Projects The options PLASMA_EXTERNAL_PROJECT_1-3 allow you to specify folders outside the Plasma repository, which will be integrated into the solution. This is the most practical way to store your own code in a separate repository, yet have it all compiled in the same solution. This makes building, linking and debugging code as convenient as if it was stored inside the Plasma file structure. Build Filter The option PLASMA_BUILD_FILTER allows you to strip down the code that is included in the solution. This is mainly meant for use cases where Plasma is integrated as a submodule and you only need parts of its functionality. Advanced CMake Options Checking Advanced in the CMake GUI will show additional options to configure the Plasma build. These are mostly used to remove specific 3rd party code (and all dependent features). This is particularly helpful, if you want to buildPlasmaon a platform on which one of the dependencies may not compile. See Also Supported Platforms C++ Project Generation PlasmaEngine as a Submodule","title":"CMake Setup"},{"location":"build/cmake-config/#cmake-setup","text":"To generate a solution, run the CMake GUI. Specify Where is the source code and Where to build the binaries , then run Configure . As a generator, pick Visual Studio 2022 x64 (or one of the other supported platforms ). PLASMA_ENABLE_QT_SUPPORT Disable this setting, if you want to compile Plasma without Qt. This will remove all editor code and several tools from the final solution. The default is on . When possible the Plasma CMake scripts will automatically download Qt libraries and set everything up for you. On configurations for which we do not support fully automatic setup, you need to install Qt manually and then set set PLASMA_QT_DIR to its installation folder. PLASMA_BUILD_FMOD Enable this, if you want to FMOD sound support in your build. One Windows and Linux the default is on . PLASMA_BUILD_PHYSX Enable this, if you want to add NVIDIA PhysX support to your build. Once enabled, the next run of \"Configure\" will automatically download PhysX binaries (Windows only) and set the PLASMA_PHYSX_SDK variable accordingly. The default is off . Note that PhysX support has been superseded by the Jolt Physics integration . PLASMA_BUILD_RMLUI Enable this, if you want to add support for RmlUi to your build. The default is on . Once you have configured everything, run Generate and then Open Project .","title":"CMake Setup"},{"location":"build/cmake-config/#adding-a-custom-project","text":"The easiest way to get started with a custom project, is to use the C++ project generation . Another method is to copy an existing sample, such as the Sample Game Plugin . For starters, just create it in the same location, within the Plasma source tree. If you want to move it into your own repository, you can then reference its location as an external project (see below).","title":"Adding a Custom Project"},{"location":"build/cmake-config/#external-projects","text":"The options PLASMA_EXTERNAL_PROJECT_1-3 allow you to specify folders outside the Plasma repository, which will be integrated into the solution. This is the most practical way to store your own code in a separate repository, yet have it all compiled in the same solution. This makes building, linking and debugging code as convenient as if it was stored inside the Plasma file structure.","title":"External Projects"},{"location":"build/cmake-config/#build-filter","text":"The option PLASMA_BUILD_FILTER allows you to strip down the code that is included in the solution. This is mainly meant for use cases where Plasma is integrated as a submodule and you only need parts of its functionality.","title":"Build Filter"},{"location":"build/cmake-config/#advanced-cmake-options","text":"Checking Advanced in the CMake GUI will show additional options to configure the Plasma build. These are mostly used to remove specific 3rd party code (and all dependent features). This is particularly helpful, if you want to buildPlasmaon a platform on which one of the dependencies may not compile.","title":"Advanced CMake Options"},{"location":"build/cmake-config/#see-also","text":"Supported Platforms C++ Project Generation PlasmaEngine as a Submodule","title":"See Also"},{"location":"build/sdk-root/","text":"SDK Root Folder When the engine launches, one of the first things it usually does, is to detect the exact path of the SDK root folder . This folder is a so called special directory and is mostly used when mounting data directories . Special directories are referenced with a \">\" at the beginning, and are only allowed in few places, such as when adding data directories. For example, the 'base' data directory is mounted like this: plFileSystem::AddDataDirectory(\">sdk/Data/Base\"); This adds the folder \"Data/Base\" that is located inside the folder where the SDK (Plasma Engine) is stored. Default Strategy for Locating SDK Root The default strategy by which the engine detects the SDK root folder, is to start at the location of the application binary, and walk the file structure up, until it finds a folder, which contains the sub-folders \"Data/Base\". So for instance, if the running application is located in C:/PlasmaEngine/Bin/MyGame.exe , the search will start in C:/PlasmaEngine/Bin , where no such folder is found. Then it will continue in C:/PlasmaEngine . That folder does have the sub-structure C:/PlasmaEngine/Data/Base , so the SDK root is determined to be C:/PlasmaEngine . This strategy works, as long as the application binary is located somewhere inside the Plasma Engine SDK. Redirecting to SDK Root If you use a different file structure, the default strategy won't work. This is commonly the case when integrating Plasma Engine as a Submodule . For example your file structure may look like this: C:/MyRepo C:/MyRepo/PlasmaEngine-module/ ... C:/MyRepo/Bin/MyGame.exe C:/MyRepo/OtherData/ ... Here Plasma Engine is integrated into another repository. The 'Bin' folder is top level, just as the 'Plasma Engine-module' folder. To enable such a pattern, you can place a 'redirection file', which points to the SDK root folder. The file has to be called plSdkRoot.txt and it must be located somewhere along the path that the default strategy searches. In this case it would be put into C:/MyRepo/plSdkRoot.txt and it would contain the string PlasmaEngine-module . This way, when the engine searches for the folder that contains 'Data/Base', it will reach C:/MyRepo , see the plSdkRoot.txt file, read its content, append the relative path inside to its current path ( C:/MyRepo/PlasmaEngine-module ) and find C:/MyRepo/PlasmaEngine-module/Data/Base , which means it determines C:/MyRepo/PlasmaEngine-module to be the SDK root folder. Using a redirection file is the least invasive method and it works for all Plasma applications, e.g. the editor, samples and tools. Custom SDK Root You can fully control where the SDK root should be and how it is found, if you write your own application (TODO) . During early startup you can simply set the path of the SDK root folder with plFileSystem::SetSdkRootDirectory() . This can be preferable when you use a very different structure. Note that this method will only work for applications that you control. Tools such as plInspector or the editor expect to find the SDK root through the default search strategy (or through a redirection file). When to Redirect At All The SDK root folder doesn't need to point to the folder where Plasma Engine is stored. This is only necessary, when you really need the data that is stored in Data/Base . If you only use a fraction of Plasma, for example only the Foundation library, or not the editor and rendering code, then you can also use a very different folder as your root (for example C:/MyRepo in the example above). In such cases your application would almost certainly specify its custom SDK root in its startup code directly. See Also Plasma Engine as a Submodule CMake Setup Building Plasma Engine","title":"SDK Root Folder"},{"location":"build/sdk-root/#sdk-root-folder","text":"When the engine launches, one of the first things it usually does, is to detect the exact path of the SDK root folder . This folder is a so called special directory and is mostly used when mounting data directories . Special directories are referenced with a \">\" at the beginning, and are only allowed in few places, such as when adding data directories. For example, the 'base' data directory is mounted like this: plFileSystem::AddDataDirectory(\">sdk/Data/Base\"); This adds the folder \"Data/Base\" that is located inside the folder where the SDK (Plasma Engine) is stored.","title":"SDK Root Folder"},{"location":"build/sdk-root/#default-strategy-for-locating-sdk-root","text":"The default strategy by which the engine detects the SDK root folder, is to start at the location of the application binary, and walk the file structure up, until it finds a folder, which contains the sub-folders \"Data/Base\". So for instance, if the running application is located in C:/PlasmaEngine/Bin/MyGame.exe , the search will start in C:/PlasmaEngine/Bin , where no such folder is found. Then it will continue in C:/PlasmaEngine . That folder does have the sub-structure C:/PlasmaEngine/Data/Base , so the SDK root is determined to be C:/PlasmaEngine . This strategy works, as long as the application binary is located somewhere inside the Plasma Engine SDK.","title":"Default Strategy for Locating SDK Root"},{"location":"build/sdk-root/#redirecting-to-sdk-root","text":"If you use a different file structure, the default strategy won't work. This is commonly the case when integrating Plasma Engine as a Submodule . For example your file structure may look like this: C:/MyRepo C:/MyRepo/PlasmaEngine-module/ ... C:/MyRepo/Bin/MyGame.exe C:/MyRepo/OtherData/ ... Here Plasma Engine is integrated into another repository. The 'Bin' folder is top level, just as the 'Plasma Engine-module' folder. To enable such a pattern, you can place a 'redirection file', which points to the SDK root folder. The file has to be called plSdkRoot.txt and it must be located somewhere along the path that the default strategy searches. In this case it would be put into C:/MyRepo/plSdkRoot.txt and it would contain the string PlasmaEngine-module . This way, when the engine searches for the folder that contains 'Data/Base', it will reach C:/MyRepo , see the plSdkRoot.txt file, read its content, append the relative path inside to its current path ( C:/MyRepo/PlasmaEngine-module ) and find C:/MyRepo/PlasmaEngine-module/Data/Base , which means it determines C:/MyRepo/PlasmaEngine-module to be the SDK root folder. Using a redirection file is the least invasive method and it works for all Plasma applications, e.g. the editor, samples and tools.","title":"Redirecting to SDK Root"},{"location":"build/sdk-root/#custom-sdk-root","text":"You can fully control where the SDK root should be and how it is found, if you write your own application (TODO) . During early startup you can simply set the path of the SDK root folder with plFileSystem::SetSdkRootDirectory() . This can be preferable when you use a very different structure. Note that this method will only work for applications that you control. Tools such as plInspector or the editor expect to find the SDK root through the default search strategy (or through a redirection file).","title":"Custom SDK Root"},{"location":"build/sdk-root/#when-to-redirect-at-all","text":"The SDK root folder doesn't need to point to the folder where Plasma Engine is stored. This is only necessary, when you really need the data that is stored in Data/Base . If you only use a fraction of Plasma, for example only the Foundation library, or not the editor and rendering code, then you can also use a very different folder as your root (for example C:/MyRepo in the example above). In such cases your application would almost certainly specify its custom SDK root in its startup code directly.","title":"When to Redirect At All"},{"location":"build/sdk-root/#see-also","text":"Plasma Engine as a Submodule CMake Setup Building Plasma Engine","title":"See Also"},{"location":"build/submodule/","text":"Plasma Engine as a Submodule When using git and CMake for a project, Plasma Engine can be integrated as a submodule into the git repository and referenced from CMake. First PlasmaEngine needs to be added as a submodule to git: git submodule add https://github.com/PlasmaEngine/PlasmaEngine.git Additionally, if you want to use the precompiled tools and the sample content from Plasma, you also need to pull in its submodules as well: cd PlasmaEngine git submodule init git submodule update Next, add the Plasma Engine folder in your root CMakeLists.txt : # Set the build filter, if you only want to integrate parts of Plasma into your build. # set(PLASMA_BUILD_FILTER \"FoundationOnly\") add_subdirectory(PlasmaEngine) The Plasma Engine language detection can be reused by including the Plasma Engine submodule utility file: # include the Plasma submodule utility CMake functions include(\"PlasmaEngine/Code/BuildSystem/CMake/plUtilsSubmodule.cmake\") pl_detect_languages() project(\"MyProject\" LANGUAGES ${PLASMA_LANGUAGES}) Important: This kind of integration is useful, if you want to integrate Plasma code into your project, for instance, if you want to use plFoundation as your base library. Since the Plasma folder isn't top-level in this setup, using the full engine and all the data located in the data directories won't work out of the box. For additional options, see the CMake setup page. Strip Unnecessary Code When integrating Plasma this way, you may only want a subset of the available functionality. For instance, you may only need the plFoundation base library (and 3rd party dependencies). You can achieve this by configuring the build filter SDK Root Folder When integrating Plasma as a submodule, it is common for the binaries to be located outside of the Plasma Engine sub-folder, which means the engine won't be able to find the SDK root folder anymore. See this article for ways to fix this. See Also CMake Setup","title":"Plasma Engine as a Submodule"},{"location":"build/submodule/#plasma-engine-as-a-submodule","text":"When using git and CMake for a project, Plasma Engine can be integrated as a submodule into the git repository and referenced from CMake. First PlasmaEngine needs to be added as a submodule to git: git submodule add https://github.com/PlasmaEngine/PlasmaEngine.git Additionally, if you want to use the precompiled tools and the sample content from Plasma, you also need to pull in its submodules as well: cd PlasmaEngine git submodule init git submodule update Next, add the Plasma Engine folder in your root CMakeLists.txt : # Set the build filter, if you only want to integrate parts of Plasma into your build. # set(PLASMA_BUILD_FILTER \"FoundationOnly\") add_subdirectory(PlasmaEngine) The Plasma Engine language detection can be reused by including the Plasma Engine submodule utility file: # include the Plasma submodule utility CMake functions include(\"PlasmaEngine/Code/BuildSystem/CMake/plUtilsSubmodule.cmake\") pl_detect_languages() project(\"MyProject\" LANGUAGES ${PLASMA_LANGUAGES}) Important: This kind of integration is useful, if you want to integrate Plasma code into your project, for instance, if you want to use plFoundation as your base library. Since the Plasma folder isn't top-level in this setup, using the full engine and all the data located in the data directories won't work out of the box. For additional options, see the CMake setup page.","title":"Plasma Engine as a Submodule"},{"location":"build/submodule/#strip-unnecessary-code","text":"When integrating Plasma this way, you may only want a subset of the available functionality. For instance, you may only need the plFoundation base library (and 3rd party dependencies). You can achieve this by configuring the build filter","title":"Strip Unnecessary Code"},{"location":"build/submodule/#sdk-root-folder","text":"When integrating Plasma as a submodule, it is common for the binaries to be located outside of the Plasma Engine sub-folder, which means the engine won't be able to find the SDK root folder anymore. See this article for ways to fix this.","title":"SDK Root Folder"},{"location":"build/submodule/#see-also","text":"CMake Setup","title":"See Also"},{"location":"build/supported-platforms/","text":"Supported Platforms Plasma Engine is developed primarily on Windows 10/11, using Visual Studio 2022 in 64 Bit builds. As such platform has the largest feature set and is the best tested one. The code uses C++ 11, 14 and 17 features, but only where broad compiler support is available. The renderer currently uses DX11 on Windows and a Vulkan implementation is in progress. The editor is currently available on Windows, and being ported to Linux. On Mac, Android and Linux currently only the base libraries are fully functional. Once the Vulkan renderer is more mature, the goal is to have most features available everywhere. List of Supported Platforms Windows 10/11 desktop ( details ) Windows 10/11 UWP ( details ) OS X 10.9 (Mavericks) ( details ) Linux ( details ) Android 6.0 Marshmallow (API 23) or newer ( details ) See Also Building Plasma Engine","title":"Supported Platforms"},{"location":"build/supported-platforms/#supported-platforms","text":"Plasma Engine is developed primarily on Windows 10/11, using Visual Studio 2022 in 64 Bit builds. As such platform has the largest feature set and is the best tested one. The code uses C++ 11, 14 and 17 features, but only where broad compiler support is available. The renderer currently uses DX11 on Windows and a Vulkan implementation is in progress. The editor is currently available on Windows, and being ported to Linux. On Mac, Android and Linux currently only the base libraries are fully functional. Once the Vulkan renderer is more mature, the goal is to have most features available everywhere.","title":"Supported Platforms"},{"location":"build/supported-platforms/#list-of-supported-platforms","text":"Windows 10/11 desktop ( details ) Windows 10/11 UWP ( details ) OS X 10.9 (Mavericks) ( details ) Linux ( details ) Android 6.0 Marshmallow (API 23) or newer ( details )","title":"List of Supported Platforms"},{"location":"build/supported-platforms/#see-also","text":"Building Plasma Engine","title":"See Also"},{"location":"custom-code/custom-code-overview/","text":"Custom Code There are currently three ways to write custom code: Custom Code with C++ Custom Code with TypeScript Custom Code with Visual Scripts C++ Extending the engine with C++ is the most versatile and efficient. With C++ you have full access to the entire engine, giving you all the power you need. Any serious project will have to use C++ code for some parts. For things like game logic you can start with script code instead, and migrate critical parts to C++ on demand. Extending the renderer and access to third party integrations is mostly only possible from C++ code. Using the C++ Project Generation feature, it is very quick and easy to set up a custom C++ plugin. C++ obviously has the downsides of longer compilation times, and live updating code is not possible. There is a way for game plugins to be modifiable while the editor runs, and the editor can fully shut down and reload its engine process, which does enable a form of live reloading of C++ code . TypeScript The TypeScript integration allows you to write custom components. The integration provides access to the most important aspects that are needed for game code. C++ components can be accessed, as long as they expose their functionality through reflection . TypeScript is very useful for game logic and allows to quickly create complex prefabs . TypeScript is transpiled to JavaScript code, which is interpreted in a VM. Its performance is therefore far worse than C++. However, migrating a critical component from TypeScript to C++ at a later stage is possible. TypeScript code is updated every time you run a scene , which allows for quick iteration times. Visual Scripting Visual script code is the most limited both in functionality and tooling. It can be used for simple if-then scripts. The visual script system is not bad, but it is very far away from what can be done with TypeScript, and due to auto-completion and type checking provided by Visual Studio Code, using TypeScript is even more convenient. Without serious investment into the visual scripting system and tools (for which the core team has no time), there is no reason to ever prefer it over the other options.","title":"Custom Code"},{"location":"custom-code/custom-code-overview/#custom-code","text":"There are currently three ways to write custom code: Custom Code with C++ Custom Code with TypeScript Custom Code with Visual Scripts","title":"Custom Code"},{"location":"custom-code/custom-code-overview/#c","text":"Extending the engine with C++ is the most versatile and efficient. With C++ you have full access to the entire engine, giving you all the power you need. Any serious project will have to use C++ code for some parts. For things like game logic you can start with script code instead, and migrate critical parts to C++ on demand. Extending the renderer and access to third party integrations is mostly only possible from C++ code. Using the C++ Project Generation feature, it is very quick and easy to set up a custom C++ plugin. C++ obviously has the downsides of longer compilation times, and live updating code is not possible. There is a way for game plugins to be modifiable while the editor runs, and the editor can fully shut down and reload its engine process, which does enable a form of live reloading of C++ code .","title":"C++"},{"location":"custom-code/custom-code-overview/#typescript","text":"The TypeScript integration allows you to write custom components. The integration provides access to the most important aspects that are needed for game code. C++ components can be accessed, as long as they expose their functionality through reflection . TypeScript is very useful for game logic and allows to quickly create complex prefabs . TypeScript is transpiled to JavaScript code, which is interpreted in a VM. Its performance is therefore far worse than C++. However, migrating a critical component from TypeScript to C++ at a later stage is possible. TypeScript code is updated every time you run a scene , which allows for quick iteration times.","title":"TypeScript"},{"location":"custom-code/custom-code-overview/#visual-scripting","text":"Visual script code is the most limited both in functionality and tooling. It can be used for simple if-then scripts. The visual script system is not bad, but it is very far away from what can be done with TypeScript, and due to auto-completion and type checking provided by Visual Studio Code, using TypeScript is even more convenient. Without serious investment into the visual scripting system and tools (for which the core team has no time), there is no reason to ever prefer it over the other options.","title":"Visual Scripting"},{"location":"custom-code/cpp/cpp-code-reload/","text":"Hot Reloading C++ Game Plugins in the Editor When writing game code in C++, the most annoying aspect are the iteration times. Due to how C++ works, it is nearly impossible to swap out code and replace it with a newer version. Some engines try to do this, but there are always limitations, and the effort to get this working and keep it from breaking is quite big. Plasma Engine is no different here, reloading code at runtime is not possible. However, the editor is split into two processes: The actual editor process which displays the UI, and manages the scene state, and the engine process which does the scene rendering and executes the actual game code. This separation makes the editor more resilient to crashes. To recover from the crash you can click the viewport. When you click the button, the editor launches a new engine process, synchronizes the latest scene state over and continues as if nothing happened. That of course means, that when the new engine process launches, it also loads the latest state of all plugins. So if any of the plugins was modified, we would now see these modifications. So by simply nuking and restarting the entire engine process, we can get some form of C++ code hot reloading. Unfortunately, you can't compile a plugin while a process is using it, because the process prevents other applications from writing the DLL. We can solve this problem, by making a copy of our game plugin DLL, and loading that instead. That means that the original DLL is not actually used, and our IDE can modify it further. This trick works quite well if we only do it for plugins that are loaded fully dynamic, meaning that no other plugin tries to link against it. If that were the case, both the original plugin, as well as the copied plugin would get loaded, which is not good. Therefore only select game plugins may use this copy mechanic, which is why you need to set this up manually. How to Enable Plugin Hot Reloading In the plugin selection dialog, select your custom plugin and check Enable Reload . Note: If you used the C++ Project Generation feature to create your project, the reference to your C++ plugin is automatically set up this way. Now you can modify the code of your plugin and compile it, while the editor is open. Of course, you can't do that while being attached with a debugger. When a plugin is marked as Enable Reload , and it gets modified, the editor already automatically restarts the engine process , once no scene is being simulated. Terminate and Restart the Engine You can always manually restart the engine process through Tools > Reload Engine or the hotkey Ctrl+Shift+F4 . Restrictions The Enable Reload option should only be used for select game plugins. Enabling this feature can have unintended side-effects. If any code links against a plugin, that plugin cannot be loaded as a copy. Therefore, if you want to put shared code into a separate library that other users of your plugins link against, you can't load that shared library as a copy. You can't compile code while debugging a process. To compile your code, you first have to detach your debugger. In Visual Studio that can be done via Debug > Detach All . Consequently, if you want to continue debugging after you restarted the engine process, you need to manually re-attach your debugger to EditorEngineProcess.exe . In Visual Studio this is done via Debug > Attach to Process... or even better Debug > Reattach to Process ( SHIFT+ALT+P ) when you want to repeat the same thing a second time. See Also Debugging C++ Code Engine Plugins C++ Project Generation","title":"Hot Reloading C++ Game Plugins in the Editor"},{"location":"custom-code/cpp/cpp-code-reload/#hot-reloading-c-game-plugins-in-the-editor","text":"When writing game code in C++, the most annoying aspect are the iteration times. Due to how C++ works, it is nearly impossible to swap out code and replace it with a newer version. Some engines try to do this, but there are always limitations, and the effort to get this working and keep it from breaking is quite big. Plasma Engine is no different here, reloading code at runtime is not possible. However, the editor is split into two processes: The actual editor process which displays the UI, and manages the scene state, and the engine process which does the scene rendering and executes the actual game code. This separation makes the editor more resilient to crashes. To recover from the crash you can click the viewport. When you click the button, the editor launches a new engine process, synchronizes the latest scene state over and continues as if nothing happened. That of course means, that when the new engine process launches, it also loads the latest state of all plugins. So if any of the plugins was modified, we would now see these modifications. So by simply nuking and restarting the entire engine process, we can get some form of C++ code hot reloading. Unfortunately, you can't compile a plugin while a process is using it, because the process prevents other applications from writing the DLL. We can solve this problem, by making a copy of our game plugin DLL, and loading that instead. That means that the original DLL is not actually used, and our IDE can modify it further. This trick works quite well if we only do it for plugins that are loaded fully dynamic, meaning that no other plugin tries to link against it. If that were the case, both the original plugin, as well as the copied plugin would get loaded, which is not good. Therefore only select game plugins may use this copy mechanic, which is why you need to set this up manually.","title":"Hot Reloading C++ Game Plugins in the Editor"},{"location":"custom-code/cpp/cpp-code-reload/#how-to-enable-plugin-hot-reloading","text":"In the plugin selection dialog, select your custom plugin and check Enable Reload . Note: If you used the C++ Project Generation feature to create your project, the reference to your C++ plugin is automatically set up this way. Now you can modify the code of your plugin and compile it, while the editor is open. Of course, you can't do that while being attached with a debugger. When a plugin is marked as Enable Reload , and it gets modified, the editor already automatically restarts the engine process , once no scene is being simulated.","title":"How to Enable Plugin Hot Reloading"},{"location":"custom-code/cpp/cpp-code-reload/#terminate-and-restart-the-engine","text":"You can always manually restart the engine process through Tools > Reload Engine or the hotkey Ctrl+Shift+F4 .","title":"Terminate and Restart the Engine"},{"location":"custom-code/cpp/cpp-code-reload/#restrictions","text":"The Enable Reload option should only be used for select game plugins. Enabling this feature can have unintended side-effects. If any code links against a plugin, that plugin cannot be loaded as a copy. Therefore, if you want to put shared code into a separate library that other users of your plugins link against, you can't load that shared library as a copy. You can't compile code while debugging a process. To compile your code, you first have to detach your debugger. In Visual Studio that can be done via Debug > Detach All . Consequently, if you want to continue debugging after you restarted the engine process, you need to manually re-attach your debugger to EditorEngineProcess.exe . In Visual Studio this is done via Debug > Attach to Process... or even better Debug > Reattach to Process ( SHIFT+ALT+P ) when you want to repeat the same thing a second time.","title":"Restrictions"},{"location":"custom-code/cpp/cpp-code-reload/#see-also","text":"Debugging C++ Code Engine Plugins C++ Project Generation","title":"See Also"},{"location":"custom-code/cpp/cpp-overview/","text":"Custom Code with C++ To extend the engine with C++ code, you should put your code into an engine plugin . This enables both the editor as well as plPlayer to load and execute your code. Build Setup The first thing you need to set up is your build system, meaning you need to have a project for your plugin. The easiest way to do so, is to use the C++ Project Generation functionality in the editor. Another option is to add your own project into the Plasma Engine source tree and just use Plasma's CMake build setup and generated solution. A cleaner approach is to do basically the same, but to put your code outside the Plasma source tree, and add a reference to that folder. This will integrate your source into the Plasma Engine solution. Make sure to use the same CMakeLists.txt files as the engine plugins in Plasma do. With this option, you can have your code in a separate repository. Another way is to use your own CMake based build setup, and integrate Plasma Engine as a Submodule . All three solutions give you tight integration of your code and the Plasma Engine code, which makes debugging more convenient. You can, of course, also build Plasma Engine once and just link against its libraries. Custom Game Code For an example how to set up a game plugin, see the Sample Game Plugin . It is best to start with writing a custom component . If you need higher level game logic, have a look at game states . Also have a look at input . Once you add more complex systems, you may need the startup system to properly initialize them and shut them down. Debugging & Profiling The chapter Debugging C++ Code gives some useful tips. Things you should also be aware of are the console and CVars , as well as stats and debug rendering . Finally, have a look at profiling , to know where your code spends most of its time. Standalone App If you want to write your own stand-alone application, have a look at the application (TODO) chapter. However, we encourage you to have your entire game code in a plugin, otherwise you can't use any of it in the editor. You would mostly need your own application for the finishing touches of how you present the main menu, etc, and a good starting point is to just copy the plPlayer and make adjustments. See Also Custom Code Sample Game Plugin Engine Plugins The World / Scenegraph System","title":"Custom Code with C++"},{"location":"custom-code/cpp/cpp-overview/#custom-code-with-c","text":"To extend the engine with C++ code, you should put your code into an engine plugin . This enables both the editor as well as plPlayer to load and execute your code.","title":"Custom Code with C++"},{"location":"custom-code/cpp/cpp-overview/#build-setup","text":"The first thing you need to set up is your build system, meaning you need to have a project for your plugin. The easiest way to do so, is to use the C++ Project Generation functionality in the editor. Another option is to add your own project into the Plasma Engine source tree and just use Plasma's CMake build setup and generated solution. A cleaner approach is to do basically the same, but to put your code outside the Plasma source tree, and add a reference to that folder. This will integrate your source into the Plasma Engine solution. Make sure to use the same CMakeLists.txt files as the engine plugins in Plasma do. With this option, you can have your code in a separate repository. Another way is to use your own CMake based build setup, and integrate Plasma Engine as a Submodule . All three solutions give you tight integration of your code and the Plasma Engine code, which makes debugging more convenient. You can, of course, also build Plasma Engine once and just link against its libraries.","title":"Build Setup"},{"location":"custom-code/cpp/cpp-overview/#custom-game-code","text":"For an example how to set up a game plugin, see the Sample Game Plugin . It is best to start with writing a custom component . If you need higher level game logic, have a look at game states . Also have a look at input . Once you add more complex systems, you may need the startup system to properly initialize them and shut them down.","title":"Custom Game Code"},{"location":"custom-code/cpp/cpp-overview/#debugging-profiling","text":"The chapter Debugging C++ Code gives some useful tips. Things you should also be aware of are the console and CVars , as well as stats and debug rendering . Finally, have a look at profiling , to know where your code spends most of its time.","title":"Debugging &amp; Profiling"},{"location":"custom-code/cpp/cpp-overview/#standalone-app","text":"If you want to write your own stand-alone application, have a look at the application (TODO) chapter. However, we encourage you to have your entire game code in a plugin, otherwise you can't use any of it in the editor. You would mostly need your own application for the finishing touches of how you present the main menu, etc, and a good starting point is to just copy the plPlayer and make adjustments.","title":"Standalone App"},{"location":"custom-code/cpp/cpp-overview/#see-also","text":"Custom Code Sample Game Plugin Engine Plugins The World / Scenegraph System","title":"See Also"},{"location":"custom-code/cpp/cpp-project-generation/","text":"C++ Project Generation There are multiple ways to get custom C++ code into Plasma. The best way is to create an engine plugin , because this way the code can be executed directly inside the editor. Additionally such a plugins can be hot reloaded to some degree. Plasma uses CMake for its build infrastructure. See this chapter about different ways to integrate your own project into the Plasma build. The most convenient way, however, is to let the editor create a stand-alone project for you. This way you get a CMake based project that only contains your code, but has all the necessary references set up to link against Plasma and output its DLLs to the right folder. Note: One downside with this approach is that your solution does not contain the Plasma engine sources. That makes it less convenient to search for existing functionality, look up code API docs and stepping through Plasma code while debugging. However, if you want that, you can include the generated plugin as an external project into the Plasma engine solution. How to Generate a New Solution Select Editor > Open C++ Project... . Choose the desired MSVC solution version. Press the Generate Solution button and wait for it to finish. The CMake output is output in the Output window. In case of errors, please have a look here. If everything went fine, you can open the project and compile the code. Attention: The code has to be built for the very same build type that the editor is running in ( Debug , Dev or Shipping ), otherwise the editor won't be able to load the DLL. If the build fails because certain Plasma DLLs are missing, you are most likely building the wrong build type. The same may be true if you do code changes, but running the game from the editor doesn't reflect those changes. Note: After compiling your new plugin for the very first time, you have to close the editor project and reopen it, for it to load the information properly. Due to automatic hot reloading this shouldn't be necessary later anymore, but it can still be necessary, for example when you add a new component type to your C++ code. Opening an Existing Solution If you have generated the solution before, the Open Solution button will be active right away when you open this dialog. In this case you don't need to generate the solution again. Regenerating a Solution If you have added or removed source files on disk, you need to regenerate the solution for those changes to show up in the Visual Studio solution. You have two options to do so: Run CMake yourself. For example you can use the CMake GUI app, point it to the plugin's build directory, and then Configure and Generate the solution at any point you like. Rerun Generate Solution from the dialog above. This will clear the CMake cache and fully regenerate the solution. Be aware that this resets all CMake options to their default values and often takes longer than strictly necessary. See Also Custom Code with C++ Hot Reloading C++ Game Plugins in the Editor Engine Plugins Sample Game Plugin","title":"C++ Project Generation"},{"location":"custom-code/cpp/cpp-project-generation/#c-project-generation","text":"There are multiple ways to get custom C++ code into Plasma. The best way is to create an engine plugin , because this way the code can be executed directly inside the editor. Additionally such a plugins can be hot reloaded to some degree. Plasma uses CMake for its build infrastructure. See this chapter about different ways to integrate your own project into the Plasma build. The most convenient way, however, is to let the editor create a stand-alone project for you. This way you get a CMake based project that only contains your code, but has all the necessary references set up to link against Plasma and output its DLLs to the right folder. Note: One downside with this approach is that your solution does not contain the Plasma engine sources. That makes it less convenient to search for existing functionality, look up code API docs and stepping through Plasma code while debugging. However, if you want that, you can include the generated plugin as an external project into the Plasma engine solution.","title":"C++ Project Generation"},{"location":"custom-code/cpp/cpp-project-generation/#how-to-generate-a-new-solution","text":"Select Editor > Open C++ Project... . Choose the desired MSVC solution version. Press the Generate Solution button and wait for it to finish. The CMake output is output in the Output window. In case of errors, please have a look here. If everything went fine, you can open the project and compile the code. Attention: The code has to be built for the very same build type that the editor is running in ( Debug , Dev or Shipping ), otherwise the editor won't be able to load the DLL. If the build fails because certain Plasma DLLs are missing, you are most likely building the wrong build type. The same may be true if you do code changes, but running the game from the editor doesn't reflect those changes. Note: After compiling your new plugin for the very first time, you have to close the editor project and reopen it, for it to load the information properly. Due to automatic hot reloading this shouldn't be necessary later anymore, but it can still be necessary, for example when you add a new component type to your C++ code.","title":"How to Generate a New Solution"},{"location":"custom-code/cpp/cpp-project-generation/#opening-an-existing-solution","text":"If you have generated the solution before, the Open Solution button will be active right away when you open this dialog. In this case you don't need to generate the solution again.","title":"Opening an Existing Solution"},{"location":"custom-code/cpp/cpp-project-generation/#regenerating-a-solution","text":"If you have added or removed source files on disk, you need to regenerate the solution for those changes to show up in the Visual Studio solution. You have two options to do so: Run CMake yourself. For example you can use the CMake GUI app, point it to the plugin's build directory, and then Configure and Generate the solution at any point you like. Rerun Generate Solution from the dialog above. This will clear the CMake cache and fully regenerate the solution. Be aware that this resets all CMake options to their default values and often takes longer than strictly necessary.","title":"Regenerating a Solution"},{"location":"custom-code/cpp/cpp-project-generation/#see-also","text":"Custom Code with C++ Hot Reloading C++ Game Plugins in the Editor Engine Plugins Sample Game Plugin","title":"See Also"},{"location":"custom-code/cpp/custom-cpp-component/","text":"Custom Components with C++ To write a custom C++ component, the first thing you need is a custom engine plugin . Once you have that, and have it enabled in your project settings , any custom component that you define in that plugin will show up in the editor and can be attached to game objects . The Sample Game Plugin shows all the pieces that you need, including multiple components to get inspiration from. This article describes the steps to create a simple custom component. Before you continue, please read the components chapter , as it already covers most things that you need to know. Component Manager Declaration For every type of C++ component there is a corresponding component manager . The component manager is responsible for allocating and deallocating components and for updating them. Each component manager is tied to a single world , so if you have multiple worlds, each world will hold its own instance of each component manager. A component manager is a world module , so it can register functions to be called during specific update phases of the world. For the vast majority of components we only need a component manager that calls Update() on our component type once a frame. We can declare such a simple manager like this in the header file for our component: using DemoComponentManager = plComponentManagerSimple<class DemoComponent, plComponentUpdateType::WhenSimulating>; Component Class Declaration Next, we declare our component class. All components must derive (at least indirectly) from plComponent . Also vital is to insert the PLASMA_DECLARE_COMPONENT_TYPE macro, where you pass in the own component class name, the base class, and the component manager class. class DemoComponent : public plComponent { PLASMA_DECLARE_COMPONENT_TYPE(DemoComponent, plComponent, DemoComponentManager); ////////////////////////////////////////////////////////////////////////// // plComponent public: virtual void SerializeComponent(plWorldWriter& stream) const override; virtual void DeserializeComponent(plWorldReader& stream) override; protected: virtual void OnSimulationStarted() override; ////////////////////////////////////////////////////////////////////////// // DemoComponent public: DemoComponent(); ~DemoComponent(); private: void Update(); float m_fAmplitude = 1.0f; // [ property ] plAngle m_Speed = plAngle::Degree(90); // [ property ] }; Here we override a couple of functions from plComponent . For the binary serialization we must implement plComponent::SerializeComponent() . As long as you test your component only inside the editor, you don't yet need to implement these functions, as the editor stores reflected properties automatically. However, once you want to export your scene, these functions are used, and if you forgot to properly serialize something, the exported scene will not work correctly. Note that our sample component has a (non-virtual) function called Update() . This is necessary because we use the plComponentManagerSimple here, which expects to find such a function. If you write your own component manager, you can do this differently. Reflection Block In our cpp file we need to insert a reflection block for our component type. This tells the engine all the details about our component, for instance which properties it has. // clang-format off PLASMA_BEGIN_COMPONENT_TYPE(DemoComponent, 3 /* version */, plComponentMode::Dynamic) { PLASMA_BEGIN_PROPERTIES { PLASMA_MEMBER_PROPERTY(\"Amplitude\", m_fAmplitude)->AddAttributes(new plDefaultValueAttribute(1), new plClampValueAttribute(0, 10)), PLASMA_MEMBER_PROPERTY(\"Speed\", m_Speed)->AddAttributes(new plDefaultValueAttribute(plAngle::Degree(90))), } PLASMA_END_PROPERTIES; PLASMA_BEGIN_ATTRIBUTES { new plCategoryAttribute(\"SampleGamePlugin\"), } PLASMA_END_ATTRIBUTES; } PLASMA_END_COMPONENT_TYPE // clang-format on This information is used in various ways. The editor uses it for the UI. Attributes on each property allow you to configure what default values the editor should use, and whether it should clamp the range for values, etc. Bindings to other languages also use this information to generate the necessary code. Everything that is not mentioned in this block, is internal to the C++ code and hidden from the tools. Initialization and Update Next up, we implement our basic component code: DemoComponent::DemoComponent() = default; DemoComponent::~DemoComponent() = default; void DemoComponent::OnSimulationStarted() { SUPER::OnSimulationStarted(); // this component doesn't need to anything for initialization } void DemoComponent::Update() { const plTime curTime = GetWorld()->GetClock().GetAccumulatedTime(); const plAngle curAngle = curTime.AsFloatInSeconds() * m_Speed; const float curHeight = plMath::Sin(curAngle) * m_fAmplitude; GetOwner()->SetLocalPosition(plVec3(0, 0, curHeight)); } Components rarely need to do much in their constructor and destructor. Most setup should be done in plComponent::OnSimulationStarted() . For components that should already have functionality in the editor, while the simulation is not yet running, you should do your setup in plComponent::OnActivated() instead. There is no OnSimulationStopped() , as this would always be the same as plComponent::OnDeactivated() . As you can see, this component modifies the position of its owner object during its update. This is why we had to use plComponentMode::Dynamic in the reflection block, to tell the engine that objects with this component attached may change their position. Serialization Finally, to make our component also work in exported scenes, we need to implement serialization: void DemoComponent::SerializeComponent(plWorldWriter& stream) const { SUPER::SerializeComponent(stream); auto& s = stream.GetStream(); s << m_fAmplitude; s << m_Speed; } This writes out the data in the latest format. If you change the format, you should increase the version number of your component in the reflection block at the very top. Obviously, at runtime we also need to deserialize our component. This is where we implement backwards compatibility for older exported scenes: void DemoComponent::DeserializeComponent(plWorldReader& stream) { SUPER::DeserializeComponent(stream); const plUInt32 uiVersion = stream.GetComponentTypeVersion(GetStaticRTTI()); auto& s = stream.GetStream(); s >> m_fAmplitude; if (uiVersion <= 2) { // up to version 2 the angle was stored as a float in degree // convert this to plAngle float fDegree; s >> fDegree; m_Speed = plAngle::Degree(fDegree); } else { s >> m_Speed; } } Conclusion Adding a custom component in C++ is not hard. Use the Sample Game Plugin as a playground to get started. Of course with C++ you have the typical restriction that you can't hot reload code, you have to close the editor, compile your plugin and reopen the editor. Hot Reloading C++ Game Plugins in the Editor describes a mechanism that can basically do all that for you with a single button press, though. Armed with these basics, you should have a look at existing components to see how to solve specific issues. See Also Components Custom Code Sample Game Plugin Hot Reloading C++ Game Plugins in the Editor","title":"Custom Components with C++"},{"location":"custom-code/cpp/custom-cpp-component/#custom-components-with-c","text":"To write a custom C++ component, the first thing you need is a custom engine plugin . Once you have that, and have it enabled in your project settings , any custom component that you define in that plugin will show up in the editor and can be attached to game objects . The Sample Game Plugin shows all the pieces that you need, including multiple components to get inspiration from. This article describes the steps to create a simple custom component. Before you continue, please read the components chapter , as it already covers most things that you need to know.","title":"Custom Components with C++"},{"location":"custom-code/cpp/custom-cpp-component/#component-manager-declaration","text":"For every type of C++ component there is a corresponding component manager . The component manager is responsible for allocating and deallocating components and for updating them. Each component manager is tied to a single world , so if you have multiple worlds, each world will hold its own instance of each component manager. A component manager is a world module , so it can register functions to be called during specific update phases of the world. For the vast majority of components we only need a component manager that calls Update() on our component type once a frame. We can declare such a simple manager like this in the header file for our component: using DemoComponentManager = plComponentManagerSimple<class DemoComponent, plComponentUpdateType::WhenSimulating>;","title":"Component Manager Declaration"},{"location":"custom-code/cpp/custom-cpp-component/#component-class-declaration","text":"Next, we declare our component class. All components must derive (at least indirectly) from plComponent . Also vital is to insert the PLASMA_DECLARE_COMPONENT_TYPE macro, where you pass in the own component class name, the base class, and the component manager class. class DemoComponent : public plComponent { PLASMA_DECLARE_COMPONENT_TYPE(DemoComponent, plComponent, DemoComponentManager); ////////////////////////////////////////////////////////////////////////// // plComponent public: virtual void SerializeComponent(plWorldWriter& stream) const override; virtual void DeserializeComponent(plWorldReader& stream) override; protected: virtual void OnSimulationStarted() override; ////////////////////////////////////////////////////////////////////////// // DemoComponent public: DemoComponent(); ~DemoComponent(); private: void Update(); float m_fAmplitude = 1.0f; // [ property ] plAngle m_Speed = plAngle::Degree(90); // [ property ] }; Here we override a couple of functions from plComponent . For the binary serialization we must implement plComponent::SerializeComponent() . As long as you test your component only inside the editor, you don't yet need to implement these functions, as the editor stores reflected properties automatically. However, once you want to export your scene, these functions are used, and if you forgot to properly serialize something, the exported scene will not work correctly. Note that our sample component has a (non-virtual) function called Update() . This is necessary because we use the plComponentManagerSimple here, which expects to find such a function. If you write your own component manager, you can do this differently.","title":"Component Class Declaration"},{"location":"custom-code/cpp/custom-cpp-component/#reflection-block","text":"In our cpp file we need to insert a reflection block for our component type. This tells the engine all the details about our component, for instance which properties it has. // clang-format off PLASMA_BEGIN_COMPONENT_TYPE(DemoComponent, 3 /* version */, plComponentMode::Dynamic) { PLASMA_BEGIN_PROPERTIES { PLASMA_MEMBER_PROPERTY(\"Amplitude\", m_fAmplitude)->AddAttributes(new plDefaultValueAttribute(1), new plClampValueAttribute(0, 10)), PLASMA_MEMBER_PROPERTY(\"Speed\", m_Speed)->AddAttributes(new plDefaultValueAttribute(plAngle::Degree(90))), } PLASMA_END_PROPERTIES; PLASMA_BEGIN_ATTRIBUTES { new plCategoryAttribute(\"SampleGamePlugin\"), } PLASMA_END_ATTRIBUTES; } PLASMA_END_COMPONENT_TYPE // clang-format on This information is used in various ways. The editor uses it for the UI. Attributes on each property allow you to configure what default values the editor should use, and whether it should clamp the range for values, etc. Bindings to other languages also use this information to generate the necessary code. Everything that is not mentioned in this block, is internal to the C++ code and hidden from the tools.","title":"Reflection Block"},{"location":"custom-code/cpp/custom-cpp-component/#initialization-and-update","text":"Next up, we implement our basic component code: DemoComponent::DemoComponent() = default; DemoComponent::~DemoComponent() = default; void DemoComponent::OnSimulationStarted() { SUPER::OnSimulationStarted(); // this component doesn't need to anything for initialization } void DemoComponent::Update() { const plTime curTime = GetWorld()->GetClock().GetAccumulatedTime(); const plAngle curAngle = curTime.AsFloatInSeconds() * m_Speed; const float curHeight = plMath::Sin(curAngle) * m_fAmplitude; GetOwner()->SetLocalPosition(plVec3(0, 0, curHeight)); } Components rarely need to do much in their constructor and destructor. Most setup should be done in plComponent::OnSimulationStarted() . For components that should already have functionality in the editor, while the simulation is not yet running, you should do your setup in plComponent::OnActivated() instead. There is no OnSimulationStopped() , as this would always be the same as plComponent::OnDeactivated() . As you can see, this component modifies the position of its owner object during its update. This is why we had to use plComponentMode::Dynamic in the reflection block, to tell the engine that objects with this component attached may change their position.","title":"Initialization and Update"},{"location":"custom-code/cpp/custom-cpp-component/#serialization","text":"Finally, to make our component also work in exported scenes, we need to implement serialization: void DemoComponent::SerializeComponent(plWorldWriter& stream) const { SUPER::SerializeComponent(stream); auto& s = stream.GetStream(); s << m_fAmplitude; s << m_Speed; } This writes out the data in the latest format. If you change the format, you should increase the version number of your component in the reflection block at the very top. Obviously, at runtime we also need to deserialize our component. This is where we implement backwards compatibility for older exported scenes: void DemoComponent::DeserializeComponent(plWorldReader& stream) { SUPER::DeserializeComponent(stream); const plUInt32 uiVersion = stream.GetComponentTypeVersion(GetStaticRTTI()); auto& s = stream.GetStream(); s >> m_fAmplitude; if (uiVersion <= 2) { // up to version 2 the angle was stored as a float in degree // convert this to plAngle float fDegree; s >> fDegree; m_Speed = plAngle::Degree(fDegree); } else { s >> m_Speed; } }","title":"Serialization"},{"location":"custom-code/cpp/custom-cpp-component/#conclusion","text":"Adding a custom component in C++ is not hard. Use the Sample Game Plugin as a playground to get started. Of course with C++ you have the typical restriction that you can't hot reload code, you have to close the editor, compile your plugin and reopen the editor. Hot Reloading C++ Game Plugins in the Editor describes a mechanism that can basically do all that for you with a single button press, though. Armed with these basics, you should have a look at existing components to see how to solve specific issues.","title":"Conclusion"},{"location":"custom-code/cpp/custom-cpp-component/#see-also","text":"Components Custom Code Sample Game Plugin Hot Reloading C++ Game Plugins in the Editor","title":"See Also"},{"location":"custom-code/cpp/engine-plugins/","text":"Engine Plugins Engine plugins are the best way to get your custom code into the engine, such that it is accessible by the editor and also plPlayer . Contrary to using a plugin, you could also build your own application (TODO) , which may link to static libraries that contain your code. However, that approach means that your code cannot be loaded into the editor process and therefore you won't be able to leverage those tools to their full extent. We strongly advise against that. Creating a Plugin The easiest way to create a custom plugin is to use the C++ Project Generation . This will create a template project for you, set up the CMake files for your build and configure the plugin selection . Creating a Plugin Manually In case you want more control, you can also set up your game plugin manually. The Sample Game Plugin is a good reference for how to do that. To just create a plugin, at all, you only need very little setup. The only files you need to look at are: CMakeLists.txt SampleGamePluginDLL.h SampleGamePlugin.cpp Build System Setup The file CMakeLists.txt is only of interest here in case you want to reuse the Plasma build infrastructure to generate your library. If you use Plasma Engine as a Submodule then you probably have your own CMake scripts. Either way, you need to add a project that generates a DLL. DLL Symbol Import/Export The file SampleGamePluginDLL.h only contains #define s for DLL import/export macros. // Configure the DLL Import/Export Define #if PLASMA_ENABLED(PLASMA_COMPILE_ENGINE_AS_DLL) # ifdef BUILDSYSTEM_BUILDING_SAMPLEGAMEPLUGIN_LIB # define PLASMA_SAMPLEGAMEPLUGIN_DLL __declspec(dllexport) # else # define PLASMA_SAMPLEGAMEPLUGIN_DLL __declspec(dllimport) # endif #else # define PLASMA_SAMPLEGAMEPLUGIN_DLL #endif If your plugin will be entirely on its own, you don't even need this. However, if you want to use multiple plugins and some of them should contain shared code, then others need to be able to link against the shared libraries and access classes and functions from that library. By tagging classes with these macros you can export symbols from a DLL and thus make those things available to other code. For examples how to use this, just search the sample plugin. Plugin Callbacks Plasma provides additional hooks for initialization when a plugin gets loaded or unloaded. You can find these in SampleGamePlugin.cpp : PLASMA_PLUGIN_ON_LOADED() { // you could do something here, though this is rare } PLASMA_PLUGIN_ON_UNLOADED() { // you could do something here, though this is rare } These callbacks are optional, though in some cases you may want to register and unregister things here. However, it is way more common to rather use the startup system instead. Loading a Plugin If you want to load a plugin from code, you would use plPlugin::LoadPlugin() and provide only the name (no path) of your plugin. Make sure that the DLL is stored in the same directory as all other DLLs and EXEs. The more convenient way to load your game plugin, though, is to enable it in the project settings . Then it will be automatically loaded by every application (TODO) . Add Custom Code Once you have your basic plugin set up and can load it into your project, you can start adding custom code. The easiest way to get started is to write a custom component . Once you need control over higher level game logic, you can add your own game state . And if you need to initialize and shut down certain systems, you should utilize the startup system . Utility Plugins If you want to write a plugin that provides some functionality for shared access, like some integration of a third-party library, the process is exactly the same. The only difference is, that such libraries would never contain a game state . Also, have a look at singletons if your plugin is supposed to provide an implementation of some abstract interface. See Also Sample Game Plugin Game States Custom Components with C++ Startup System Singleton Interfaces","title":"Engine Plugins"},{"location":"custom-code/cpp/engine-plugins/#engine-plugins","text":"Engine plugins are the best way to get your custom code into the engine, such that it is accessible by the editor and also plPlayer . Contrary to using a plugin, you could also build your own application (TODO) , which may link to static libraries that contain your code. However, that approach means that your code cannot be loaded into the editor process and therefore you won't be able to leverage those tools to their full extent. We strongly advise against that.","title":"Engine Plugins"},{"location":"custom-code/cpp/engine-plugins/#creating-a-plugin","text":"The easiest way to create a custom plugin is to use the C++ Project Generation . This will create a template project for you, set up the CMake files for your build and configure the plugin selection .","title":"Creating a Plugin"},{"location":"custom-code/cpp/engine-plugins/#creating-a-plugin-manually","text":"In case you want more control, you can also set up your game plugin manually. The Sample Game Plugin is a good reference for how to do that. To just create a plugin, at all, you only need very little setup. The only files you need to look at are: CMakeLists.txt SampleGamePluginDLL.h SampleGamePlugin.cpp","title":"Creating a Plugin Manually"},{"location":"custom-code/cpp/engine-plugins/#build-system-setup","text":"The file CMakeLists.txt is only of interest here in case you want to reuse the Plasma build infrastructure to generate your library. If you use Plasma Engine as a Submodule then you probably have your own CMake scripts. Either way, you need to add a project that generates a DLL.","title":"Build System Setup"},{"location":"custom-code/cpp/engine-plugins/#dll-symbol-importexport","text":"The file SampleGamePluginDLL.h only contains #define s for DLL import/export macros. // Configure the DLL Import/Export Define #if PLASMA_ENABLED(PLASMA_COMPILE_ENGINE_AS_DLL) # ifdef BUILDSYSTEM_BUILDING_SAMPLEGAMEPLUGIN_LIB # define PLASMA_SAMPLEGAMEPLUGIN_DLL __declspec(dllexport) # else # define PLASMA_SAMPLEGAMEPLUGIN_DLL __declspec(dllimport) # endif #else # define PLASMA_SAMPLEGAMEPLUGIN_DLL #endif If your plugin will be entirely on its own, you don't even need this. However, if you want to use multiple plugins and some of them should contain shared code, then others need to be able to link against the shared libraries and access classes and functions from that library. By tagging classes with these macros you can export symbols from a DLL and thus make those things available to other code. For examples how to use this, just search the sample plugin.","title":"DLL Symbol Import/Export"},{"location":"custom-code/cpp/engine-plugins/#plugin-callbacks","text":"Plasma provides additional hooks for initialization when a plugin gets loaded or unloaded. You can find these in SampleGamePlugin.cpp : PLASMA_PLUGIN_ON_LOADED() { // you could do something here, though this is rare } PLASMA_PLUGIN_ON_UNLOADED() { // you could do something here, though this is rare } These callbacks are optional, though in some cases you may want to register and unregister things here. However, it is way more common to rather use the startup system instead.","title":"Plugin Callbacks"},{"location":"custom-code/cpp/engine-plugins/#loading-a-plugin","text":"If you want to load a plugin from code, you would use plPlugin::LoadPlugin() and provide only the name (no path) of your plugin. Make sure that the DLL is stored in the same directory as all other DLLs and EXEs. The more convenient way to load your game plugin, though, is to enable it in the project settings . Then it will be automatically loaded by every application (TODO) .","title":"Loading a Plugin"},{"location":"custom-code/cpp/engine-plugins/#add-custom-code","text":"Once you have your basic plugin set up and can load it into your project, you can start adding custom code. The easiest way to get started is to write a custom component . Once you need control over higher level game logic, you can add your own game state . And if you need to initialize and shut down certain systems, you should utilize the startup system .","title":"Add Custom Code"},{"location":"custom-code/cpp/engine-plugins/#utility-plugins","text":"If you want to write a plugin that provides some functionality for shared access, like some integration of a third-party library, the process is exactly the same. The only difference is, that such libraries would never contain a game state . Also, have a look at singletons if your plugin is supposed to provide an implementation of some abstract interface.","title":"Utility Plugins"},{"location":"custom-code/cpp/engine-plugins/#see-also","text":"Sample Game Plugin Game States Custom Components with C++ Startup System Singleton Interfaces","title":"See Also"},{"location":"custom-code/game_logic/forward-events-to-game-state-component/","text":"Forward Events To Game State Component This component forwards any message that it receives to the active game state . Game states can have message handlers just like any other reflected type. However, since they are not part of the world , messages are not delivered to them. By attaching this component to a game object, all event messages that arrive here are forwarded to the active game state. This way, a game state can receive information, such as when a trigger gets activated. Multiple of these components can exist in a scene, gathering and forwarding messages from many different game objects, so that the game state can react to all of them. See Also Game States Messaging","title":"Forward Events To Game State Component"},{"location":"custom-code/game_logic/forward-events-to-game-state-component/#forward-events-to-game-state-component","text":"This component forwards any message that it receives to the active game state . Game states can have message handlers just like any other reflected type. However, since they are not part of the world , messages are not delivered to them. By attaching this component to a game object, all event messages that arrive here are forwarded to the active game state. This way, a game state can receive information, such as when a trigger gets activated. Multiple of these components can exist in a scene, gathering and forwarding messages from many different game objects, so that the game state can react to all of them.","title":"Forward Events To Game State Component"},{"location":"custom-code/game_logic/forward-events-to-game-state-component/#see-also","text":"Game States Messaging","title":"See Also"},{"location":"custom-code/game_logic/state-machine-asset/","text":"State Machine Asset A state machine asset is used to create a state machine that is commonly used to keep track of an entity's state. State machines are very useful to give objects functionality. For example a door might have the states closed , opening , open and closing and in each state it has to do certain things (apply a rotation) and allow or disallow interaction. State machines are also often used for AI (creatures or NPCs), to give them behavior. Since many AI types are rather simple, this is a viable option. For more complex AI logic, state machines quickly become too limited. State Machine Concept At any time exactly one state is active in a state machine. The active state determines what actions an entity will do and what other states it can transition into. For example in the image above you can see that this state machine can only transition from the Idle state into the Alert state (and back), but it can't directly transition into the Attacking state. What an entity does, when a certain state is active, is usually up to other code . On the state node in the state machine asset you select what type of state this is. Different state types can be implemented in C++ or visual scripts . State machines are updated regularly and during every update they may transition into another state. Possible transitions are represented by arrows between states. Just as with states, there are also different types of transitions . As with states, custom transition types can be implemented with C++ or visual scripts . During the state machine update, each transition on the active state is queried, whether its condition is met. If so, the transition is taken, and the state that it points to becomes the new active state. What it means that a transition's condition is met, is up to the transition type's implementation. For example the blackboard transition inspects values from a nearby blackboard , allowing you to set up logical rules. Another type of transition may simply wait for a second and then allow to transition further, acting as a timer. In many state machines there are states that can be reached from pretty much every other state. For example the Dead state in the image above is simply reached whenever the health of a creature reaches zero, no matter which state it currently is in. In a pure state machine, one would add transitions from every node to that final state. However, since this is cumbersome, Plasma also allows to set up transitions that can transition from any active state to their target state. In the state machine asset, such transitions are represented as an Any State (taking the place of any other state ) from which they are drawn to the target state. Configuring State Machines Right click into the window and choose New State or New Any State to add a node. To add a transition between states, click the + on a node and drag it to another state. Select state nodes in the main window to see their properties . Here you can give the state a name and select the state type . Most state types have custom properties that also need to be configured here. Select transition arrows to see their properties . Here you can select the transition type . Typically you also then need to configure the transition's properties. If you don't select a state type, a state simply does nothing. If you have a transition without a transition type, the transition will be taken immediately. This can be used to simply chain states. For example the start state is always active, even while the editor is not simulating. It is sometimes undesireable to have this state do anything. Therefore, a second state can be used as the \"real\" start state, and a transition without a type can be added between the two. This transition is only going to be taken once the scene is being simulated. Default Initial State Right click on a state and select Set as Initial State to make a state the default initial state . That means, when this state machine is used in a state machine component or as a nested state machine , and no initial state is selected by the user, this state is used. State Types You have to select a state type for every node in your state machine. Send Message State When the Send Message State gets activated (transitioned into) or deactivated (transitioned out of), it sends the plMsgStateMachineStateChanged event message to the game object on which the executing state machine component is attached. Thus other components, such as TypeScript components can listen for this message and react accordingly. The message can be sent with a delay. Also, if this state type is configured to send neither a message on enter, nor on exit, it effectively does nothing and can be used for states that don't require further action. You can also turn on logging on enter or exit, for better debugging. Nested State Machine A Nested State Machine state references another state machine. When the state is entered, it starts executing an instance of that state machine. Messages sent from the nested state machine will be delivered to the same owner game object. As long as the surrounding state stays active, the nested state machine gets updated. Once the surrounding state is exited, execution of the nested state machine is suspended. You can choose whether it gets reset to the initial state, or stay in the last active state. This way, once the nested state machine gets activated again later, it may either start from the beginning, or resume where it left off. Nested state machines can be very useful to reuse state machines and to make editing easier. Also the fact that a state machine can be suspended and resumed at its last active state allows for more complex behavior. If no initial state is specified, the default initial state is used. Compound State A Compound State has no functionality by itself, rather it holds an array of other state types. All events (state entered/left) are forwarded to all sub-states equally. Thus it can be used to trigger multiple reactions at the same time. Script State The Script State runs a visual scripts . It executes the OnEnter , OnExit and Update event handlers for state machine states. Switch Object State This state sets the enabled flag on a game object and disables all other objects in the same group. This state allows to easily switch the representation of a game object. For instance you may have two objects states: normal and burning . You can basically just build two objects, one in the normal state, and one with all the effects needed for the fire. Then you group both objects under a shared parent (e.g. with name 'visuals'), give both of them a name ('normal', 'burning') and disable one of them. When the state machine transitions from the normal state to the burning state, you can then use this type of state to say that from the 'visuals' group you want to activate the 'burning' object and deactivate all other objects in the same group. Because the state activates one object and deactivates all others, you can have many different visuals and switch between them. You can also only activate an object and keep the rest in the group as they are (e.g. to enable more and more effects). If you only give a group path, but no object name, you can also use it to just disable all objects in a group. If multiple objects in the same group have the same name, they will all get activated simultaneously. Make sure that essential other objects (like the physics representation or other scripts) are located on other objects, that don't get deactivated. State Properties PathToGroup : A list of object names, separated with slashes, that forms a relative path starting at the game object on which the state machine component is located, to a game object which acts as a group and contains multiple sub-objects. ObjectToEnable : The name of the sub-object in the group that should get enabled . DeactivateOthers : If true, all other objects within the group get disabled. Otherwise they stay as they are. Transition Types You have to select a transition type for every transition in your state machine. Blackboard Conditions Blackboard conditions query the blackboard . You can configure the transition condition to be fulfilled either when at least one or all the specified blackboard entries have the desired values. Timeout Transition This transition is fulfilled after a certain amount of time has passed. This can be used to automatically transition to the next state after a fixed amount of time. Together with a compound transition it can also be used to prevent transitioning before a minimum of time has passed. Compound Transition The Compound Transition allows you to create complex logical conditions. It holds an array of other transition types. You can select whether either at least one of them, or all of these conditions need to be met, for the compound transition to be fulfilled. For example, you can add a blackboard condition and a timeout transition. If you set the compound transition to AND , both of these must be true, meaning the transition can only be taken after the blackboard values are correct and the minimum time has passed. If, however, you set it to OR , the transition is taken once either the timeout or the blackboard state is fulfilled. Note that you can use nested compound transitions to create even more complex logical conditions. Transition Event Transition This type of transition is taken when a transition event with the expected name has been raised. Transition events can be raised from visual scripts or C++ through the state machine component . This allows script code to report back, that a certain condition was met, and that the state machine may leave its current state. Executing State Machines In theory state machines could be used in many contexts. In custom C++ code you are free to instantiate state machines directly via plStateMachineResource . In a scene, you can instantiate a state machine through the state machine component . See Also State Machine Component Custom Code","title":"State Machine Asset"},{"location":"custom-code/game_logic/state-machine-asset/#state-machine-asset","text":"A state machine asset is used to create a state machine that is commonly used to keep track of an entity's state. State machines are very useful to give objects functionality. For example a door might have the states closed , opening , open and closing and in each state it has to do certain things (apply a rotation) and allow or disallow interaction. State machines are also often used for AI (creatures or NPCs), to give them behavior. Since many AI types are rather simple, this is a viable option. For more complex AI logic, state machines quickly become too limited.","title":"State Machine Asset"},{"location":"custom-code/game_logic/state-machine-asset/#state-machine-concept","text":"At any time exactly one state is active in a state machine. The active state determines what actions an entity will do and what other states it can transition into. For example in the image above you can see that this state machine can only transition from the Idle state into the Alert state (and back), but it can't directly transition into the Attacking state. What an entity does, when a certain state is active, is usually up to other code . On the state node in the state machine asset you select what type of state this is. Different state types can be implemented in C++ or visual scripts . State machines are updated regularly and during every update they may transition into another state. Possible transitions are represented by arrows between states. Just as with states, there are also different types of transitions . As with states, custom transition types can be implemented with C++ or visual scripts . During the state machine update, each transition on the active state is queried, whether its condition is met. If so, the transition is taken, and the state that it points to becomes the new active state. What it means that a transition's condition is met, is up to the transition type's implementation. For example the blackboard transition inspects values from a nearby blackboard , allowing you to set up logical rules. Another type of transition may simply wait for a second and then allow to transition further, acting as a timer. In many state machines there are states that can be reached from pretty much every other state. For example the Dead state in the image above is simply reached whenever the health of a creature reaches zero, no matter which state it currently is in. In a pure state machine, one would add transitions from every node to that final state. However, since this is cumbersome, Plasma also allows to set up transitions that can transition from any active state to their target state. In the state machine asset, such transitions are represented as an Any State (taking the place of any other state ) from which they are drawn to the target state.","title":"State Machine Concept"},{"location":"custom-code/game_logic/state-machine-asset/#configuring-state-machines","text":"Right click into the window and choose New State or New Any State to add a node. To add a transition between states, click the + on a node and drag it to another state. Select state nodes in the main window to see their properties . Here you can give the state a name and select the state type . Most state types have custom properties that also need to be configured here. Select transition arrows to see their properties . Here you can select the transition type . Typically you also then need to configure the transition's properties. If you don't select a state type, a state simply does nothing. If you have a transition without a transition type, the transition will be taken immediately. This can be used to simply chain states. For example the start state is always active, even while the editor is not simulating. It is sometimes undesireable to have this state do anything. Therefore, a second state can be used as the \"real\" start state, and a transition without a type can be added between the two. This transition is only going to be taken once the scene is being simulated.","title":"Configuring State Machines"},{"location":"custom-code/game_logic/state-machine-asset/#default-initial-state","text":"Right click on a state and select Set as Initial State to make a state the default initial state . That means, when this state machine is used in a state machine component or as a nested state machine , and no initial state is selected by the user, this state is used.","title":"Default Initial State"},{"location":"custom-code/game_logic/state-machine-asset/#state-types","text":"You have to select a state type for every node in your state machine.","title":"State Types"},{"location":"custom-code/game_logic/state-machine-asset/#send-message-state","text":"When the Send Message State gets activated (transitioned into) or deactivated (transitioned out of), it sends the plMsgStateMachineStateChanged event message to the game object on which the executing state machine component is attached. Thus other components, such as TypeScript components can listen for this message and react accordingly. The message can be sent with a delay. Also, if this state type is configured to send neither a message on enter, nor on exit, it effectively does nothing and can be used for states that don't require further action. You can also turn on logging on enter or exit, for better debugging.","title":"Send Message State"},{"location":"custom-code/game_logic/state-machine-asset/#nested-state-machine","text":"A Nested State Machine state references another state machine. When the state is entered, it starts executing an instance of that state machine. Messages sent from the nested state machine will be delivered to the same owner game object. As long as the surrounding state stays active, the nested state machine gets updated. Once the surrounding state is exited, execution of the nested state machine is suspended. You can choose whether it gets reset to the initial state, or stay in the last active state. This way, once the nested state machine gets activated again later, it may either start from the beginning, or resume where it left off. Nested state machines can be very useful to reuse state machines and to make editing easier. Also the fact that a state machine can be suspended and resumed at its last active state allows for more complex behavior. If no initial state is specified, the default initial state is used.","title":"Nested State Machine"},{"location":"custom-code/game_logic/state-machine-asset/#compound-state","text":"A Compound State has no functionality by itself, rather it holds an array of other state types. All events (state entered/left) are forwarded to all sub-states equally. Thus it can be used to trigger multiple reactions at the same time.","title":"Compound State"},{"location":"custom-code/game_logic/state-machine-asset/#script-state","text":"The Script State runs a visual scripts . It executes the OnEnter , OnExit and Update event handlers for state machine states.","title":"Script State"},{"location":"custom-code/game_logic/state-machine-asset/#switch-object-state","text":"This state sets the enabled flag on a game object and disables all other objects in the same group. This state allows to easily switch the representation of a game object. For instance you may have two objects states: normal and burning . You can basically just build two objects, one in the normal state, and one with all the effects needed for the fire. Then you group both objects under a shared parent (e.g. with name 'visuals'), give both of them a name ('normal', 'burning') and disable one of them. When the state machine transitions from the normal state to the burning state, you can then use this type of state to say that from the 'visuals' group you want to activate the 'burning' object and deactivate all other objects in the same group. Because the state activates one object and deactivates all others, you can have many different visuals and switch between them. You can also only activate an object and keep the rest in the group as they are (e.g. to enable more and more effects). If you only give a group path, but no object name, you can also use it to just disable all objects in a group. If multiple objects in the same group have the same name, they will all get activated simultaneously. Make sure that essential other objects (like the physics representation or other scripts) are located on other objects, that don't get deactivated.","title":"Switch Object State"},{"location":"custom-code/game_logic/state-machine-asset/#state-properties","text":"PathToGroup : A list of object names, separated with slashes, that forms a relative path starting at the game object on which the state machine component is located, to a game object which acts as a group and contains multiple sub-objects. ObjectToEnable : The name of the sub-object in the group that should get enabled . DeactivateOthers : If true, all other objects within the group get disabled. Otherwise they stay as they are.","title":"State Properties"},{"location":"custom-code/game_logic/state-machine-asset/#transition-types","text":"You have to select a transition type for every transition in your state machine.","title":"Transition Types"},{"location":"custom-code/game_logic/state-machine-asset/#blackboard-conditions","text":"Blackboard conditions query the blackboard . You can configure the transition condition to be fulfilled either when at least one or all the specified blackboard entries have the desired values.","title":"Blackboard Conditions"},{"location":"custom-code/game_logic/state-machine-asset/#timeout-transition","text":"This transition is fulfilled after a certain amount of time has passed. This can be used to automatically transition to the next state after a fixed amount of time. Together with a compound transition it can also be used to prevent transitioning before a minimum of time has passed.","title":"Timeout Transition"},{"location":"custom-code/game_logic/state-machine-asset/#compound-transition","text":"The Compound Transition allows you to create complex logical conditions. It holds an array of other transition types. You can select whether either at least one of them, or all of these conditions need to be met, for the compound transition to be fulfilled. For example, you can add a blackboard condition and a timeout transition. If you set the compound transition to AND , both of these must be true, meaning the transition can only be taken after the blackboard values are correct and the minimum time has passed. If, however, you set it to OR , the transition is taken once either the timeout or the blackboard state is fulfilled. Note that you can use nested compound transitions to create even more complex logical conditions.","title":"Compound Transition"},{"location":"custom-code/game_logic/state-machine-asset/#transition-event-transition","text":"This type of transition is taken when a transition event with the expected name has been raised. Transition events can be raised from visual scripts or C++ through the state machine component . This allows script code to report back, that a certain condition was met, and that the state machine may leave its current state.","title":"Transition Event Transition"},{"location":"custom-code/game_logic/state-machine-asset/#executing-state-machines","text":"In theory state machines could be used in many contexts. In custom C++ code you are free to instantiate state machines directly via plStateMachineResource . In a scene, you can instantiate a state machine through the state machine component .","title":"Executing State Machines"},{"location":"custom-code/game_logic/state-machine-asset/#see-also","text":"State Machine Component Custom Code","title":"See Also"},{"location":"custom-code/game_logic/state-machine-component/","text":"State Machine Component The state machine component creates an instance of a state machine asset and updates that every frame. The InitialState property determines the starting state of the state machine. This component also enables a state machine to send messages to components attached to the same game object . For example the plMsgStateMachineStateChanged event message will be broadcast on this object, if the state machine contains a corresponding state. Component Properties Resource : The state machine to use. InitialState : In which state the state machine is supposed to start. If left empty, the default initial state is used. See Also State Machine Asset Custom Code","title":"State Machine Component"},{"location":"custom-code/game_logic/state-machine-component/#state-machine-component","text":"The state machine component creates an instance of a state machine asset and updates that every frame. The InitialState property determines the starting state of the state machine. This component also enables a state machine to send messages to components attached to the same game object . For example the plMsgStateMachineStateChanged event message will be broadcast on this object, if the state machine contains a corresponding state.","title":"State Machine Component"},{"location":"custom-code/game_logic/state-machine-component/#component-properties","text":"Resource : The state machine to use. InitialState : In which state the state machine is supposed to start. If left empty, the default initial state is used.","title":"Component Properties"},{"location":"custom-code/game_logic/state-machine-component/#see-also","text":"State Machine Asset Custom Code","title":"See Also"},{"location":"custom-code/game_logic/trigger-delay-modifier-component/","text":"Trigger Delay Modifier Component Handles plMsgTriggerTriggered events and sends new messages after a delay. This is typically used to activate something not right away, but when something is present inside a trigger volume for a minimum duration. Similarly, it can also be used from deactivating something too early. The enter and leave messages are sent only when an empty trigger is entered or when the last object leaves the trigger. While any object is already inside the trigger, no change event is sent. Therefore this component can't be used to keep track of all the objects inside a trigger. The enter and leave events can be sent with a delay. The enter event is only sent, if the trigger had at least one object inside it for the full duration of the delay. Which exact object may change, but once the trigger contains no object, the timer is reset. The sent plMsgTriggerTriggered does not contain a reference to the triggering object, since there may be multiple and they may change randomly. Component Properties ActivationDelay , DeactivationDelay : The time that an object needs to be present inside the trigger, or have fully left the trigger area, before the trigger is considered to be activated or deactivated. Only after this delay is the plMsgTriggerTriggered event sent to the parent object. See Also Jolt Trigger Component Messaging","title":"Trigger Delay Modifier Component"},{"location":"custom-code/game_logic/trigger-delay-modifier-component/#trigger-delay-modifier-component","text":"Handles plMsgTriggerTriggered events and sends new messages after a delay. This is typically used to activate something not right away, but when something is present inside a trigger volume for a minimum duration. Similarly, it can also be used from deactivating something too early. The enter and leave messages are sent only when an empty trigger is entered or when the last object leaves the trigger. While any object is already inside the trigger, no change event is sent. Therefore this component can't be used to keep track of all the objects inside a trigger. The enter and leave events can be sent with a delay. The enter event is only sent, if the trigger had at least one object inside it for the full duration of the delay. Which exact object may change, but once the trigger contains no object, the timer is reset. The sent plMsgTriggerTriggered does not contain a reference to the triggering object, since there may be multiple and they may change randomly.","title":"Trigger Delay Modifier Component"},{"location":"custom-code/game_logic/trigger-delay-modifier-component/#component-properties","text":"ActivationDelay , DeactivationDelay : The time that an object needs to be present inside the trigger, or have fully left the trigger area, before the trigger is considered to be activated or deactivated. Only after this delay is the plMsgTriggerTriggered event sent to the parent object.","title":"Component Properties"},{"location":"custom-code/game_logic/trigger-delay-modifier-component/#see-also","text":"Jolt Trigger Component Messaging","title":"See Also"},{"location":"custom-code/typescript/custom-ts-components/","text":"Custom Components with TypeScript To create a new component type, create a new TypeScript asset . In that asset document, click the toolbar button to edit the script with Visual Studio Code. This will not only open the text editor, but also ensure that the .ts file exists and contains a basic template for your new component. Base Class Your component class must extend one of these base classes: pl.TypescriptComponent pl.TickedTypescriptComponent If it extends pl.TypescriptComponent , it can react to messages, startup/shutdown and activation/deactivation callbacks. However, it will not be updated regularly. Though this can be achieved through messages . If it extends pl.TickedTypescriptComponent , the member function Tick() is executed regularly. The rate at which it shall be called can be modified using SetTickInterval() . Often game components need to do regular checks and update their own state. Use the ticked base class when this is necessary. Choose a tick interval that is as long as possible to reduce their performance cost. You can also dynamically change the tick rate, to e.g. do more updates when the player is close, than when they are far away. Whenever possible, though, prefer to use the non-ticked base class and have no regular update, at all. Such components rely on other machnisms, such as triggers to detect when they need to react, and they can use delayed messages (sent by others or by themselves) to trigger follow up work. Tick Function When extending pl.TickedTypescriptComponent , the component code must contain a function with this signature: Tick(): void { } It will be executed during the game update whenever enough time has passed. Use SetTickInterval() to adjust the frequency. Initialization The template code contains examples for these functions: Initialize() Deinitialize() OnActivated() OnDeactivated() OnSimulationStarted() These functions are called in the same way as for C++ components. See Component Activation for details. Message Handlers TypeScript components can both send and receive messages. The article Messaging in TypeScript Code explains this in more detail. To handle messages, message handler functions must be registered first. This is done on a per-type basis, rather than per instance. Therefore you have to register message handlers from within the static function RegisterMessageHandlers() . Auto Generated Code The editor may insert auto generated code into the .ts file. This is needed for example for variables that are supposed to show up as exposed parameters . Special code comments are used to tag the are where the editor can insert the generated code: /* BEGIN AUTO-GENERATED: VARIABLES */ /* END AUTO-GENERATED: VARIABLES */ Important: Don't remove these comments and don't put any of your code between these two lines. Writing Your Component To initialize things, use the OnSimulationStarted() callback. For regular updates, put your code into the Tick() function. Use messaging to communicate with unknown component types or when a delay is desired. For all known component types, you can call functions or read and write properties directly. For an overview what functionality is available through TypeScript, check out the TypeScript API . See Also TypeScript Asset Messaging in TypeScript Code TypeScript API Custom Code with TypeScript","title":"Custom Components with TypeScript"},{"location":"custom-code/typescript/custom-ts-components/#custom-components-with-typescript","text":"To create a new component type, create a new TypeScript asset . In that asset document, click the toolbar button to edit the script with Visual Studio Code. This will not only open the text editor, but also ensure that the .ts file exists and contains a basic template for your new component.","title":"Custom Components with TypeScript"},{"location":"custom-code/typescript/custom-ts-components/#base-class","text":"Your component class must extend one of these base classes: pl.TypescriptComponent pl.TickedTypescriptComponent If it extends pl.TypescriptComponent , it can react to messages, startup/shutdown and activation/deactivation callbacks. However, it will not be updated regularly. Though this can be achieved through messages . If it extends pl.TickedTypescriptComponent , the member function Tick() is executed regularly. The rate at which it shall be called can be modified using SetTickInterval() . Often game components need to do regular checks and update their own state. Use the ticked base class when this is necessary. Choose a tick interval that is as long as possible to reduce their performance cost. You can also dynamically change the tick rate, to e.g. do more updates when the player is close, than when they are far away. Whenever possible, though, prefer to use the non-ticked base class and have no regular update, at all. Such components rely on other machnisms, such as triggers to detect when they need to react, and they can use delayed messages (sent by others or by themselves) to trigger follow up work.","title":"Base Class"},{"location":"custom-code/typescript/custom-ts-components/#tick-function","text":"When extending pl.TickedTypescriptComponent , the component code must contain a function with this signature: Tick(): void { } It will be executed during the game update whenever enough time has passed. Use SetTickInterval() to adjust the frequency.","title":"Tick Function"},{"location":"custom-code/typescript/custom-ts-components/#initialization","text":"The template code contains examples for these functions: Initialize() Deinitialize() OnActivated() OnDeactivated() OnSimulationStarted() These functions are called in the same way as for C++ components. See Component Activation for details.","title":"Initialization"},{"location":"custom-code/typescript/custom-ts-components/#message-handlers","text":"TypeScript components can both send and receive messages. The article Messaging in TypeScript Code explains this in more detail. To handle messages, message handler functions must be registered first. This is done on a per-type basis, rather than per instance. Therefore you have to register message handlers from within the static function RegisterMessageHandlers() .","title":"Message Handlers"},{"location":"custom-code/typescript/custom-ts-components/#auto-generated-code","text":"The editor may insert auto generated code into the .ts file. This is needed for example for variables that are supposed to show up as exposed parameters . Special code comments are used to tag the are where the editor can insert the generated code: /* BEGIN AUTO-GENERATED: VARIABLES */ /* END AUTO-GENERATED: VARIABLES */ Important: Don't remove these comments and don't put any of your code between these two lines.","title":"Auto Generated Code"},{"location":"custom-code/typescript/custom-ts-components/#writing-your-component","text":"To initialize things, use the OnSimulationStarted() callback. For regular updates, put your code into the Tick() function. Use messaging to communicate with unknown component types or when a delay is desired. For all known component types, you can call functions or read and write properties directly. For an overview what functionality is available through TypeScript, check out the TypeScript API .","title":"Writing Your Component"},{"location":"custom-code/typescript/custom-ts-components/#see-also","text":"TypeScript Asset Messaging in TypeScript Code TypeScript API Custom Code with TypeScript","title":"See Also"},{"location":"custom-code/typescript/ts-api/","text":"TypeScript API This page gives an overview over the functionality that Plasma exposes through TypeScript. For an introduction to the TypeScript language please refer to the web (for example TypeScript in 5 minutes ). Note that you don't need to install anything to use TypeScript in Plasma, the required TypeScript transpiler is already included. API Documentation All TypeScript APIs are documented with code comments. In Visual Studio Code you can see the documentation for each function by hovering the mouse cursor over it. You can also jump to a function or class declaration using F12 . This is useful to see what functions are available on a given class. Importing Files ( require ) TypeScript and JavaScript have multiple mechanisms how to make code from other files available. In Plasma only the require mechanism will work: import Plasma= require(\"TypeScript/pl\") This imports all exported declarations from the file TypeScript/pl.ts into an object called pl in this file. Thus typing pl. grants access to all the exported classes, namespaces and functions from that file. The path given to require must be relative to a data directory . For example, the file above is located in the Plugins data directory. Note: require always returns an object and therefore you must assign its result to a variable. Consequently, there is no way to make the imported names globally accessible, you can only access them through that variable. To import multiple files, you need to store each result in a different variable: import Plasma= require(\"TypeScript/pl\") import _ge = require(\"Scripting/GameEnums\") Re-exporting Imported Declarations You can re-export declarations from a .ts file that you imported from somewhere else. For plenty of examples, look at the file pl.ts : import __Log = require(\"TypeScript/pl/Log\") export import Log = __Log.Log; Here, everything from the file Log.ts is imported into the variable __Log . We then selectively re-export the namespace Log from the variable __Log again, under the name Log . We could rename the exported symbol, if we wanted. Unfortunately, there does not seem to be a way to re-export all declarations automatically, you need to name each one individually. Scenegraph pl.Component pl.Component is the base class for all component types, including the C++ components. Your custom components must extend either pl.TypescriptComponent or pl.TickedTypescriptComponent . The functionality exposed through pl.Component is mostly identical to all other components . If you hold a reference to a component for more than a frame, it is vital to use pl.Component.IsValid() to check whether the component is still alive, before accessing it. If IsValid() returns true, the component can be accessed safely for the rest of the frame. pl.GameObject pl.GameObject exposes the game object functionality to TypeScript mostly 1:1. Through this you modify object positions, delete or move child nodes, access attached components and send messages. You can't extend game objects. If you hold a reference to a game object for more than a frame, it is vital to use pl.GameObject.IsValid() to check whether it is still alive, before accessing it. If IsValid() returns true, the game object can be accessed safely for the rest of the frame. pl.World pl.World exposes the world functionality. However, the functionality provided is only a limited subset. Some functionality is simply not needed in the TypeScript binding, and some is exposed differently. Since all TypeScript code is executed in the context of one specific world, you can't access a different world from TypeScript code. Therefore, there is no need to get the world that you operate in (as is common in C++). Therefore pl.World is only a namespace, not a class, and all functionality is always accessible. Additionally, functionality like pl.Clock and pl.Random , which are in C++ bound directly to a world, are similarly just global namespaces in TypeScript and not exposed through pl.World . pl.Message pl.Message and pl.EventMessage are base classes for all messages. The page Messaging in TypeScript Code goes into more detail. Math TypeScript already provides mathematical functions through the Math namespace. Additionally, the Plasma API provides these classes: pl.Vec2 and pl.Vec3 : 2 and 3 component vectors for 2D and 3D linear algebra. pl.Mat3 and pl.Mat4 : A 3x3 and 4x4 matrix. pl.Quat : A quaternion class to handle rotations. pl.Transform : A transform stores a position ( pl.Vec3 ), a rotation ( pl.Quat ) and a scale factor ( pl.Vec3 ). It is mainly used to represent object transformations, and is more convenient than using 4x4 matrices. pl.Angle : Provides utility functions to work with angles. Mostly to convert between radians and degree. pl.Color : A utility class to work with colors. All colors are treated as HDR colors in linear space, though conversions to and from Gamma space are provided. See color spaces (TODO) for details. Debugging pl.Log The pl.Log namespace contains functions for writing messages to the log . This is a useful tool for debugging. pl.Debug The pl.Debug namespace contains various functionality. There are functions for debug rendering, ie. to insert shapes into the rendered output, which can be helpful in visualizing many aspects. pl.Debug also provides access to CVars and console functions . Utilities pl.Clock The pl.Clock namespace has functions to access the world's clock. The clock represents the game time, meaning it advances at its own pace, which can be adjusted dynamically. When you need to know how much time has passed since the last frame (not the last Tick() ), use pl.Clock.GetTimeDiff() . Use pl.Clock.GetAccumulatedTime() it you need to measure longer durations. pl.Time In TypeScript code time should be stored as number types and measured in seconds. This is how all functions expect time values. pl.Time contains utility functions to convert time values to other units and to query the current system time. pl.Random The pl.Random namespace contains functions to get random numbers. Physics In pl.Physics you find functions to query the physics engine. For example to do raycasts or overlap tests. See Also TypeScript Component Messaging in TypeScript Code","title":"TypeScript API"},{"location":"custom-code/typescript/ts-api/#typescript-api","text":"This page gives an overview over the functionality that Plasma exposes through TypeScript. For an introduction to the TypeScript language please refer to the web (for example TypeScript in 5 minutes ). Note that you don't need to install anything to use TypeScript in Plasma, the required TypeScript transpiler is already included.","title":"TypeScript API"},{"location":"custom-code/typescript/ts-api/#api-documentation","text":"All TypeScript APIs are documented with code comments. In Visual Studio Code you can see the documentation for each function by hovering the mouse cursor over it. You can also jump to a function or class declaration using F12 . This is useful to see what functions are available on a given class.","title":"API Documentation"},{"location":"custom-code/typescript/ts-api/#importing-files-require","text":"TypeScript and JavaScript have multiple mechanisms how to make code from other files available. In Plasma only the require mechanism will work: import Plasma= require(\"TypeScript/pl\") This imports all exported declarations from the file TypeScript/pl.ts into an object called pl in this file. Thus typing pl. grants access to all the exported classes, namespaces and functions from that file. The path given to require must be relative to a data directory . For example, the file above is located in the Plugins data directory. Note: require always returns an object and therefore you must assign its result to a variable. Consequently, there is no way to make the imported names globally accessible, you can only access them through that variable. To import multiple files, you need to store each result in a different variable: import Plasma= require(\"TypeScript/pl\") import _ge = require(\"Scripting/GameEnums\")","title":"Importing Files (require)"},{"location":"custom-code/typescript/ts-api/#re-exporting-imported-declarations","text":"You can re-export declarations from a .ts file that you imported from somewhere else. For plenty of examples, look at the file pl.ts : import __Log = require(\"TypeScript/pl/Log\") export import Log = __Log.Log; Here, everything from the file Log.ts is imported into the variable __Log . We then selectively re-export the namespace Log from the variable __Log again, under the name Log . We could rename the exported symbol, if we wanted. Unfortunately, there does not seem to be a way to re-export all declarations automatically, you need to name each one individually.","title":"Re-exporting Imported Declarations"},{"location":"custom-code/typescript/ts-api/#scenegraph","text":"","title":"Scenegraph"},{"location":"custom-code/typescript/ts-api/#plcomponent","text":"pl.Component is the base class for all component types, including the C++ components. Your custom components must extend either pl.TypescriptComponent or pl.TickedTypescriptComponent . The functionality exposed through pl.Component is mostly identical to all other components . If you hold a reference to a component for more than a frame, it is vital to use pl.Component.IsValid() to check whether the component is still alive, before accessing it. If IsValid() returns true, the component can be accessed safely for the rest of the frame.","title":"pl.Component"},{"location":"custom-code/typescript/ts-api/#plgameobject","text":"pl.GameObject exposes the game object functionality to TypeScript mostly 1:1. Through this you modify object positions, delete or move child nodes, access attached components and send messages. You can't extend game objects. If you hold a reference to a game object for more than a frame, it is vital to use pl.GameObject.IsValid() to check whether it is still alive, before accessing it. If IsValid() returns true, the game object can be accessed safely for the rest of the frame.","title":"pl.GameObject"},{"location":"custom-code/typescript/ts-api/#plworld","text":"pl.World exposes the world functionality. However, the functionality provided is only a limited subset. Some functionality is simply not needed in the TypeScript binding, and some is exposed differently. Since all TypeScript code is executed in the context of one specific world, you can't access a different world from TypeScript code. Therefore, there is no need to get the world that you operate in (as is common in C++). Therefore pl.World is only a namespace, not a class, and all functionality is always accessible. Additionally, functionality like pl.Clock and pl.Random , which are in C++ bound directly to a world, are similarly just global namespaces in TypeScript and not exposed through pl.World .","title":"pl.World"},{"location":"custom-code/typescript/ts-api/#plmessage","text":"pl.Message and pl.EventMessage are base classes for all messages. The page Messaging in TypeScript Code goes into more detail.","title":"pl.Message"},{"location":"custom-code/typescript/ts-api/#math","text":"TypeScript already provides mathematical functions through the Math namespace. Additionally, the Plasma API provides these classes: pl.Vec2 and pl.Vec3 : 2 and 3 component vectors for 2D and 3D linear algebra. pl.Mat3 and pl.Mat4 : A 3x3 and 4x4 matrix. pl.Quat : A quaternion class to handle rotations. pl.Transform : A transform stores a position ( pl.Vec3 ), a rotation ( pl.Quat ) and a scale factor ( pl.Vec3 ). It is mainly used to represent object transformations, and is more convenient than using 4x4 matrices. pl.Angle : Provides utility functions to work with angles. Mostly to convert between radians and degree. pl.Color : A utility class to work with colors. All colors are treated as HDR colors in linear space, though conversions to and from Gamma space are provided. See color spaces (TODO) for details.","title":"Math"},{"location":"custom-code/typescript/ts-api/#debugging","text":"","title":"Debugging"},{"location":"custom-code/typescript/ts-api/#pllog","text":"The pl.Log namespace contains functions for writing messages to the log . This is a useful tool for debugging.","title":"pl.Log"},{"location":"custom-code/typescript/ts-api/#pldebug","text":"The pl.Debug namespace contains various functionality. There are functions for debug rendering, ie. to insert shapes into the rendered output, which can be helpful in visualizing many aspects. pl.Debug also provides access to CVars and console functions .","title":"pl.Debug"},{"location":"custom-code/typescript/ts-api/#utilities","text":"","title":"Utilities"},{"location":"custom-code/typescript/ts-api/#plclock","text":"The pl.Clock namespace has functions to access the world's clock. The clock represents the game time, meaning it advances at its own pace, which can be adjusted dynamically. When you need to know how much time has passed since the last frame (not the last Tick() ), use pl.Clock.GetTimeDiff() . Use pl.Clock.GetAccumulatedTime() it you need to measure longer durations.","title":"pl.Clock"},{"location":"custom-code/typescript/ts-api/#pltime","text":"In TypeScript code time should be stored as number types and measured in seconds. This is how all functions expect time values. pl.Time contains utility functions to convert time values to other units and to query the current system time.","title":"pl.Time"},{"location":"custom-code/typescript/ts-api/#plrandom","text":"The pl.Random namespace contains functions to get random numbers.","title":"pl.Random"},{"location":"custom-code/typescript/ts-api/#physics","text":"In pl.Physics you find functions to query the physics engine. For example to do raycasts or overlap tests.","title":"Physics"},{"location":"custom-code/typescript/ts-api/#see-also","text":"TypeScript Component Messaging in TypeScript Code","title":"See Also"},{"location":"custom-code/typescript/ts-asset/","text":"TypeScript Asset Each TypeScript asset manages the script for a single custom TypeScript component . You create a new TypeScript asset using Editor > Create Document... . The document displays the script file contents as read-only. The document has no text editing functionality. The actual script code is stored in a separate .ts file, such that you can edit it with a regular text editor. After creating a new TypeScript asset, the ScriptFile property will have a default value for the .ts file. You can change this location, if you wish. Code Editing The plEditor assumes that you have Visual Studio Code installed. When you click the VSC icon in the toolbar, it will launch Visual Studio Code with a workspace setup that includes all data directories of the project . It also makes sure that the referenced ScriptFile is created and filled out with some template code, if the file doesn't exist yet. From there on, you can write the code in the external editor. Transforming the asset ( Ctrl+E ) will transpile the script and update the text preview. Script Parameters In the asset properties you can add variables . Each variable has a name and a default value. After changing the variables you have to transform the script (toolbar button or Ctrl+E ). This will insert the necessary code into the script. Make sure to not touch the markers for the auto-generated code section. The variables that you add here will become exposed parameters for this script. Meaning, when this asset is used through a TypeScript component , the component can override the values of these variables. Thus you can instantiate the same script many times with different starting parameters. Non-Component TS Files You can split up script code into multiple files, for example to easier share code between TS components. Simply create as many .ts files as you need and 'include' them as needed (using require ). Having a TypeScript asset is not required for such files. TypeScript assets are only needed for the main TS files that represent a proper custom component type. The TypeScript assets are needed in the editor to be able to select the desired TS component code on a TypeScript component , and at runtime they are used to know how to instantiate the script. The assets are not used to identify which files to transpile. Instead, the editor will simply transpile all .ts files that it finds in any data directory of the project. See Also Custom Components with TypeScript TypeScript Component","title":"TypeScript Asset"},{"location":"custom-code/typescript/ts-asset/#typescript-asset","text":"Each TypeScript asset manages the script for a single custom TypeScript component . You create a new TypeScript asset using Editor > Create Document... . The document displays the script file contents as read-only. The document has no text editing functionality. The actual script code is stored in a separate .ts file, such that you can edit it with a regular text editor. After creating a new TypeScript asset, the ScriptFile property will have a default value for the .ts file. You can change this location, if you wish.","title":"TypeScript Asset"},{"location":"custom-code/typescript/ts-asset/#code-editing","text":"The plEditor assumes that you have Visual Studio Code installed. When you click the VSC icon in the toolbar, it will launch Visual Studio Code with a workspace setup that includes all data directories of the project . It also makes sure that the referenced ScriptFile is created and filled out with some template code, if the file doesn't exist yet. From there on, you can write the code in the external editor. Transforming the asset ( Ctrl+E ) will transpile the script and update the text preview.","title":"Code Editing"},{"location":"custom-code/typescript/ts-asset/#script-parameters","text":"In the asset properties you can add variables . Each variable has a name and a default value. After changing the variables you have to transform the script (toolbar button or Ctrl+E ). This will insert the necessary code into the script. Make sure to not touch the markers for the auto-generated code section. The variables that you add here will become exposed parameters for this script. Meaning, when this asset is used through a TypeScript component , the component can override the values of these variables. Thus you can instantiate the same script many times with different starting parameters.","title":"Script Parameters"},{"location":"custom-code/typescript/ts-asset/#non-component-ts-files","text":"You can split up script code into multiple files, for example to easier share code between TS components. Simply create as many .ts files as you need and 'include' them as needed (using require ). Having a TypeScript asset is not required for such files. TypeScript assets are only needed for the main TS files that represent a proper custom component type. The TypeScript assets are needed in the editor to be able to select the desired TS component code on a TypeScript component , and at runtime they are used to know how to instantiate the script. The assets are not used to identify which files to transpile. Instead, the editor will simply transpile all .ts files that it finds in any data directory of the project.","title":"Non-Component TS Files"},{"location":"custom-code/typescript/ts-asset/#see-also","text":"Custom Components with TypeScript TypeScript Component","title":"See Also"},{"location":"custom-code/typescript/ts-component/","text":"TypeScript Component The TypeScript Component represents a custom component that was written in TypeScript, rather than in C++. The component itself is a C++ component. It mediates between C++ and TypeScript by forwarding C++ events and messages to the script code and back. Properties HandleGlobalEvents : If enabled, this component acts as a Global Event Message Handler . This is useful for scripts that should implement logic for an entire level. Script : The TypeScript asset reference for which script to run. Parameters : In case the referenced script has exposed parameters , they are being listed here and can be modified. When the script gets instantiated, the values of these parameters are passed into the script. Modifying the values after the script was started currently has no effect. See Also TypeScript Asset","title":"TypeScript Component"},{"location":"custom-code/typescript/ts-component/#typescript-component","text":"The TypeScript Component represents a custom component that was written in TypeScript, rather than in C++. The component itself is a C++ component. It mediates between C++ and TypeScript by forwarding C++ events and messages to the script code and back.","title":"TypeScript Component"},{"location":"custom-code/typescript/ts-component/#properties","text":"HandleGlobalEvents : If enabled, this component acts as a Global Event Message Handler . This is useful for scripts that should implement logic for an entire level. Script : The TypeScript asset reference for which script to run. Parameters : In case the referenced script has exposed parameters , they are being listed here and can be modified. When the script gets instantiated, the values of these parameters are passed into the script. Modifying the values after the script was started currently has no effect.","title":"Properties"},{"location":"custom-code/typescript/ts-component/#see-also","text":"TypeScript Asset","title":"See Also"},{"location":"custom-code/typescript/ts-messaging/","text":"Messaging in TypeScript Code TypeScript components can both send and receive messages. The way messages can be sent, posted and received, and how messages are routed is identical to the behavior on the C++ side. Please read the chapter about messaging to familiarize yourself with the general concepts. The main difference in TypeScript is, that messages that have been declared in C++ can be sent and received in TypeScript, but messages that have been declared in TypeScript can only be sent and received by TypeScript code. Sending Messages You can either send a message directly to a specific component (through pl.Component ) or to a game object hierarchy (through pl.GameObject ). Contrary to the C++ API, there are no functions on pl.World to send messages. The SendMessage() functions on pl.Component and pl.GameObject take an additional boolean parameter expectResultData . If this is set to true, that means that the sender of the message expects the receiver(s) to write back result data into the sent message, and intends to read those results afterwards. If it is set to false, the message sender does either not expect result data in the message, or doesn't intend to read it. This is an optimization, if you need any result data, set the parameter to true, which means additional work is necessary to synchronize the message back to the caller. Otherwise keep this set to the default value (false). Sending Event Messages TypeScript components can raise event messages on themselves through pl.TypescriptComponent.BroadcastEvent() . Note: At the moment TypeScript components can't raise event messages on other components or game objects. Handling Messages To handle messages of a specific type, a component needs a function that takes that message type as its only parameter, and it must register that function as a message handler: static RegisterMessageHandlers() { pl.TypescriptComponent.RegisterMessageHandler(pl.MsgSetColor, \"OnMsgSetColor\"); } OnMsgSetColor(msg: pl.MsgSetColor): void { pl.Log.Info(\"MsgSetColor: \" + msg.Color.r + \", \" + msg.Color.g + \", \" + msg.Color.b + \", \" + msg.Color.a); } The static function RegisterMessageHandlers() is the only place where your code may call pl.TypescriptComponent.RegisterMessageHandler() . It is an error to call this from anywhere else. Declaring a Message in TypeScript You declare a custom message in TypeScript by extending pl.Message : export class MsgShowText extends pl.Message { PLASMA_DECLARE_MESSAGE_TYPE; text: string; } Important: It is vital to insert PLASMA_DECLARE_MESSAGE_TYPE; into the body of the message to make it work. If you need to send a message from one component and handle it in other component types, you should put the message declaration into a separate .ts file and import that file from both component files. See Importing Files ( require ) for details. Declaring Event Messages Note: At the moment it is not supported to declare event messages . See Also TypeScript API Custom Components with TypeScript","title":"Messaging in TypeScript Code"},{"location":"custom-code/typescript/ts-messaging/#messaging-in-typescript-code","text":"TypeScript components can both send and receive messages. The way messages can be sent, posted and received, and how messages are routed is identical to the behavior on the C++ side. Please read the chapter about messaging to familiarize yourself with the general concepts. The main difference in TypeScript is, that messages that have been declared in C++ can be sent and received in TypeScript, but messages that have been declared in TypeScript can only be sent and received by TypeScript code.","title":"Messaging in TypeScript Code"},{"location":"custom-code/typescript/ts-messaging/#sending-messages","text":"You can either send a message directly to a specific component (through pl.Component ) or to a game object hierarchy (through pl.GameObject ). Contrary to the C++ API, there are no functions on pl.World to send messages. The SendMessage() functions on pl.Component and pl.GameObject take an additional boolean parameter expectResultData . If this is set to true, that means that the sender of the message expects the receiver(s) to write back result data into the sent message, and intends to read those results afterwards. If it is set to false, the message sender does either not expect result data in the message, or doesn't intend to read it. This is an optimization, if you need any result data, set the parameter to true, which means additional work is necessary to synchronize the message back to the caller. Otherwise keep this set to the default value (false).","title":"Sending Messages"},{"location":"custom-code/typescript/ts-messaging/#sending-event-messages","text":"TypeScript components can raise event messages on themselves through pl.TypescriptComponent.BroadcastEvent() . Note: At the moment TypeScript components can't raise event messages on other components or game objects.","title":"Sending Event Messages"},{"location":"custom-code/typescript/ts-messaging/#handling-messages","text":"To handle messages of a specific type, a component needs a function that takes that message type as its only parameter, and it must register that function as a message handler: static RegisterMessageHandlers() { pl.TypescriptComponent.RegisterMessageHandler(pl.MsgSetColor, \"OnMsgSetColor\"); } OnMsgSetColor(msg: pl.MsgSetColor): void { pl.Log.Info(\"MsgSetColor: \" + msg.Color.r + \", \" + msg.Color.g + \", \" + msg.Color.b + \", \" + msg.Color.a); } The static function RegisterMessageHandlers() is the only place where your code may call pl.TypescriptComponent.RegisterMessageHandler() . It is an error to call this from anywhere else.","title":"Handling Messages"},{"location":"custom-code/typescript/ts-messaging/#declaring-a-message-in-typescript","text":"You declare a custom message in TypeScript by extending pl.Message : export class MsgShowText extends pl.Message { PLASMA_DECLARE_MESSAGE_TYPE; text: string; } Important: It is vital to insert PLASMA_DECLARE_MESSAGE_TYPE; into the body of the message to make it work. If you need to send a message from one component and handle it in other component types, you should put the message declaration into a separate .ts file and import that file from both component files. See Importing Files ( require ) for details.","title":"Declaring a Message in TypeScript"},{"location":"custom-code/typescript/ts-messaging/#declaring-event-messages","text":"Note: At the moment it is not supported to declare event messages .","title":"Declaring Event Messages"},{"location":"custom-code/typescript/ts-messaging/#see-also","text":"TypeScript API Custom Components with TypeScript","title":"See Also"},{"location":"custom-code/typescript/typescript-overview/","text":"Custom Code with TypeScript TypeScript (TS) is a language that is built on top of JavaScript. All JavaScript is valid TypeScript, but additionally TypeScript allows you to use strong typing to improve your coding experience. The TypeScript code itself is never executed, instead it is transpiled from TypeScript into pure JavaScript, and then interpreted by a regular JavaScript interpreter. The benefit of using TypeScript is mostly in ease of use. You can edit TypeScript code with any text editor. However, plEditor assumes that you have Visual Studio Code (VSC) installed. When you open a TS file from the editor, it will open an entire workspace in VSC, which contains references to all your project's data directories . This enables VSC to give you full code-completion and type-checking functionality. Editing TypeScript Code Providing a decent text editing experience for code is difficult. Therefore plEditor doesn't even attempt to. All code editing has to be done in an external editor, usually meaning Visual Studio Code. For details see the TypeScript asset . Extending the Engine with TypeScript The TypeScript integration allows you to create custom components . TypeScript components can interact both with each other, as well as with C++ components. The APIs available to TypeScript code are deliberately very similar to their C++ counterparts, to make it easy to migrate a TypeScript component to C++, if the need arises. At the moment you can't use TypeScript to write things like custom game states . Instantiating TypeScript Components TypeScript code is executed through the TypeScript component . This is effectively a C++ component which forwards everything of relevance to script code and back. Therefore you never add a script directly to a game object , instead you attach a TypeScript component, which then references the desired script. Compiling TypeScript Code All functionality that TypeScript provides over JavaScript is technically possible to do with pure JavaScript, it is just very cumbersome. Therefore any piece of TypeScript can be transformed to (more complex) JavaScript code. This step is called transpiling . All script code in a project has to be fully transpiled before a scene can be simulated. Therefore running a scene always triggers the transpilation step. Since the TypeScript transpiler is itself written in JavaScript and therefore executed in a JavaScript virtual machine, this step is unfortunately quite slow, especially for larger files. All results are cached, though, so only modified files ever need to be transpiled again. Note that C++ reflection information is used to expose C++ components, enums, and messages to TypeScript, meaning that certain changes of C++ code can also trigger re-transpilation of some TypeScript files. Messaging TypeScript code can use messages to communicate both with other TS components as well as with C++ components. TypeScript code can handle any message, and it can send (or post ) any message. To communicate with another TS component, you can define custom message types directly in script code. To communicate with a C++ component, only C++ messages can be used, as the C++ code has no means to know and handle a TypeScript message. If necessary to do so, the custom message type has to be defined in C++. See Messaging in TypeScript Code for details. Functionality Available in TypeScript The TypeScript binding is a mixture of auto-generated code and hand-crafted APIs specifically tailored to provide a smooth experience. Auto-Generated Code Where possible reflection information is used to automatically generate the necessary TypeScript code to give Visual Studio Code the needed information for code-completion and error-checking. This is, for example, used to expose all C++ components, enums, flags, and messages. The generated TS code is stored in each project in the folder TypeScript/pl and you will notice that for instance the file AllComponents.ts is re-transpiled when a C++ component is added. Consequently, for things like messages and components, only reflected parts can be available to TypeScript. For components this is obvious, as only reflected parts will show up in the editor UI as well, but for messages you may come across a C++ message for which members are missing in the TS version, as reflecting message properties is technically not necessary for the message to work. If you do need that message on the TypeScript side, you need to add the proper reflection information. Additionally, not all kinds of reflected properties are currently supported for TypeScript. Array , map and set properties are not available, as well as game object handles and component handles . Such reflected properties are simply not included in the auto-generated TS code. Hand-Crafted Code Although auto-generating code is the most convenient to keep large amounts of code in sync, there are limits what can be done. TypeScript and C++ are often very different and to not compromise the usability or performance of either side, the way some aspects are exposed to TypeScript has to be chosen carefully. Basic types such as the math classes (Vec2, Vec3, Mat4, Quat, ...) have been designed to be very similar to their C++ counterparts. However, a big design goal was to minimize the amount of temporaries ('garbage') produced when using them, as this has a significant impact on performance. Therefore, using those classes you will notice that their functions often work 'in place', instead of returning a modified clone. Additionally, TypeScript doesn't support function overloading, which is why the TypeScript variants of pl.Vec3 , etc use more explicit function names for disambiguation. Finally, bindings for larger systems, such as worlds or physics are also built by hand. Here, exposing the C++ API 1:1 to TypeScript would simply not yield a good user experience. Instead, the TypeScript binding is fine tuned to expose useful functionality, and to hide pointless complexity. If you need full control over every aspect, there is no way around using C++ anyway. Consequently, if you decide that scripting is fine for your use case, the binding tries to make this as convenient as possible. See Also TypeScript Visual Studio Code Custom Code TypeScript Asset Custom Components with TypeScript","title":"Custom Code with TypeScript"},{"location":"custom-code/typescript/typescript-overview/#custom-code-with-typescript","text":"TypeScript (TS) is a language that is built on top of JavaScript. All JavaScript is valid TypeScript, but additionally TypeScript allows you to use strong typing to improve your coding experience. The TypeScript code itself is never executed, instead it is transpiled from TypeScript into pure JavaScript, and then interpreted by a regular JavaScript interpreter. The benefit of using TypeScript is mostly in ease of use. You can edit TypeScript code with any text editor. However, plEditor assumes that you have Visual Studio Code (VSC) installed. When you open a TS file from the editor, it will open an entire workspace in VSC, which contains references to all your project's data directories . This enables VSC to give you full code-completion and type-checking functionality.","title":"Custom Code with TypeScript"},{"location":"custom-code/typescript/typescript-overview/#editing-typescript-code","text":"Providing a decent text editing experience for code is difficult. Therefore plEditor doesn't even attempt to. All code editing has to be done in an external editor, usually meaning Visual Studio Code. For details see the TypeScript asset .","title":"Editing TypeScript Code"},{"location":"custom-code/typescript/typescript-overview/#extending-the-engine-with-typescript","text":"The TypeScript integration allows you to create custom components . TypeScript components can interact both with each other, as well as with C++ components. The APIs available to TypeScript code are deliberately very similar to their C++ counterparts, to make it easy to migrate a TypeScript component to C++, if the need arises. At the moment you can't use TypeScript to write things like custom game states .","title":"Extending the Engine with TypeScript"},{"location":"custom-code/typescript/typescript-overview/#instantiating-typescript-components","text":"TypeScript code is executed through the TypeScript component . This is effectively a C++ component which forwards everything of relevance to script code and back. Therefore you never add a script directly to a game object , instead you attach a TypeScript component, which then references the desired script.","title":"Instantiating TypeScript Components"},{"location":"custom-code/typescript/typescript-overview/#compiling-typescript-code","text":"All functionality that TypeScript provides over JavaScript is technically possible to do with pure JavaScript, it is just very cumbersome. Therefore any piece of TypeScript can be transformed to (more complex) JavaScript code. This step is called transpiling . All script code in a project has to be fully transpiled before a scene can be simulated. Therefore running a scene always triggers the transpilation step. Since the TypeScript transpiler is itself written in JavaScript and therefore executed in a JavaScript virtual machine, this step is unfortunately quite slow, especially for larger files. All results are cached, though, so only modified files ever need to be transpiled again. Note that C++ reflection information is used to expose C++ components, enums, and messages to TypeScript, meaning that certain changes of C++ code can also trigger re-transpilation of some TypeScript files.","title":"Compiling TypeScript Code"},{"location":"custom-code/typescript/typescript-overview/#messaging","text":"TypeScript code can use messages to communicate both with other TS components as well as with C++ components. TypeScript code can handle any message, and it can send (or post ) any message. To communicate with another TS component, you can define custom message types directly in script code. To communicate with a C++ component, only C++ messages can be used, as the C++ code has no means to know and handle a TypeScript message. If necessary to do so, the custom message type has to be defined in C++. See Messaging in TypeScript Code for details.","title":"Messaging"},{"location":"custom-code/typescript/typescript-overview/#functionality-available-in-typescript","text":"The TypeScript binding is a mixture of auto-generated code and hand-crafted APIs specifically tailored to provide a smooth experience.","title":"Functionality Available in TypeScript"},{"location":"custom-code/typescript/typescript-overview/#auto-generated-code","text":"Where possible reflection information is used to automatically generate the necessary TypeScript code to give Visual Studio Code the needed information for code-completion and error-checking. This is, for example, used to expose all C++ components, enums, flags, and messages. The generated TS code is stored in each project in the folder TypeScript/pl and you will notice that for instance the file AllComponents.ts is re-transpiled when a C++ component is added. Consequently, for things like messages and components, only reflected parts can be available to TypeScript. For components this is obvious, as only reflected parts will show up in the editor UI as well, but for messages you may come across a C++ message for which members are missing in the TS version, as reflecting message properties is technically not necessary for the message to work. If you do need that message on the TypeScript side, you need to add the proper reflection information. Additionally, not all kinds of reflected properties are currently supported for TypeScript. Array , map and set properties are not available, as well as game object handles and component handles . Such reflected properties are simply not included in the auto-generated TS code.","title":"Auto-Generated Code"},{"location":"custom-code/typescript/typescript-overview/#hand-crafted-code","text":"Although auto-generating code is the most convenient to keep large amounts of code in sync, there are limits what can be done. TypeScript and C++ are often very different and to not compromise the usability or performance of either side, the way some aspects are exposed to TypeScript has to be chosen carefully. Basic types such as the math classes (Vec2, Vec3, Mat4, Quat, ...) have been designed to be very similar to their C++ counterparts. However, a big design goal was to minimize the amount of temporaries ('garbage') produced when using them, as this has a significant impact on performance. Therefore, using those classes you will notice that their functions often work 'in place', instead of returning a modified clone. Additionally, TypeScript doesn't support function overloading, which is why the TypeScript variants of pl.Vec3 , etc use more explicit function names for disambiguation. Finally, bindings for larger systems, such as worlds or physics are also built by hand. Here, exposing the C++ API 1:1 to TypeScript would simply not yield a good user experience. Instead, the TypeScript binding is fine tuned to expose useful functionality, and to hide pointless complexity. If you need full control over every aspect, there is no way around using C++ anyway. Consequently, if you decide that scripting is fine for your use case, the binding tries to make this as convenient as possible.","title":"Hand-Crafted Code"},{"location":"custom-code/typescript/typescript-overview/#see-also","text":"TypeScript Visual Studio Code Custom Code TypeScript Asset Custom Components with TypeScript","title":"See Also"},{"location":"custom-code/visual-script/script-component/","text":"Script Component The Script Component represents a custom component that was written using visual scripting . The component itself is a C++ component. It mediates between C++ and the visual script by forwarding C++ events and messages to the script and back. Component Properties HandleGlobalEvents : If enabled, this component acts as a Global Event Message Handler . This is useful for scripts that should implement logic for an entire level. Script : The visual script to execute. Parameters : In case the referenced script has exposed parameters , they are listed here and can be modified. When the script gets instantiated, the values of these parameters are passed into the script. See Also Custom Code with Visual Scripts Visual Script Class Asset Custom Code","title":"Script Component"},{"location":"custom-code/visual-script/script-component/#script-component","text":"The Script Component represents a custom component that was written using visual scripting . The component itself is a C++ component. It mediates between C++ and the visual script by forwarding C++ events and messages to the script and back.","title":"Script Component"},{"location":"custom-code/visual-script/script-component/#component-properties","text":"HandleGlobalEvents : If enabled, this component acts as a Global Event Message Handler . This is useful for scripts that should implement logic for an entire level. Script : The visual script to execute. Parameters : In case the referenced script has exposed parameters , they are listed here and can be modified. When the script gets instantiated, the values of these parameters are passed into the script.","title":"Component Properties"},{"location":"custom-code/visual-script/script-component/#see-also","text":"Custom Code with Visual Scripts Visual Script Class Asset Custom Code","title":"See Also"},{"location":"custom-code/visual-script/visual-script-class-asset/","text":"Visual Script Class Asset The Visual Script Class asset enables you to define custom logic for components and state machines using a visual programming language. Its intended use is to bridge the gap between what other components provide. For example a trigger component provides an event when something enters an area, and a spawn component can spawn a prefab , however, to have a creature spawn in a room when the player enters it, you need something that connects the two. Visual scripts are a great way to accomplish this. Visual Script Editor The image above shows the visual script editor layout. On the left is the graph editor where you can add and connect nodes . Right-click and drag to pan the view. Use the mouse wheel to zoom. On the right is the property panel which shows the properties of the selected node. Node connections cannot be selected and don't have properties. When nothing is selected, as in this case, the general script properties are displayed. Script Base Class Deselect all nodes to see the script properties. The Base Class property defines in which scenario the script may be used. Base Class: Component When Component is selected as the base class, the script acts like a component. Component specific functions like Component::GetScriptOwner() are only available with this base class. These scripts are used in conjunction with the script component to execute them. Base Class: State Machine State When StateMachineState is selected as the base class, the script acts like a custom state for a state machine . In this case, different functions are available, such as StateMachineState::GetScriptOwner() . These scripts are used in state machines through the script state . Visual Script Variables Through the general script properties you can add variables to your script. These may just be internal variables to keep track of state, but when the Expose flag is enabled, they become exposed parameters . These variables will show up where the script is used (for example on script components ) and allow you to pass in different starting values. Editing Visual Scripts To build a visual script, right-click into the graph editor to open the context menu: The menu shows all the available nodes that can be added. Type into the menu's header to search for specific items. Nodes have pins on the left or right side with which they can be connected. The flow of execution and data goes from left to right. You connect pins via drag and drop. Every pin has a color-coded type. Not all pin types are compatible. Once you start dragging from a pin, all incompatible pins are greyed out and connections snap towards compatible pins. Depending on the target pin type, a data conversion may happen. For example, if a number pin is connected with a string pin , the number will be converted to text. Nodes When you select a node, you will see its properties on the right. When a node has input pins (pins on the left), it often also has properties for those pins. This is used for setting an input to a constant value , rather than passing it in through the pin. So when an input pin is unconnected, the script will use the value from the property grid. Some nodes have additional configuration properties, that cannot be passed in through a pin, for example GameObject::TryGetComponentOfBaseType() where you have to select the desired base type. Execution Pins Execution pins are the grey, arrow-shaped pins at the top of nodes. Not all nodes have them. These pins define the order in which nodes are executed. Only very few nodes have only an outgoing execution pin. These are entry points for the script, meaning they are where script execution starts. For instance, in the image at the top, the Update node is an entry point. Whenever the script gets updated (usually once every frame), this node gets executed. After that, the node that is connected to it through an execution pin gets executed. This continue until the last node in a chain was executed. Some nodes have multiple outgoing execution pins, such as the Switch nodes. These nodes are used to conditionally execute one or another code path . When a node has an incoming execution pin, it must be connected to something, otherwise the node can never be executed. However, nodes that do not have execution pins, at all, are executed on demand whenever their output is needed by another node that is being executed. Data Pins All the round pins are data pins meaning they represent some kind of data. Many different types of data are supported. Numbers are generally convertible into other number types, and nearly everything can be converted to the string type, but most other types mostly exist to be passed unmodified from one node to another. Coroutines The flow of execution starts at an entry point node and follows the execution pins to the right until the last node is reached. Usually this will happen within on script update and thus all nodes along the path are executed within the same game tick. Coroutines allow you to pause execution at any point in the graph, and have the script continue there at a later time. Consequently, a script may have multiple threads of execution , meaning that there might be several execution paths active over a longer period of time. Have a look at this script: There are two entry points , the OnEnter node and the Update node. OnEnter is only executed once when the state machine state got activated. However, Update is executed once every game tick (every frame). The OnEnter code path uses the Wait function. This turns the entire execution path into a coroutine . What this means is that when Wait is encountered, this execution path pauses for a second. After the wait is over, it continues from that point until it runs into the next Wait call. Tip: When an execution path uses coroutine functionality and thus may execute over a longer duration, the entry point node of that path shows an extra icon of two crossing arrows in its top left corner. See the OnEnter node in the image above and compare it to the Update node, which is not a coroutine. In the mean time, the Update node is executed every frame. Thus when it runs, you already have two threads of execution , the one starting in the Update node which finishes right away, and the one that started from the OnEnter which is dormant for a time, but continues after a while. Coroutines are a powerful and very convenient feature, as they make it possible to write code in a very linear fashion, even though there are complex, temporal dependencies. A common use case for coroutines is to sequence AI tasks or quest objectives. For example one may instruct an NPC to walk to a position and then sit down. The command to walk somewhere is given through a node. But then an AI system has to calculate a path and steer the creature around obstacles to make it reach that point. None of this is part of the visual script, instead the walkTo node would be a coroutine node that pauses the script until the task is fulfilled or failed. On success the script would then run the next node to play the sit down animation. So the script is very simple, even though walking to a spot is a very complex operation. Coroutine Modes When an entry point node gets executed that already spawn a thread of execution before, and that thread is not yet finished, there are three different ways to continue. Stop Other : In this case, the existing coroutine gets canceled without notice. Use this mode when you always only want to react to the latest update. For example, you may have a coroutine that moves a creature to a picked location when the player clicks somewhere. Once the player clicks somewhere else, you would want the creature only to walk to the new target, and want to cancel the previous coroutine. Don't Create New : In this mode you let an existing coroutine fully finish before starting a new one. For example a door would react to a button press by fully opening or closing and only react to another button press when the first action is over. Allow Overlap : In this mode every single event would spawn a new coroutine, which all execute in parallel. For example a timer may fire once a second and every time you want to react to this by doing something complex, then you would use this mode to react to all events equally. The coroutine mode is selectable on every entry point node. Advanced Coroutine Features Every entry point provides a coroutine ID. This can be used to cancel a specific coroutine if needed. Similarly, there are functions to stop all coroutines or start separate ones. You can also use the Yield node to interrupt a script at a specific point and have it continue in the next frame. Loops You can execute loops to iterate over data or repeat certain actions. Several different loop nodes are available. They all operate in the same way, that they have two outgoing execution pins . One execution pin is for the loop body . This execution path will be executed repeatedly until the loop is finished. Finally, the completed execution pin is executed to continue with the code that comes after the loop. For example, the following script loops from 0 to 8 (inclusive) and for each iteration it adds the loop index to the counter variable. After the loop has been completed the execution flow continues at the completed pin, so a SetColor message is sent to the owner game object. Also note that the loop body contains a Yield statement so the loop is paused after every iteration and resumed the next frame (see the coroutines section above). Node Types The following broad categories of nodes exist: Event Handlers Event handlers are nodes that get executed when a certain message is sent to any of the objects that this script is responsible for. All event handlers are entry points into the script and most scripts will only execute as a reaction to an event. Blackboards All the nodes for working with blackboards . Clock There are two clocks, the global clock and the world clock . The global one always advances in real-time and should be used for animating things that are independent of the game speed. The world clock should be used for all game-play functionality that should slow down or speed up according to the game's speed, so that they work correctly in slow-motion. Component Here you find all functionality shared by all components such as: GetOwner : Returns the components owner game object. GetWorld : Returns the world that the component belongs to. Additionally, all component specific functionality can be found in the sub-menus. Coroutine Here you find all functionality to work with coroutines, see the coroutines section above. Debug These nodes are for debug rendering . Enums These nodes are for working with enum values. There are two node types for each enum. The value nodes just return a fixed value and can be used to pass along. The switch nodes are used for reading an enum value and then executing a code path depending on the value. Game Object Game objects nodes are for reading and writing object transforms, finding and accessing child objects and components. Log Nodes for logging . Logic This group contains mathematical logic operators as well as conditions and loops. Very important nodes are: Branch : An if condition node with two possible outcomes. Switch : Several variants to map one value to multiple possible outcomes. Compare : Checks whether two values are equal, with a boolean result. Is Valid : Checks whether the incoming value, such as a game object or component, can still be used. Math All sorts of mathematical operations for working with number types. Messages While event handlers react to messages, the script can also send messages to other objects. Messages can be sent directly to a component, or to a game object, in which case they may be broadcast to all components on that object, or even to the whole sub-tree of objects and components. If the Send Mode is set to Event , however, they are delivered not downwards in the hierarchy, but upwards along the parent chain of the target object, to the closest component that handles this type of message. See this chapter for details. Property For reading and writing component properties. StateMachineInstance and StateMachineState For interacting with state machines . This is mainly necessary when the script itself is used as a StateMachineState . Be aware that the state machine instance in which the script is run, is passed into the script through the OnEnter , OnExit and Update nodes. String For working with strings, e.g. to format a string by combining variable values. Time For working with the time data type. Type Conversion These nodes are for converting variables from one type to another. Especially important is the ConvertTo node, which is used for converting a Variant to an expected type. A Variant is a variable that can contain data of many different types. When a node returns a variant, you usually expect that it contains a certain type and using the ConvertTo node, you can get to it. Variable Nodes These nodes operate on visual script variables . The variables have to be declared on the script first. Use these to keep track of state within the script and also to read state that was passed in through exposed parameters . World These nodes provide access to the world , which is used for managing objects. See Also Custom Code with Visual Scripts Script Component","title":"Visual Script Class Asset"},{"location":"custom-code/visual-script/visual-script-class-asset/#visual-script-class-asset","text":"The Visual Script Class asset enables you to define custom logic for components and state machines using a visual programming language. Its intended use is to bridge the gap between what other components provide. For example a trigger component provides an event when something enters an area, and a spawn component can spawn a prefab , however, to have a creature spawn in a room when the player enters it, you need something that connects the two. Visual scripts are a great way to accomplish this.","title":"Visual Script Class Asset"},{"location":"custom-code/visual-script/visual-script-class-asset/#visual-script-editor","text":"The image above shows the visual script editor layout. On the left is the graph editor where you can add and connect nodes . Right-click and drag to pan the view. Use the mouse wheel to zoom. On the right is the property panel which shows the properties of the selected node. Node connections cannot be selected and don't have properties. When nothing is selected, as in this case, the general script properties are displayed.","title":"Visual Script Editor"},{"location":"custom-code/visual-script/visual-script-class-asset/#script-base-class","text":"Deselect all nodes to see the script properties. The Base Class property defines in which scenario the script may be used.","title":"Script Base Class"},{"location":"custom-code/visual-script/visual-script-class-asset/#base-class-component","text":"When Component is selected as the base class, the script acts like a component. Component specific functions like Component::GetScriptOwner() are only available with this base class. These scripts are used in conjunction with the script component to execute them.","title":"Base Class: Component"},{"location":"custom-code/visual-script/visual-script-class-asset/#base-class-state-machine-state","text":"When StateMachineState is selected as the base class, the script acts like a custom state for a state machine . In this case, different functions are available, such as StateMachineState::GetScriptOwner() . These scripts are used in state machines through the script state .","title":"Base Class: State Machine State"},{"location":"custom-code/visual-script/visual-script-class-asset/#visual-script-variables","text":"Through the general script properties you can add variables to your script. These may just be internal variables to keep track of state, but when the Expose flag is enabled, they become exposed parameters . These variables will show up where the script is used (for example on script components ) and allow you to pass in different starting values.","title":"Visual Script Variables"},{"location":"custom-code/visual-script/visual-script-class-asset/#editing-visual-scripts","text":"To build a visual script, right-click into the graph editor to open the context menu: The menu shows all the available nodes that can be added. Type into the menu's header to search for specific items. Nodes have pins on the left or right side with which they can be connected. The flow of execution and data goes from left to right. You connect pins via drag and drop. Every pin has a color-coded type. Not all pin types are compatible. Once you start dragging from a pin, all incompatible pins are greyed out and connections snap towards compatible pins. Depending on the target pin type, a data conversion may happen. For example, if a number pin is connected with a string pin , the number will be converted to text.","title":"Editing Visual Scripts"},{"location":"custom-code/visual-script/visual-script-class-asset/#nodes","text":"When you select a node, you will see its properties on the right. When a node has input pins (pins on the left), it often also has properties for those pins. This is used for setting an input to a constant value , rather than passing it in through the pin. So when an input pin is unconnected, the script will use the value from the property grid. Some nodes have additional configuration properties, that cannot be passed in through a pin, for example GameObject::TryGetComponentOfBaseType() where you have to select the desired base type.","title":"Nodes"},{"location":"custom-code/visual-script/visual-script-class-asset/#execution-pins","text":"Execution pins are the grey, arrow-shaped pins at the top of nodes. Not all nodes have them. These pins define the order in which nodes are executed. Only very few nodes have only an outgoing execution pin. These are entry points for the script, meaning they are where script execution starts. For instance, in the image at the top, the Update node is an entry point. Whenever the script gets updated (usually once every frame), this node gets executed. After that, the node that is connected to it through an execution pin gets executed. This continue until the last node in a chain was executed. Some nodes have multiple outgoing execution pins, such as the Switch nodes. These nodes are used to conditionally execute one or another code path . When a node has an incoming execution pin, it must be connected to something, otherwise the node can never be executed. However, nodes that do not have execution pins, at all, are executed on demand whenever their output is needed by another node that is being executed.","title":"Execution Pins"},{"location":"custom-code/visual-script/visual-script-class-asset/#data-pins","text":"All the round pins are data pins meaning they represent some kind of data. Many different types of data are supported. Numbers are generally convertible into other number types, and nearly everything can be converted to the string type, but most other types mostly exist to be passed unmodified from one node to another.","title":"Data Pins"},{"location":"custom-code/visual-script/visual-script-class-asset/#coroutines","text":"The flow of execution starts at an entry point node and follows the execution pins to the right until the last node is reached. Usually this will happen within on script update and thus all nodes along the path are executed within the same game tick. Coroutines allow you to pause execution at any point in the graph, and have the script continue there at a later time. Consequently, a script may have multiple threads of execution , meaning that there might be several execution paths active over a longer period of time. Have a look at this script: There are two entry points , the OnEnter node and the Update node. OnEnter is only executed once when the state machine state got activated. However, Update is executed once every game tick (every frame). The OnEnter code path uses the Wait function. This turns the entire execution path into a coroutine . What this means is that when Wait is encountered, this execution path pauses for a second. After the wait is over, it continues from that point until it runs into the next Wait call. Tip: When an execution path uses coroutine functionality and thus may execute over a longer duration, the entry point node of that path shows an extra icon of two crossing arrows in its top left corner. See the OnEnter node in the image above and compare it to the Update node, which is not a coroutine. In the mean time, the Update node is executed every frame. Thus when it runs, you already have two threads of execution , the one starting in the Update node which finishes right away, and the one that started from the OnEnter which is dormant for a time, but continues after a while. Coroutines are a powerful and very convenient feature, as they make it possible to write code in a very linear fashion, even though there are complex, temporal dependencies. A common use case for coroutines is to sequence AI tasks or quest objectives. For example one may instruct an NPC to walk to a position and then sit down. The command to walk somewhere is given through a node. But then an AI system has to calculate a path and steer the creature around obstacles to make it reach that point. None of this is part of the visual script, instead the walkTo node would be a coroutine node that pauses the script until the task is fulfilled or failed. On success the script would then run the next node to play the sit down animation. So the script is very simple, even though walking to a spot is a very complex operation.","title":"Coroutines"},{"location":"custom-code/visual-script/visual-script-class-asset/#coroutine-modes","text":"When an entry point node gets executed that already spawn a thread of execution before, and that thread is not yet finished, there are three different ways to continue. Stop Other : In this case, the existing coroutine gets canceled without notice. Use this mode when you always only want to react to the latest update. For example, you may have a coroutine that moves a creature to a picked location when the player clicks somewhere. Once the player clicks somewhere else, you would want the creature only to walk to the new target, and want to cancel the previous coroutine. Don't Create New : In this mode you let an existing coroutine fully finish before starting a new one. For example a door would react to a button press by fully opening or closing and only react to another button press when the first action is over. Allow Overlap : In this mode every single event would spawn a new coroutine, which all execute in parallel. For example a timer may fire once a second and every time you want to react to this by doing something complex, then you would use this mode to react to all events equally. The coroutine mode is selectable on every entry point node.","title":"Coroutine Modes"},{"location":"custom-code/visual-script/visual-script-class-asset/#advanced-coroutine-features","text":"Every entry point provides a coroutine ID. This can be used to cancel a specific coroutine if needed. Similarly, there are functions to stop all coroutines or start separate ones. You can also use the Yield node to interrupt a script at a specific point and have it continue in the next frame.","title":"Advanced Coroutine Features"},{"location":"custom-code/visual-script/visual-script-class-asset/#loops","text":"You can execute loops to iterate over data or repeat certain actions. Several different loop nodes are available. They all operate in the same way, that they have two outgoing execution pins . One execution pin is for the loop body . This execution path will be executed repeatedly until the loop is finished. Finally, the completed execution pin is executed to continue with the code that comes after the loop. For example, the following script loops from 0 to 8 (inclusive) and for each iteration it adds the loop index to the counter variable. After the loop has been completed the execution flow continues at the completed pin, so a SetColor message is sent to the owner game object. Also note that the loop body contains a Yield statement so the loop is paused after every iteration and resumed the next frame (see the coroutines section above).","title":"Loops"},{"location":"custom-code/visual-script/visual-script-class-asset/#node-types","text":"The following broad categories of nodes exist:","title":"Node Types"},{"location":"custom-code/visual-script/visual-script-class-asset/#event-handlers","text":"Event handlers are nodes that get executed when a certain message is sent to any of the objects that this script is responsible for. All event handlers are entry points into the script and most scripts will only execute as a reaction to an event.","title":"Event Handlers"},{"location":"custom-code/visual-script/visual-script-class-asset/#blackboards","text":"All the nodes for working with blackboards .","title":"Blackboards"},{"location":"custom-code/visual-script/visual-script-class-asset/#clock","text":"There are two clocks, the global clock and the world clock . The global one always advances in real-time and should be used for animating things that are independent of the game speed. The world clock should be used for all game-play functionality that should slow down or speed up according to the game's speed, so that they work correctly in slow-motion.","title":"Clock"},{"location":"custom-code/visual-script/visual-script-class-asset/#component","text":"Here you find all functionality shared by all components such as: GetOwner : Returns the components owner game object. GetWorld : Returns the world that the component belongs to. Additionally, all component specific functionality can be found in the sub-menus.","title":"Component"},{"location":"custom-code/visual-script/visual-script-class-asset/#coroutine","text":"Here you find all functionality to work with coroutines, see the coroutines section above.","title":"Coroutine"},{"location":"custom-code/visual-script/visual-script-class-asset/#debug","text":"These nodes are for debug rendering .","title":"Debug"},{"location":"custom-code/visual-script/visual-script-class-asset/#enums","text":"These nodes are for working with enum values. There are two node types for each enum. The value nodes just return a fixed value and can be used to pass along. The switch nodes are used for reading an enum value and then executing a code path depending on the value.","title":"Enums"},{"location":"custom-code/visual-script/visual-script-class-asset/#game-object","text":"Game objects nodes are for reading and writing object transforms, finding and accessing child objects and components.","title":"Game Object"},{"location":"custom-code/visual-script/visual-script-class-asset/#log","text":"Nodes for logging .","title":"Log"},{"location":"custom-code/visual-script/visual-script-class-asset/#logic","text":"This group contains mathematical logic operators as well as conditions and loops. Very important nodes are: Branch : An if condition node with two possible outcomes. Switch : Several variants to map one value to multiple possible outcomes. Compare : Checks whether two values are equal, with a boolean result. Is Valid : Checks whether the incoming value, such as a game object or component, can still be used.","title":"Logic"},{"location":"custom-code/visual-script/visual-script-class-asset/#math","text":"All sorts of mathematical operations for working with number types.","title":"Math"},{"location":"custom-code/visual-script/visual-script-class-asset/#messages","text":"While event handlers react to messages, the script can also send messages to other objects. Messages can be sent directly to a component, or to a game object, in which case they may be broadcast to all components on that object, or even to the whole sub-tree of objects and components. If the Send Mode is set to Event , however, they are delivered not downwards in the hierarchy, but upwards along the parent chain of the target object, to the closest component that handles this type of message. See this chapter for details.","title":"Messages"},{"location":"custom-code/visual-script/visual-script-class-asset/#property","text":"For reading and writing component properties.","title":"Property"},{"location":"custom-code/visual-script/visual-script-class-asset/#statemachineinstance-and-statemachinestate","text":"For interacting with state machines . This is mainly necessary when the script itself is used as a StateMachineState . Be aware that the state machine instance in which the script is run, is passed into the script through the OnEnter , OnExit and Update nodes.","title":"StateMachineInstance and StateMachineState"},{"location":"custom-code/visual-script/visual-script-class-asset/#string","text":"For working with strings, e.g. to format a string by combining variable values.","title":"String"},{"location":"custom-code/visual-script/visual-script-class-asset/#time","text":"For working with the time data type.","title":"Time"},{"location":"custom-code/visual-script/visual-script-class-asset/#type-conversion","text":"These nodes are for converting variables from one type to another. Especially important is the ConvertTo node, which is used for converting a Variant to an expected type. A Variant is a variable that can contain data of many different types. When a node returns a variant, you usually expect that it contains a certain type and using the ConvertTo node, you can get to it.","title":"Type Conversion"},{"location":"custom-code/visual-script/visual-script-class-asset/#variable-nodes","text":"These nodes operate on visual script variables . The variables have to be declared on the script first. Use these to keep track of state within the script and also to read state that was passed in through exposed parameters .","title":"Variable Nodes"},{"location":"custom-code/visual-script/visual-script-class-asset/#world","text":"These nodes provide access to the world , which is used for managing objects.","title":"World"},{"location":"custom-code/visual-script/visual-script-class-asset/#see-also","text":"Custom Code with Visual Scripts Script Component","title":"See Also"},{"location":"custom-code/visual-script/visual-script-overview/","text":"Custom Code with Visual Scripts The engine supports visual scripting as a way to execute custom logic without writing code. Visual scripting aims to be quick and easy to use, while providing a subset of the engine's features that is most useful for the intended use cases. Visual Script Use Cases Visual scripting is meant for small scripts that deal with simple tasks. Often they act as glue code between other systems. For example to wait for an event from one component and then instruct another component to do something. Such logic would be overly cumbersome to set up in C++, and the performance difference is negligible. Visual script code may also be used to quickly prototype behavior to get an idea how something might work. However, dealing with edge-cases and errors typically involves a lot of complex code, and once the stage is reached where a feature should be fully fledged out and polished, it might be better to migrate to C++. In general, visual scripting is no substitue for C++ . It only provides a subset of the features. A lot of functionality is deliberately left inaccessible, such as working with resources (TODO) or game states . If you need to access them, you definitely should use C++ . Custom Components The Visual Script Class Asset allows you to write custom components to interface with other components within the same object hierarchy. Custom State Machine States The Visual Script Class Asset may also be used to write custom state machine states . In this case the script code is executed by a state machine whenever the state is active. Performance Considerations Most visual script code actually calls existing C++ functions. A custom virtual machine ministrates those calls. The execution is relatively fast and shouldn't be a concern for the intended use cases. It is more likely that you run into the limitations of editing the script , than performance bottlenecks. See Also Custom Code Visual Script Class Asset em, you definitely should use C++.","title":"Custom Code with Visual Scripts"},{"location":"custom-code/visual-script/visual-script-overview/#custom-code-with-visual-scripts","text":"The engine supports visual scripting as a way to execute custom logic without writing code. Visual scripting aims to be quick and easy to use, while providing a subset of the engine's features that is most useful for the intended use cases.","title":"Custom Code with Visual Scripts"},{"location":"custom-code/visual-script/visual-script-overview/#visual-script-use-cases","text":"Visual scripting is meant for small scripts that deal with simple tasks. Often they act as glue code between other systems. For example to wait for an event from one component and then instruct another component to do something. Such logic would be overly cumbersome to set up in C++, and the performance difference is negligible. Visual script code may also be used to quickly prototype behavior to get an idea how something might work. However, dealing with edge-cases and errors typically involves a lot of complex code, and once the stage is reached where a feature should be fully fledged out and polished, it might be better to migrate to C++. In general, visual scripting is no substitue for C++ . It only provides a subset of the features. A lot of functionality is deliberately left inaccessible, such as working with resources (TODO) or game states . If you need to access them, you definitely should use C++ .","title":"Visual Script Use Cases"},{"location":"custom-code/visual-script/visual-script-overview/#custom-components","text":"The Visual Script Class Asset allows you to write custom components to interface with other components within the same object hierarchy.","title":"Custom Components"},{"location":"custom-code/visual-script/visual-script-overview/#custom-state-machine-states","text":"The Visual Script Class Asset may also be used to write custom state machine states . In this case the script code is executed by a state machine whenever the state is active.","title":"Custom State Machine States"},{"location":"custom-code/visual-script/visual-script-overview/#performance-considerations","text":"Most visual script code actually calls existing C++ functions. A custom virtual machine ministrates those calls. The execution is relatively fast and shouldn't be a concern for the intended use cases. It is more likely that you run into the limitations of editing the script , than performance bottlenecks.","title":"Performance Considerations"},{"location":"custom-code/visual-script/visual-script-overview/#see-also","text":"Custom Code Visual Script Class Asset em, you definitely should use C++.","title":"See Also"},{"location":"debugging/console/","text":"Console The in-game console is a utility for inspecting the log , modifying CVars and calling console functions . Key bindings The default key binding for the console is: F1 - Opens/closes the console. Up / Down - Select a previously entered command from the history. F2 and F3 - Repeat last and second-to-last commands. This works even when the console is currently closed. ESC - Clears the input line. Page Up / Page Down - Scrolls the log output up / down. TAB - Auto-completes the current input. Also displays all available input options in the output. Ie. lists the names of CVars and console functions and prints their descriptions. Enter - Executes the typed command. If the typed text is only the name of a CVar without an assignment, this will simply print the current value and the description of the CVar. Modify CVars You can modify CVars by typing: CVarName = value See the CVars chapter for details. Binding Keys To bind commands to certain keys you can call: bind f App.ShowFPS= This would bind the command 'App.ShowFPS=' (which toggles the display of the FPS counter) to the f-key. You can only bind commands to printable characters (a-z, 0-9) and the casing matters. So you can also bind another command to SHIFT+f by using bind F ... . To unbind a key call: unbind f Search You can filter the output of the console (the log messages) to only strings that contain some string by typing a * at the beginning: *some text Now the output window will only show strings that contain 'some text'. Console Functions Console functions are an easy way to expose C++ utility functions through the console. The class plConsoleFunction is used to wrap any function (static or method function) in a delegate and enable the console to call it. Of course, since the user can only input certain types of variables in the console, the argument types that you can use are very limited: strings, numbers (int / float) and boolean. This code snippet shows how to declare a console function in a class, for example inside a custom game state . void ConFunc_Print(plString sText); plConsoleFunction<void(plString)> m_ConFunc_Print; In the implementation the binding has to be completed. You need to provide a name under which to expose the function, a description (this should include the parameter list) and the actual function to forward the call to. For member functions this has to be an plDelegate to also bind to the class instance ( this ). SampleGameState::SampleGameState() : m_ConFunc_Print(\"Print\", \"(string arg1): Prints 'arg1' to the log\", plMakeDelegate(&SampleGameState::ConFunc_Print, this)) { } void SampleGameState::ConFunc_Print(plString sText) { plLog::Info(\"Text: '{}'\", sText); } When you now open the console (F1) in-game and press TAB, the 'Print' function will be among the listed functions. You can then execute it: Print(\"Hello Console\") If you need to call a certain function repeatedly, you can bind the call to a key or use F2 and F3 to repeat it, as long as it is the last or second-to-last command in your history. TypeScript You can also register custom console functions through the TypeScript API . See Also CVars Logging","title":"Console"},{"location":"debugging/console/#console","text":"The in-game console is a utility for inspecting the log , modifying CVars and calling console functions .","title":"Console"},{"location":"debugging/console/#key-bindings","text":"The default key binding for the console is: F1 - Opens/closes the console. Up / Down - Select a previously entered command from the history. F2 and F3 - Repeat last and second-to-last commands. This works even when the console is currently closed. ESC - Clears the input line. Page Up / Page Down - Scrolls the log output up / down. TAB - Auto-completes the current input. Also displays all available input options in the output. Ie. lists the names of CVars and console functions and prints their descriptions. Enter - Executes the typed command. If the typed text is only the name of a CVar without an assignment, this will simply print the current value and the description of the CVar.","title":"Key bindings"},{"location":"debugging/console/#modify-cvars","text":"You can modify CVars by typing: CVarName = value See the CVars chapter for details.","title":"Modify CVars"},{"location":"debugging/console/#binding-keys","text":"To bind commands to certain keys you can call: bind f App.ShowFPS= This would bind the command 'App.ShowFPS=' (which toggles the display of the FPS counter) to the f-key. You can only bind commands to printable characters (a-z, 0-9) and the casing matters. So you can also bind another command to SHIFT+f by using bind F ... . To unbind a key call: unbind f","title":"Binding Keys"},{"location":"debugging/console/#search","text":"You can filter the output of the console (the log messages) to only strings that contain some string by typing a * at the beginning: *some text Now the output window will only show strings that contain 'some text'.","title":"Search"},{"location":"debugging/console/#console-functions","text":"Console functions are an easy way to expose C++ utility functions through the console. The class plConsoleFunction is used to wrap any function (static or method function) in a delegate and enable the console to call it. Of course, since the user can only input certain types of variables in the console, the argument types that you can use are very limited: strings, numbers (int / float) and boolean. This code snippet shows how to declare a console function in a class, for example inside a custom game state . void ConFunc_Print(plString sText); plConsoleFunction<void(plString)> m_ConFunc_Print; In the implementation the binding has to be completed. You need to provide a name under which to expose the function, a description (this should include the parameter list) and the actual function to forward the call to. For member functions this has to be an plDelegate to also bind to the class instance ( this ). SampleGameState::SampleGameState() : m_ConFunc_Print(\"Print\", \"(string arg1): Prints 'arg1' to the log\", plMakeDelegate(&SampleGameState::ConFunc_Print, this)) { } void SampleGameState::ConFunc_Print(plString sText) { plLog::Info(\"Text: '{}'\", sText); } When you now open the console (F1) in-game and press TAB, the 'Print' function will be among the listed functions. You can then execute it: Print(\"Hello Console\") If you need to call a certain function repeatedly, you can bind the call to a key or use F2 and F3 to repeat it, as long as it is the last or second-to-last command in your history.","title":"Console Functions"},{"location":"debugging/console/#typescript","text":"You can also register custom console functions through the TypeScript API .","title":"TypeScript"},{"location":"debugging/console/#see-also","text":"CVars Logging","title":"See Also"},{"location":"debugging/cvars/","text":"CVars CVars are global variables used for configuring the runtime. They are used for development to enable or modify hidden features. Types of CVars Only a strictly limited set of CVar types is supported: plCVarBool plCVarFloat plCVarInt plCVarString Accessing and Modifying CVars CVars are exposed in multiple ways. plEditor In Plasma Editor you can open Panels > CVars to show a panel that allows you to modify CVars. Be aware that some CVars only have an effect when simulating the scene, and some even only when using Play-the-Game mode. The latter mostly happens when the effect of a CVar is implemented by a Game State . plInspector plInspector allows you to modify CVars of the connected process using the same UI as the editor. In-Game Console In-game a convenient way to modify CVars is the console . Press TAB to list all CVars Type the beginning of a CVar name and press TAB to list CVars with just that prefix name. Type CVarName = value to modify the CVar's value. cmd cvar_bool = true cvar_bool = cvar_int = 3 cvar_string = \"test\" For boolean CVars, typing 'var =' will toggle the variables value, which can be very handy, especially combined with using F2 to repeat the previous console command. You can also do basic arithmetic like so: cmd cvar_bool = not cvar_bool cvar_int = cvar_int + 1 TypeScript CVars can also be accessed through the TypeScript API . Command Line CVars can be set through command line arguments using this syntax: MyGame.exe -CvarName Value For example: MyGame.exe -Game.DebugDisplay true -fmod_MasterVolume 0.1 Values specified through the command line take precedence over stored values. Saving State The value of a CVar is typically discarded when the program closes, however, if the CVar uses plCVarFlags::Save , it will be saved and restored in the next run. Be careful with this flag, as it can be very confusing when it is used to toggle subtle behavior. Be especially careful keeping this flag in for production code. There is also plCVarFlags::RequiresRestart which means that modifying that variable will take no effect unless you restart the application. This can be used for things like screen resolutions and other initial values. Callbacks You can subscribe to events for either all CVars or specific ones, to be informed when a CVar is modified. Example Code You create a CVar simply by instantiating it as a global variable somewhere in a cpp file: #include <Foundation/Configuration/CVar.h> plCVarBool cvar_DebugDisplay(\"Game.DebugDisplay\", false, plCVarFlags::Default, \"Whether the game should display debug geometry.\"); Then you can just treat it like a regular variable to read or write its value: if (cvar_DebugDisplay) { plDebugRenderer::DrawLineSphere(m_pMainWorld, plBoundingSphere(plVec3::ZeroVector(), 1.0f), plColor::Orange); } See Also Console plInspector","title":"CVars"},{"location":"debugging/cvars/#cvars","text":"CVars are global variables used for configuring the runtime. They are used for development to enable or modify hidden features.","title":"CVars"},{"location":"debugging/cvars/#types-of-cvars","text":"Only a strictly limited set of CVar types is supported: plCVarBool plCVarFloat plCVarInt plCVarString","title":"Types of CVars"},{"location":"debugging/cvars/#accessing-and-modifying-cvars","text":"CVars are exposed in multiple ways.","title":"Accessing and Modifying CVars"},{"location":"debugging/cvars/#pleditor","text":"In Plasma Editor you can open Panels > CVars to show a panel that allows you to modify CVars. Be aware that some CVars only have an effect when simulating the scene, and some even only when using Play-the-Game mode. The latter mostly happens when the effect of a CVar is implemented by a Game State .","title":"plEditor"},{"location":"debugging/cvars/#plinspector","text":"plInspector allows you to modify CVars of the connected process using the same UI as the editor.","title":"plInspector"},{"location":"debugging/cvars/#in-game-console","text":"In-game a convenient way to modify CVars is the console . Press TAB to list all CVars Type the beginning of a CVar name and press TAB to list CVars with just that prefix name. Type CVarName = value to modify the CVar's value. cmd cvar_bool = true cvar_bool = cvar_int = 3 cvar_string = \"test\" For boolean CVars, typing 'var =' will toggle the variables value, which can be very handy, especially combined with using F2 to repeat the previous console command. You can also do basic arithmetic like so: cmd cvar_bool = not cvar_bool cvar_int = cvar_int + 1","title":"In-Game Console"},{"location":"debugging/cvars/#typescript","text":"CVars can also be accessed through the TypeScript API .","title":"TypeScript"},{"location":"debugging/cvars/#command-line","text":"CVars can be set through command line arguments using this syntax: MyGame.exe -CvarName Value For example: MyGame.exe -Game.DebugDisplay true -fmod_MasterVolume 0.1 Values specified through the command line take precedence over stored values.","title":"Command Line"},{"location":"debugging/cvars/#saving-state","text":"The value of a CVar is typically discarded when the program closes, however, if the CVar uses plCVarFlags::Save , it will be saved and restored in the next run. Be careful with this flag, as it can be very confusing when it is used to toggle subtle behavior. Be especially careful keeping this flag in for production code. There is also plCVarFlags::RequiresRestart which means that modifying that variable will take no effect unless you restart the application. This can be used for things like screen resolutions and other initial values.","title":"Saving State"},{"location":"debugging/cvars/#callbacks","text":"You can subscribe to events for either all CVars or specific ones, to be informed when a CVar is modified.","title":"Callbacks"},{"location":"debugging/cvars/#example-code","text":"You create a CVar simply by instantiating it as a global variable somewhere in a cpp file: #include <Foundation/Configuration/CVar.h> plCVarBool cvar_DebugDisplay(\"Game.DebugDisplay\", false, plCVarFlags::Default, \"Whether the game should display debug geometry.\"); Then you can just treat it like a regular variable to read or write its value: if (cvar_DebugDisplay) { plDebugRenderer::DrawLineSphere(m_pMainWorld, plBoundingSphere(plVec3::ZeroVector(), 1.0f), plColor::Orange); }","title":"Example Code"},{"location":"debugging/cvars/#see-also","text":"Console plInspector","title":"See Also"},{"location":"debugging/debug-cpp/","text":"Debugging C++ Code Visual Studio Debug Visualizers To ease debugging the C++ code, we provide a natvis file, located at Code/Engine/Foundation/PlasmaEngine.natvis . The file adds improved inspection for Plasma specific code, such as the container and string classes. This file is already referenced by our CMake scripts in the Foundation library and therefore works out of the box. Debugging the Editor Please be aware that the editor uses at least one, but potentially multiple child processes for its operation. Everything that is actually 3D rendered, is done by the EditorEngineProcess.exe , which is spawned when the editor opens a project. This makes the editor more resilient. However, it means that if you launch the editor in a debugger, by default breakpoints inside the engine runtime code cannot be hit, as you are not attached to the right process. You can attach manually to this process. However, it is much easier to use an extension for Visual Studio: Microsoft Child Process Debugging Power Tool This tool enables Visual Studio to automatically attach to all child processes spawned by the parent process, which makes debugging the Plasma editor much easier. After installing the extension go to Debug > Other Debug Targets > Child Process Debugging Settings... . There you need to enable child process debugging. Additionally, you may want to exclude debugging Git ( git.exe ) and Visual Studio Code ( code.exe ), as those may be spawned by the Plasma editor (indirectly) and have some annoying behavior when attached to with a debugger: Hot Reloading C++ Game Code You can't reload any C++ code at runtime. You can, however, reload the entire engine process in the editor. See this chapter for details. See Also Debug Rendering Jolt Debug Visualizations Shader Debugging Hot Reloading C++ Game Plugins in the Editor","title":"Debugging C++ Code"},{"location":"debugging/debug-cpp/#debugging-c-code","text":"","title":"Debugging C++ Code"},{"location":"debugging/debug-cpp/#visual-studio-debug-visualizers","text":"To ease debugging the C++ code, we provide a natvis file, located at Code/Engine/Foundation/PlasmaEngine.natvis . The file adds improved inspection for Plasma specific code, such as the container and string classes. This file is already referenced by our CMake scripts in the Foundation library and therefore works out of the box.","title":"Visual Studio Debug Visualizers"},{"location":"debugging/debug-cpp/#debugging-the-editor","text":"Please be aware that the editor uses at least one, but potentially multiple child processes for its operation. Everything that is actually 3D rendered, is done by the EditorEngineProcess.exe , which is spawned when the editor opens a project. This makes the editor more resilient. However, it means that if you launch the editor in a debugger, by default breakpoints inside the engine runtime code cannot be hit, as you are not attached to the right process. You can attach manually to this process. However, it is much easier to use an extension for Visual Studio: Microsoft Child Process Debugging Power Tool This tool enables Visual Studio to automatically attach to all child processes spawned by the parent process, which makes debugging the Plasma editor much easier. After installing the extension go to Debug > Other Debug Targets > Child Process Debugging Settings... . There you need to enable child process debugging. Additionally, you may want to exclude debugging Git ( git.exe ) and Visual Studio Code ( code.exe ), as those may be spawned by the Plasma editor (indirectly) and have some annoying behavior when attached to with a debugger:","title":"Debugging the Editor"},{"location":"debugging/debug-cpp/#hot-reloading-c-game-code","text":"You can't reload any C++ code at runtime. You can, however, reload the entire engine process in the editor. See this chapter for details.","title":"Hot Reloading C++ Game Code"},{"location":"debugging/debug-cpp/#see-also","text":"Debug Rendering Jolt Debug Visualizations Shader Debugging Hot Reloading C++ Game Plugins in the Editor","title":"See Also"},{"location":"debugging/debug-rendering/","text":"Debug Rendering The rendering in Plasma is optimized to handle large and complex data efficiently. However, to achieve this you must implement certain patterns, and to get started a non-trivial amount of code is involved. To visualize simple things, the plDebugRenderer is provided. This class allows you to quickly add debug visualizations to your game, with just a few lines of code. This interface is also (partially) available through TypeScript . Usage The plDebugRenderer class has a number of static functions that you can call from any thread and at any time. The debug renderer gathers all requests and renders them at a specific point in the frame. Afterwards all requests are cleared and you must resubmit the same calls in the next frame, if you want it to appear again. Render Context Every function in plDebugRenderer takes an plDebugRendererContext as its first parameter. This specifies where the debug geometry should be rendered. You can either pass in an plViewHandle or an plWorld here. If you pass in a view handle, the geometry will only be rendered in the corresponding plView , meaning you can have the debug geometry only appear in a specific render target. If you pass in an plWorld , the geometry appears in all views that render that world. It is more common to bind debug geometry to an entire plWorld , such that it appears in any view. The TypeScript binding omits the plDebugRendererContext parameter and always binds it to the world. Debug Geometry Types The debug renderer allows you to render arbitrary lines and triangles. For convenience it also provides functions to render a number of common shapes: 3D lines 2D lines (screenspace) Boxes (wireframe) Boxes (solid) Spheres (wireframe) Capsules (wireframe) Frustums (wireframe) 3D triangles (solid) 3D triangles (textured) 2D rectangles (solid, screenspace) 2D rectangles (textured, screenspace) 2D text (screenspace) 2D info text (screenspace, automatically placed non-overlapping) 3D text (3D location, fixed size and always facing the camera) Shading The debug renderer always uses a fullbright shading model. No lighting is ever applied to debug geometry. In fact, most geometry simply has a color, and only some functions allow you to choose a texture. Example A full example for how to use the debug renderer is given in the Sample Game Plugin . Here, the DebugRenderComponent shows how to utilize the debug renderer. The following code snippet is sufficient to render a wireframe sphere at the location of the component: plBoundingSphere sphere; sphere.SetElements(plVec3::ZeroVector(), m_fSize); plDebugRenderer::DrawLineSphere(GetWorld(), sphere, m_Color, ownerTransform); This has to be called in every frame in which it should appear. Therefore this has been added to the DebugRenderComponent::Update() function. Performance Considerations The debug renderer is meant for quickly visualizing data. If the data potentially changes every frame anyway (like visualizing AI raycasts), its performance will be on par with what a 'proper' solution could do. However, if you want to place things in a scene that may stay for a longer duration, it's usually much better to instead build a game object, attach a mesh component and just render a proper asset. This enables culling, static object optimizations, and gives you the option to choose materials . See Also Debugging C++ Code Shader Debugging Jolt Debug Visualizations Custom Code with TypeScript","title":"Debug Rendering"},{"location":"debugging/debug-rendering/#debug-rendering","text":"The rendering in Plasma is optimized to handle large and complex data efficiently. However, to achieve this you must implement certain patterns, and to get started a non-trivial amount of code is involved. To visualize simple things, the plDebugRenderer is provided. This class allows you to quickly add debug visualizations to your game, with just a few lines of code. This interface is also (partially) available through TypeScript .","title":"Debug Rendering"},{"location":"debugging/debug-rendering/#usage","text":"The plDebugRenderer class has a number of static functions that you can call from any thread and at any time. The debug renderer gathers all requests and renders them at a specific point in the frame. Afterwards all requests are cleared and you must resubmit the same calls in the next frame, if you want it to appear again.","title":"Usage"},{"location":"debugging/debug-rendering/#render-context","text":"Every function in plDebugRenderer takes an plDebugRendererContext as its first parameter. This specifies where the debug geometry should be rendered. You can either pass in an plViewHandle or an plWorld here. If you pass in a view handle, the geometry will only be rendered in the corresponding plView , meaning you can have the debug geometry only appear in a specific render target. If you pass in an plWorld , the geometry appears in all views that render that world. It is more common to bind debug geometry to an entire plWorld , such that it appears in any view. The TypeScript binding omits the plDebugRendererContext parameter and always binds it to the world.","title":"Render Context"},{"location":"debugging/debug-rendering/#debug-geometry-types","text":"The debug renderer allows you to render arbitrary lines and triangles. For convenience it also provides functions to render a number of common shapes: 3D lines 2D lines (screenspace) Boxes (wireframe) Boxes (solid) Spheres (wireframe) Capsules (wireframe) Frustums (wireframe) 3D triangles (solid) 3D triangles (textured) 2D rectangles (solid, screenspace) 2D rectangles (textured, screenspace) 2D text (screenspace) 2D info text (screenspace, automatically placed non-overlapping) 3D text (3D location, fixed size and always facing the camera)","title":"Debug Geometry Types"},{"location":"debugging/debug-rendering/#shading","text":"The debug renderer always uses a fullbright shading model. No lighting is ever applied to debug geometry. In fact, most geometry simply has a color, and only some functions allow you to choose a texture.","title":"Shading"},{"location":"debugging/debug-rendering/#example","text":"A full example for how to use the debug renderer is given in the Sample Game Plugin . Here, the DebugRenderComponent shows how to utilize the debug renderer. The following code snippet is sufficient to render a wireframe sphere at the location of the component: plBoundingSphere sphere; sphere.SetElements(plVec3::ZeroVector(), m_fSize); plDebugRenderer::DrawLineSphere(GetWorld(), sphere, m_Color, ownerTransform); This has to be called in every frame in which it should appear. Therefore this has been added to the DebugRenderComponent::Update() function.","title":"Example"},{"location":"debugging/debug-rendering/#performance-considerations","text":"The debug renderer is meant for quickly visualizing data. If the data potentially changes every frame anyway (like visualizing AI raycasts), its performance will be on par with what a 'proper' solution could do. However, if you want to place things in a scene that may stay for a longer duration, it's usually much better to instead build a game object, attach a mesh component and just render a proper asset. This enables culling, static object optimizations, and gives you the option to choose materials .","title":"Performance Considerations"},{"location":"debugging/debug-rendering/#see-also","text":"Debugging C++ Code Shader Debugging Jolt Debug Visualizations Custom Code with TypeScript","title":"See Also"},{"location":"debugging/logging/","text":"Logging Log messages are often very helpful in finding problems. Logging Information The log is accessible through the plLog class. There are multiple functions to log information of different severity: plLog::Debug - for verbose output, will be compiled out in non-debug builds plLog::Dev - for output during development, will typically be silenced (but not compiled out) in non-development builds plLog::Info - for regular information plLog::Warning - for important information that may point at problems plLog::SeriousWarning - for problems that should be fixed but won't crash the system just now plLog::Error - for errors Log messages can be grouped using the PLASMA_LOG_BLOCK macro. TypeScript Information can also be logged through the TypeScript API . Inspecting the Log There are multiple ways to see the content of the log: In the Plasma Editor you can open Panels > Log to see two logs, the one for the editor and the one from the engine process. plInspector shows the log of the connected process. The in-game console outputs the log messages. By default all Plasma applications (TODO) also write the log output to a Log.htm file in the application's appdata folder. Thread-local logging The logging system uses a thread-local variable to store the active logging system, through which to route all messages that originate on that thread. This can be used to easily replace the entire logging backend on a thread and capture all log messages in a custom backend. See plLogSystemScope and plLogInterface , if you want to write a custom backend. This can be used to, for example, capture all output from some subsystem and prevent the messages from reaching the regular outputs. Custom Log Writers plLog is the central class for all messages to be logged. By default, it routes all messages through an instance of plGlobalLog , though you can redirect this on the calling side if you want. On plGlobalLog you can register multiple handlers that take the messages and either write them to some output or forward them to another system. This method is used to, for instance, forward log messages from one process to another. The plInspector integration, for example, registers a custom log writer to gather all log messages, and send them over the network, for display in the external tool. Plasma Editor does something similar for the messages from the engine process. For an in-depth explanation of how you can configure the system, see plLog and plGlobalLog . See Also plInspector Console","title":"Logging"},{"location":"debugging/logging/#logging","text":"Log messages are often very helpful in finding problems.","title":"Logging"},{"location":"debugging/logging/#logging-information","text":"The log is accessible through the plLog class. There are multiple functions to log information of different severity: plLog::Debug - for verbose output, will be compiled out in non-debug builds plLog::Dev - for output during development, will typically be silenced (but not compiled out) in non-development builds plLog::Info - for regular information plLog::Warning - for important information that may point at problems plLog::SeriousWarning - for problems that should be fixed but won't crash the system just now plLog::Error - for errors Log messages can be grouped using the PLASMA_LOG_BLOCK macro.","title":"Logging Information"},{"location":"debugging/logging/#typescript","text":"Information can also be logged through the TypeScript API .","title":"TypeScript"},{"location":"debugging/logging/#inspecting-the-log","text":"There are multiple ways to see the content of the log: In the Plasma Editor you can open Panels > Log to see two logs, the one for the editor and the one from the engine process. plInspector shows the log of the connected process. The in-game console outputs the log messages. By default all Plasma applications (TODO) also write the log output to a Log.htm file in the application's appdata folder.","title":"Inspecting the Log"},{"location":"debugging/logging/#thread-local-logging","text":"The logging system uses a thread-local variable to store the active logging system, through which to route all messages that originate on that thread. This can be used to easily replace the entire logging backend on a thread and capture all log messages in a custom backend. See plLogSystemScope and plLogInterface , if you want to write a custom backend. This can be used to, for example, capture all output from some subsystem and prevent the messages from reaching the regular outputs.","title":"Thread-local logging"},{"location":"debugging/logging/#custom-log-writers","text":"plLog is the central class for all messages to be logged. By default, it routes all messages through an instance of plGlobalLog , though you can redirect this on the calling side if you want. On plGlobalLog you can register multiple handlers that take the messages and either write them to some output or forward them to another system. This method is used to, for instance, forward log messages from one process to another. The plInspector integration, for example, registers a custom log writer to gather all log messages, and send them over the network, for display in the external tool. Plasma Editor does something similar for the messages from the engine process. For an in-depth explanation of how you can configure the system, see plLog and plGlobalLog .","title":"Custom Log Writers"},{"location":"debugging/logging/#see-also","text":"plInspector Console","title":"See Also"},{"location":"debugging/renderdoc/","text":"RenderDoc Integration RenderDoc is a great tool to capture rendering commands for analysis and debugging. Commonly, to analyze a rendering issue, one would launch an application through RenderDoc, such that it can hook into the application and record rendering commands. Plasma Engine has a dedicated RenderDocPlugin , to integrate RenderDoc support even better. When that plugin is active (see Project Settings ) you can trigger a RenderDoc capture at any time, even if your application was not launched through RenderDoc. Taking Captures If you write your own application (TODO) you can hook up RenderDoc in different ways, however, by default these methods are available: Press F11: The F11 key will take a capture of the current frame. Type CaptureFrame() into the game console . All captures are written to a sub-folder of the appdata data directory . On Windows this refers to the %appdata% folder, which you can find by typing %appdata% into Windows Explorer. The exact sub-folder is printed into the log (or see the in-game console ). You can then open the capture using RenderDoc. See Also plInspector Profiling Common Application Features","title":"RenderDoc Integration"},{"location":"debugging/renderdoc/#renderdoc-integration","text":"RenderDoc is a great tool to capture rendering commands for analysis and debugging. Commonly, to analyze a rendering issue, one would launch an application through RenderDoc, such that it can hook into the application and record rendering commands. Plasma Engine has a dedicated RenderDocPlugin , to integrate RenderDoc support even better. When that plugin is active (see Project Settings ) you can trigger a RenderDoc capture at any time, even if your application was not launched through RenderDoc.","title":"RenderDoc Integration"},{"location":"debugging/renderdoc/#taking-captures","text":"If you write your own application (TODO) you can hook up RenderDoc in different ways, however, by default these methods are available: Press F11: The F11 key will take a capture of the current frame. Type CaptureFrame() into the game console . All captures are written to a sub-folder of the appdata data directory . On Windows this refers to the %appdata% folder, which you can find by typing %appdata% into Windows Explorer. The exact sub-folder is printed into the log (or see the in-game console ). You can then open the capture using RenderDoc.","title":"Taking Captures"},{"location":"debugging/renderdoc/#see-also","text":"plInspector Profiling Common Application Features","title":"See Also"},{"location":"debugging/stats/","text":"Stats Stats are a feature for development and debugging. They are arbitrary key/value pairs that your application can set and update frequently, typically once per frame. Stats can be seen and plotted as a graph in plInspector . C++ Interface The stats system is available through plStats . This class allows you to add, modify and remove stats. The plInspector plugin will listen to all changes and send the updated values over the network to plInspector for visualization. You can make plInspector put your stats into a tree structure by using slashes in the stat name. Use Cases Common use cases for stats are to output: Number of things of interest (render polygons, active NPCs, ...) Time spent doing X (script execution, AI updates, ...) Network status (bandwidth usage, ...) Stats are very useful to give an insight into data that is otherwise hidden. They are particularly useful when the way a value behaves is of interest. That means when a value fluctuates and thus may be responsible for inconsistent performance or sudden spikes. When such a value is plotted as a graph in plInspector and put side-by-side with the frame time, it can become easier to find correlations. plInspector Stats appear in a tree structure. You can mark certain stats as 'favorites' using the checkmarks. Using the context menu, you can assign a stat to one of ten graphs, where its value will be plotted over time. Make sure to adjust the graph's min and max value range. See Also plInspector","title":"Stats"},{"location":"debugging/stats/#stats","text":"Stats are a feature for development and debugging. They are arbitrary key/value pairs that your application can set and update frequently, typically once per frame. Stats can be seen and plotted as a graph in plInspector .","title":"Stats"},{"location":"debugging/stats/#c-interface","text":"The stats system is available through plStats . This class allows you to add, modify and remove stats. The plInspector plugin will listen to all changes and send the updated values over the network to plInspector for visualization. You can make plInspector put your stats into a tree structure by using slashes in the stat name.","title":"C++ Interface"},{"location":"debugging/stats/#use-cases","text":"Common use cases for stats are to output: Number of things of interest (render polygons, active NPCs, ...) Time spent doing X (script execution, AI updates, ...) Network status (bandwidth usage, ...) Stats are very useful to give an insight into data that is otherwise hidden. They are particularly useful when the way a value behaves is of interest. That means when a value fluctuates and thus may be responsible for inconsistent performance or sudden spikes. When such a value is plotted as a graph in plInspector and put side-by-side with the frame time, it can become easier to find correlations.","title":"Use Cases"},{"location":"debugging/stats/#plinspector","text":"Stats appear in a tree structure. You can mark certain stats as 'favorites' using the checkmarks. Using the context menu, you can assign a stat to one of ten graphs, where its value will be plotted over time. Make sure to adjust the graph's min and max value range.","title":"plInspector"},{"location":"debugging/stats/#see-also","text":"plInspector","title":"See Also"},{"location":"debugging/components/debug-text-component/","text":"Debug Text Component The debug text component can be used to display a short piece of text at the location of the object. Important: If this component is attached to an object that has no other rendered component, its text will not show up! This component will only render its text, if the parent object is already being rendered because of other things, such as an attached mesh . Component Properties Text : The text to display. It may contain placeholders for up to four float values, e.g. {0} , {1} , etc. The values of the properties Value0 to Value3 will be embedded into the text. Value0, Value1, Value2, Value3 : Four float properties that can be set to arbitrary values. If Text contains appropriate placeholders, these values will be displayed. Color : The color of the text. See Also Debug Rendering","title":"Debug Text Component"},{"location":"debugging/components/debug-text-component/#debug-text-component","text":"The debug text component can be used to display a short piece of text at the location of the object. Important: If this component is attached to an object that has no other rendered component, its text will not show up! This component will only render its text, if the parent object is already being rendered because of other things, such as an attached mesh .","title":"Debug Text Component"},{"location":"debugging/components/debug-text-component/#component-properties","text":"Text : The text to display. It may contain placeholders for up to four float values, e.g. {0} , {1} , etc. The values of the properties Value0 to Value3 will be embedded into the text. Value0, Value1, Value2, Value3 : Four float properties that can be set to arbitrary values. If Text contains appropriate placeholders, these values will be displayed. Color : The color of the text.","title":"Component Properties"},{"location":"debugging/components/debug-text-component/#see-also","text":"Debug Rendering","title":"See Also"},{"location":"debugging/components/draw-line-component/","text":"DrawLineToObject Component The line-to component is a utility component that simply draws a colored line from its own location towards the position of a referenced object . This can be useful for debugging various aspects. Component Properties Target : The referenced object to which to draw the line. Color : The color of the line. See Also Debug Rendering","title":"DrawLineToObject Component"},{"location":"debugging/components/draw-line-component/#drawlinetoobject-component","text":"The line-to component is a utility component that simply draws a colored line from its own location towards the position of a referenced object . This can be useful for debugging various aspects.","title":"DrawLineToObject Component"},{"location":"debugging/components/draw-line-component/#component-properties","text":"Target : The referenced object to which to draw the line. Color : The color of the line.","title":"Component Properties"},{"location":"debugging/components/draw-line-component/#see-also","text":"Debug Rendering","title":"See Also"},{"location":"editor/dashboard/","text":"Dashboard The editor dashboard is a welcome screen that helps you getting started quickly. The buttons on the left-hand switch between the available options. Projects On the projects page you can easily open a recently used project . To create a new project, click the New button. To open an existing project that isn't shown in the list, click the Browse... button. To open a project selected from the list, use the Open Project button or double click the list entry. If you uncheck Always load last project , then the editor won't automatically open the project that you had open last time, when it starts up. Instead, it will always display the dashboard. Samples - TODO See Also Projects Contact","title":"Dashboard"},{"location":"editor/dashboard/#dashboard","text":"The editor dashboard is a welcome screen that helps you getting started quickly. The buttons on the left-hand switch between the available options.","title":"Dashboard"},{"location":"editor/dashboard/#projects","text":"On the projects page you can easily open a recently used project . To create a new project, click the New button. To open an existing project that isn't shown in the list, click the Browse... button. To open a project selected from the list, use the Open Project button or double click the list entry. If you uncheck Always load last project , then the editor won't automatically open the project that you had open last time, when it starts up. Instead, it will always display the dashboard.","title":"Projects"},{"location":"editor/dashboard/#samples-todo","text":"","title":"Samples - TODO"},{"location":"editor/dashboard/#see-also","text":"Projects Contact","title":"See Also"},{"location":"editor/editor-bg-operations/","text":"Editor Background Operations Most data that takes time to be generated or converted, comes from assets . However, there is also scene specific data, which does not qualify as an asset. However, generating it can take very long and therefore should only be triggered on demand. Examples include precomputed lighting or navmeshes. For complex scenes these processes can take very long. Also, if they become out of date, this is usually not a big problem immediately, which means updating the data can be delayed until it is really necessary. Certain components expose background operations . Once you place such a component in a level, its background operation will show up in the Background Operations panel. You can find this under Panels > Background Operations : This panel lists all background operations for all documents. Click Start to make one run in the background. Double click a row to jump directly to the corresponding component. Important: Background operations are currently never run automatically, it is your responsibility to decide when to execute them. Sharing Generated Data The generated data is written to the project's AssetCache/Generated sub-folder. There it is typically ignored by git and thus won't show up for source control check-in. Therefore, either every user has to regenerate the data locally themselves, or you need to force add the desired files to git. Be aware that this type of data typically changes every time and a different storage solution, for example a shared folder on a network drive, may be a better solution. Currently there is no built-in option to change the location where this data is stored, so you would need to use Windows file junctions to redirect the folder or individual files to a different location. See Also Assets","title":"Editor Background Operations"},{"location":"editor/editor-bg-operations/#editor-background-operations","text":"Most data that takes time to be generated or converted, comes from assets . However, there is also scene specific data, which does not qualify as an asset. However, generating it can take very long and therefore should only be triggered on demand. Examples include precomputed lighting or navmeshes. For complex scenes these processes can take very long. Also, if they become out of date, this is usually not a big problem immediately, which means updating the data can be delayed until it is really necessary. Certain components expose background operations . Once you place such a component in a level, its background operation will show up in the Background Operations panel. You can find this under Panels > Background Operations : This panel lists all background operations for all documents. Click Start to make one run in the background. Double click a row to jump directly to the corresponding component. Important: Background operations are currently never run automatically, it is your responsibility to decide when to execute them.","title":"Editor Background Operations"},{"location":"editor/editor-bg-operations/#sharing-generated-data","text":"The generated data is written to the project's AssetCache/Generated sub-folder. There it is typically ignored by git and thus won't show up for source control check-in. Therefore, either every user has to regenerate the data locally themselves, or you need to force add the desired files to git. Be aware that this type of data typically changes every time and a different storage solution, for example a shared folder on a network drive, may be a better solution. Currently there is no built-in option to change the location where this data is stored, so you would need to use Windows file junctions to redirect the folder or individual files to a different location.","title":"Sharing Generated Data"},{"location":"editor/editor-bg-operations/#see-also","text":"Assets","title":"See Also"},{"location":"editor/editor-documents/","text":"Editor Documents In the editor nearly everything that you work with is a dedicated document . Most documents are backed by a file, which is usually in OpenDDL format. Documents are an editor specific representation of things like scenes , prefab , materials and so on. The document format is not understood by the runtime and therefore cannot be loaded by applications like plPlayer directly. A document can represent any type of data. However, the most common case is, that it represents data that shall be converted (or transformed ) from its source representation into something that the engine runtime can use. For example a texture must be transformed from a source JPG format to a more optimized DDS format. These types of documents are called assets in Plasma and they are by far the most common type of document. The only exception to documents in the editor are things like project settings and editor settings , which are not handled by documents. See Also Assets","title":"Editor Documents"},{"location":"editor/editor-documents/#editor-documents","text":"In the editor nearly everything that you work with is a dedicated document . Most documents are backed by a file, which is usually in OpenDDL format. Documents are an editor specific representation of things like scenes , prefab , materials and so on. The document format is not understood by the runtime and therefore cannot be loaded by applications like plPlayer directly. A document can represent any type of data. However, the most common case is, that it represents data that shall be converted (or transformed ) from its source representation into something that the engine runtime can use. For example a texture must be transformed from a source JPG format to a more optimized DDS format. These types of documents are called assets in Plasma and they are by far the most common type of document. The only exception to documents in the editor are things like project settings and editor settings , which are not handled by documents.","title":"Editor Documents"},{"location":"editor/editor-documents/#see-also","text":"Assets","title":"See Also"},{"location":"editor/editor-plugins/","text":"Editor Plugins Editor plugins are fully functional, but currently undocumented. See Also Engine Plugins","title":"Editor Plugins"},{"location":"editor/editor-plugins/#editor-plugins","text":"Editor plugins are fully functional, but currently undocumented.","title":"Editor Plugins"},{"location":"editor/editor-plugins/#see-also","text":"Engine Plugins","title":"See Also"},{"location":"editor/editor-settings/","text":"Editor Settings Editor settings are user specific and independent of projects. They are stored in a OS user folder and thus are not checked into source control. Preferences Preferences are user specific editor settings. Preferences may affect the entire editor application, only a certain project , or even just a single document . The list on the left lists all the available preferences. Items prefixed with Application: affect the general editor, no matter which project is open. Items prefixed with Project: are specific to the currently open project and can be configured differently for other projects. Per-document preferences only show up while a document is open. Application: General RestoreProjectOnStartup: If enabled, the editor will load the project that was open the last time. ShowSpashscreen: Disable this to not have a splashscreen show up every time the editor starts. BackgroundAssetProcessing: If set, background assets processing will be activated by default, when you open a project. FieldOfView: The FOV of the generic editor camera. GizmoScale: Allows you to change the size of the editor gizmos. ShowInDevelopmentFeatures: Some features (mainly components ) are hidden by default from menus. That's because those features are not considered ready to be used productively. If you do want to try them out regardless, check this option for them to show up. UsePrecompiledTools: If enabled, the editor will prefer to use the tools under Data\\Tools\\Precompiled , rather than the ones from its own binary directory. The precompiled tools are built with maximum optimizations and are therefore typically faster, however, they only get updated infrequently and may not have all the latest features and bugfixes. ExpandSceneTreeOnSelection: If enabled, selecting an object in a scene will automatically expand the corresponding item in the scene tree view. If disabled, the scene tree will not change on selection from the viewport, and jumping to the selected tree item has to be done manually using CTRL+T . ClearEditorLogsOnPlay: If enabled, the engine log gets cleared every time you run a scene . Engine View Light Settings: Many assets have 3D previews that require lighting. These are the lighting settings to use. Shortcuts Editor > Editor Settings > Shortcuts... opens a dialog to configure the shortcuts. See Also Project Settings","title":"Editor Settings"},{"location":"editor/editor-settings/#editor-settings","text":"Editor settings are user specific and independent of projects. They are stored in a OS user folder and thus are not checked into source control.","title":"Editor Settings"},{"location":"editor/editor-settings/#preferences","text":"Preferences are user specific editor settings. Preferences may affect the entire editor application, only a certain project , or even just a single document . The list on the left lists all the available preferences. Items prefixed with Application: affect the general editor, no matter which project is open. Items prefixed with Project: are specific to the currently open project and can be configured differently for other projects. Per-document preferences only show up while a document is open.","title":"Preferences"},{"location":"editor/editor-settings/#application-general","text":"RestoreProjectOnStartup: If enabled, the editor will load the project that was open the last time. ShowSpashscreen: Disable this to not have a splashscreen show up every time the editor starts. BackgroundAssetProcessing: If set, background assets processing will be activated by default, when you open a project. FieldOfView: The FOV of the generic editor camera. GizmoScale: Allows you to change the size of the editor gizmos. ShowInDevelopmentFeatures: Some features (mainly components ) are hidden by default from menus. That's because those features are not considered ready to be used productively. If you do want to try them out regardless, check this option for them to show up. UsePrecompiledTools: If enabled, the editor will prefer to use the tools under Data\\Tools\\Precompiled , rather than the ones from its own binary directory. The precompiled tools are built with maximum optimizations and are therefore typically faster, however, they only get updated infrequently and may not have all the latest features and bugfixes. ExpandSceneTreeOnSelection: If enabled, selecting an object in a scene will automatically expand the corresponding item in the scene tree view. If disabled, the scene tree will not change on selection from the viewport, and jumping to the selected tree item has to be done manually using CTRL+T . ClearEditorLogsOnPlay: If enabled, the engine log gets cleared every time you run a scene . Engine View Light Settings: Many assets have 3D previews that require lighting. These are the lighting settings to use.","title":"Application: General"},{"location":"editor/editor-settings/#shortcuts","text":"Editor > Editor Settings > Shortcuts... opens a dialog to configure the shortcuts.","title":"Shortcuts"},{"location":"editor/editor-settings/#see-also","text":"Project Settings","title":"See Also"},{"location":"editor/editor-template-documents/","text":"Template Documents When you create a new document, it is typically blank. You can change this, by saving a pre-configured template document in your project's directory. Every time a new document is created, the editor checks whether such a template document is present, and if so, clones that instead. For the editor to find your template document, it has to be stored in the sub-folder Editor/DocumentTemplates and has to have the name Default . The file extension of course has match the document type. So for example if your project is located under C:/MyGame then to create a custom scene template, you would store a scene file under C:/MyGame/Editor/DocumentTemplates/Default.plScene . This can be done for any document type. See Also Editor Documents","title":"Template Documents"},{"location":"editor/editor-template-documents/#template-documents","text":"When you create a new document, it is typically blank. You can change this, by saving a pre-configured template document in your project's directory. Every time a new document is created, the editor checks whether such a template document is present, and if so, clones that instead. For the editor to find your template document, it has to be stored in the sub-folder Editor/DocumentTemplates and has to have the name Default . The file extension of course has match the document type. So for example if your project is located under C:/MyGame then to create a custom scene template, you would store a scene file under C:/MyGame/Editor/DocumentTemplates/Default.plScene . This can be done for any document type.","title":"Template Documents"},{"location":"editor/editor-template-documents/#see-also","text":"Editor Documents","title":"See Also"},{"location":"editor/editor-views/","text":"Editing Views Most documents come with at least one 3D view. The scene documents allow you to switch between single-view and quad-view mode, using the Toggle Views button in each view toolbar. Using the Perspective menu in the toolbar (the eye icon) you can switch each view to either orthographic or perspective mode. Camera controls and editing gizmos act differently in orthographic and perspective mode. Render Modes Render modes are used to visualize different aspects of the scene. They can be useful for debugging rendering issues, see potential performance hotspots, or easier edit a dark scene. Most 3D viewports allow you to switch the rendering mode through a drop down menu. Default This mode renders the scene as it would appear in the final game. Wireframe In this mode the scene is rendered only as wireframe. Either monochrome or colored. Diffuse Lit Only This mode visualizes only the diffuse lighting contributions. Specular Lit Only This mode visualizes only the specular lighting contributions, including reflections. Decal Count Visualizes how many decals affect each pixel. Yellow and red areas indicate high decal overdraw and will affect performance negatively. Light Count Visualizes how many lights affect each pixel. Yellow and red areas have many contributing lights and will affect performance negatively. Static vs Dynamic This mode visualizes which objects in the scene are static (green) and which ones are dynamic (red). Dynamic objects have a per-frame performance cost, even if they don't move. This mode allows you to find objects that are unnecessarily set to be dynamic. Texture Coordinates There are two modes to visualize the UV0 and UV1 texture coordinates. Normals and Tangents There are multiple modes to visualize normals and tangents, per-vertex and per-pixel. Diffuse Color This mode only shows the diffuse color. It can be very handy for editing a scene that is otherwise very dark. Diffuse Color Range Check In Physically Based Rendering (PBR) the diffuse color values should never be too dark or too bright, as both will not give the best possible results. This mode visualizes which areas may have non-optimal diffuse colors. Emissive Color This mode visualizes which objects use emissive colors. Specular Color Visualizes the specular color. Ambient Occlusion This mode shows ambient occlusion values. These come both from dedicated AO maps, as well as screen space ambient occlusion (SSAO). Depth Visualizes the depth of all objects. Note that depending on the near and far plane settings and the camera distance to the closest object, this mode may appear nearly entirely black or white. For this screenshot the far plane had to be adjusted. Roughness Visualizes the roughness of objects. See Also Editor Camera Scene Editing","title":"Editing Views"},{"location":"editor/editor-views/#editing-views","text":"Most documents come with at least one 3D view. The scene documents allow you to switch between single-view and quad-view mode, using the Toggle Views button in each view toolbar. Using the Perspective menu in the toolbar (the eye icon) you can switch each view to either orthographic or perspective mode. Camera controls and editing gizmos act differently in orthographic and perspective mode.","title":"Editing Views"},{"location":"editor/editor-views/#render-modes","text":"Render modes are used to visualize different aspects of the scene. They can be useful for debugging rendering issues, see potential performance hotspots, or easier edit a dark scene. Most 3D viewports allow you to switch the rendering mode through a drop down menu.","title":"Render Modes"},{"location":"editor/editor-views/#default","text":"This mode renders the scene as it would appear in the final game.","title":"Default"},{"location":"editor/editor-views/#wireframe","text":"In this mode the scene is rendered only as wireframe. Either monochrome or colored.","title":"Wireframe"},{"location":"editor/editor-views/#diffuse-lit-only","text":"This mode visualizes only the diffuse lighting contributions.","title":"Diffuse Lit Only"},{"location":"editor/editor-views/#specular-lit-only","text":"This mode visualizes only the specular lighting contributions, including reflections.","title":"Specular Lit Only"},{"location":"editor/editor-views/#decal-count","text":"Visualizes how many decals affect each pixel. Yellow and red areas indicate high decal overdraw and will affect performance negatively.","title":"Decal Count"},{"location":"editor/editor-views/#light-count","text":"Visualizes how many lights affect each pixel. Yellow and red areas have many contributing lights and will affect performance negatively.","title":"Light Count"},{"location":"editor/editor-views/#static-vs-dynamic","text":"This mode visualizes which objects in the scene are static (green) and which ones are dynamic (red). Dynamic objects have a per-frame performance cost, even if they don't move. This mode allows you to find objects that are unnecessarily set to be dynamic.","title":"Static vs Dynamic"},{"location":"editor/editor-views/#texture-coordinates","text":"There are two modes to visualize the UV0 and UV1 texture coordinates.","title":"Texture Coordinates"},{"location":"editor/editor-views/#normals-and-tangents","text":"There are multiple modes to visualize normals and tangents, per-vertex and per-pixel.","title":"Normals and Tangents"},{"location":"editor/editor-views/#diffuse-color","text":"This mode only shows the diffuse color. It can be very handy for editing a scene that is otherwise very dark.","title":"Diffuse Color"},{"location":"editor/editor-views/#diffuse-color-range-check","text":"In Physically Based Rendering (PBR) the diffuse color values should never be too dark or too bright, as both will not give the best possible results. This mode visualizes which areas may have non-optimal diffuse colors.","title":"Diffuse Color Range Check"},{"location":"editor/editor-views/#emissive-color","text":"This mode visualizes which objects use emissive colors.","title":"Emissive Color"},{"location":"editor/editor-views/#specular-color","text":"Visualizes the specular color.","title":"Specular Color"},{"location":"editor/editor-views/#ambient-occlusion","text":"This mode shows ambient occlusion values. These come both from dedicated AO maps, as well as screen space ambient occlusion (SSAO).","title":"Ambient Occlusion"},{"location":"editor/editor-views/#depth","text":"Visualizes the depth of all objects. Note that depending on the near and far plane settings and the camera distance to the closest object, this mode may appear nearly entirely black or white. For this screenshot the far plane had to be adjusted.","title":"Depth"},{"location":"editor/editor-views/#roughness","text":"Visualizes the roughness of objects.","title":"Roughness"},{"location":"editor/editor-views/#see-also","text":"Editor Camera Scene Editing","title":"See Also"},{"location":"editor/run-scene/","text":"Running a Scene When you have a scene open in the editor, there are multiple ways how you can test it. ![[toolbar-simulate-buttons.png]] Simulate Mode The Simulate Mode is activated with the Play button in the toolbar or by pressing the F5 key. It is deactivated with the Stop button or by pressing Shift+F5 . This enables the simulation of all game objects inside the editor. You can still select objects, modify their properties and move and rotate most of them. However, since at that point the simulation actively modifies the state of objects, some things will not work. For example, physically simulated objects and other objects that actively modify their position may not react to transform changes as expected. Similarly, modifying properties may not have an effect, if the components in question don't reevaluate those properties during simulation. Some objects will also be spawned by the simulation, and therefore are unknown to the editor side. Those objects cannot be picked or modified. In general, modifying the scene during simulation will work for some objects, and won't work for others. The only way to find out, is to try it. Once the simulation is stopped, though, all modifications will be applied as expected, to the reset scene. Keep Simulation Changes The simulate mode is useful to quickly check how some object behaves. It can also be used to simulate physical objects (e.g. boxes falling down) and then save that simulated transform to the scene. While the simulation is active, select the objects that you are interested in and press K (or Scene > Utilities > Keep Simulation Changes ). Once you stop the simulation, an undoable action is executed that moves the selected objects to the recorded location. You can record multiple keep changes actions during one simulation. Play the Game Mode The Play-the-Game Mode (PTG) is activated with the Controller button in the toolbar or by pressing CTRL+F5 . It is deactivated with the Stop button in the editor toolbar, by pressing ESC while the PTG window has focus. This enables the simulation, similar to the Simulate Mode . However, it additionally activates the Game State . Therefore this mode spawns a separate window and also routes all input to the running game. If the scene contains a Player Start Point component the game state may spawn the referenced prefab and thus enable you to properly play the game. You can Alt+Tab out of the PTG window, back to the editor, and modify the scene the same way as in the simulate mode, with the same restrictions. This mode is useful to quickly run the game with full input. Note that, because it is running from the editor process, its framerate is limited. For maximum performance you need to Export and Run the scene. Play From Here If the scene contains a Player Start Point component , then you can right click in the scene and select Play From Here . This starts Play-the-Game and spawns the player prefab at the desired position. This makes it quick and easy to try out a feature in the scene, without having to move the player start position. Export and Run Another way to test the scene is to export it to a binary format and run it in the plPlayer . You do so using Scene > Export and Run... or by pressing CTRL+E . If you keep Transform all Assets checked, all assets in the project will be transformed first, making sure they are up-to-date. You can uncheck this, to speed up the process, if you know that all assets that you require are up-to-date already. Update Thumbnail will use the scene camera with the Thumbnail usage hint, to create a scene thumbnail. Both Simulate Mode and Play-the-Game Mode run inside the editor process. Compared to a game running in a stand-alone process this has two drawbacks: The editor process limits the maximum framerate, and has some performance overhead of its own, so this is not useful for performance testing. All component properties are initialized from the values as the editor has saved them. This is not the (optimized) binary serialization that is used for a final game. Since the binary serialization code has to be written manually, it can happen that properties that are exposed in the editor and work fine there, have been forgotten to be included in the binary serialization and therefore have no effect in the final game. Therefore, always make sure to test custom components properly with exported scenes. Important: If a scene is composed of multiple scene layers , only the objects from the currently loaded layers are exported. Custom Player Executable In the export and run dialog you can add a custom executable to run instead of the default plPlayer application. Use this, if you have a custom game application. The command line used is the same as for plPlayer as shown in the dialog. See Also plPlayer Game States Player Start Point","title":"Running a Scene"},{"location":"editor/run-scene/#running-a-scene","text":"When you have a scene open in the editor, there are multiple ways how you can test it. ![[toolbar-simulate-buttons.png]]","title":"Running a Scene"},{"location":"editor/run-scene/#simulate-mode","text":"The Simulate Mode is activated with the Play button in the toolbar or by pressing the F5 key. It is deactivated with the Stop button or by pressing Shift+F5 . This enables the simulation of all game objects inside the editor. You can still select objects, modify their properties and move and rotate most of them. However, since at that point the simulation actively modifies the state of objects, some things will not work. For example, physically simulated objects and other objects that actively modify their position may not react to transform changes as expected. Similarly, modifying properties may not have an effect, if the components in question don't reevaluate those properties during simulation. Some objects will also be spawned by the simulation, and therefore are unknown to the editor side. Those objects cannot be picked or modified. In general, modifying the scene during simulation will work for some objects, and won't work for others. The only way to find out, is to try it. Once the simulation is stopped, though, all modifications will be applied as expected, to the reset scene.","title":"Simulate Mode"},{"location":"editor/run-scene/#keep-simulation-changes","text":"The simulate mode is useful to quickly check how some object behaves. It can also be used to simulate physical objects (e.g. boxes falling down) and then save that simulated transform to the scene. While the simulation is active, select the objects that you are interested in and press K (or Scene > Utilities > Keep Simulation Changes ). Once you stop the simulation, an undoable action is executed that moves the selected objects to the recorded location. You can record multiple keep changes actions during one simulation.","title":"Keep Simulation Changes"},{"location":"editor/run-scene/#play-the-game-mode","text":"The Play-the-Game Mode (PTG) is activated with the Controller button in the toolbar or by pressing CTRL+F5 . It is deactivated with the Stop button in the editor toolbar, by pressing ESC while the PTG window has focus. This enables the simulation, similar to the Simulate Mode . However, it additionally activates the Game State . Therefore this mode spawns a separate window and also routes all input to the running game. If the scene contains a Player Start Point component the game state may spawn the referenced prefab and thus enable you to properly play the game. You can Alt+Tab out of the PTG window, back to the editor, and modify the scene the same way as in the simulate mode, with the same restrictions. This mode is useful to quickly run the game with full input. Note that, because it is running from the editor process, its framerate is limited. For maximum performance you need to Export and Run the scene.","title":"Play the Game Mode"},{"location":"editor/run-scene/#play-from-here","text":"If the scene contains a Player Start Point component , then you can right click in the scene and select Play From Here . This starts Play-the-Game and spawns the player prefab at the desired position. This makes it quick and easy to try out a feature in the scene, without having to move the player start position.","title":"Play From Here"},{"location":"editor/run-scene/#export-and-run","text":"Another way to test the scene is to export it to a binary format and run it in the plPlayer . You do so using Scene > Export and Run... or by pressing CTRL+E . If you keep Transform all Assets checked, all assets in the project will be transformed first, making sure they are up-to-date. You can uncheck this, to speed up the process, if you know that all assets that you require are up-to-date already. Update Thumbnail will use the scene camera with the Thumbnail usage hint, to create a scene thumbnail. Both Simulate Mode and Play-the-Game Mode run inside the editor process. Compared to a game running in a stand-alone process this has two drawbacks: The editor process limits the maximum framerate, and has some performance overhead of its own, so this is not useful for performance testing. All component properties are initialized from the values as the editor has saved them. This is not the (optimized) binary serialization that is used for a final game. Since the binary serialization code has to be written manually, it can happen that properties that are exposed in the editor and work fine there, have been forgotten to be included in the binary serialization and therefore have no effect in the final game. Therefore, always make sure to test custom components properly with exported scenes. Important: If a scene is composed of multiple scene layers , only the objects from the currently loaded layers are exported.","title":"Export and Run"},{"location":"editor/run-scene/#custom-player-executable","text":"In the export and run dialog you can add a custom executable to run instead of the default plPlayer application. Use this, if you have a custom game application. The command line used is the same as for plPlayer as shown in the dialog.","title":"Custom Player Executable"},{"location":"editor/run-scene/#see-also","text":"plPlayer Game States Player Start Point","title":"See Also"},{"location":"effects/beam-component/","text":"Beam Component The beam component renders a thick line (a \"beam\") starting at the position of the beam component's owner game object and ending at the position of a selected target object . If the object at either end moves, the beam moves with it. The beam component can be used to implement laser beams, for example from trip mines. The beam component is purely a graphical effect, it has no game play functionality. It also has no logic to decide how long the beam shall be. To adjust the length of the beam, the target object has to be positioned at the desired distance. The raycast placement component works well in conjunction with the beam component, as it uses a raycast to decide where to move a referenced object to. You can attach both components to the same object and let them reference the same (dummy) target object, to get a beam that always stretches towards the closest obstacle. Component Properties TargetObject : The beam geometry starts at the beam component position and goes towards the position of the referenced object . If the target object moves, the beam follows. Material : The material to use to render the beam geometry with. Color : The tint color for the material. Width : The thickness of the beam geometry. UVUnitsPerWorldUnit : How to stretch the material across the geometry. See Also Raycast Placement Component","title":"Beam Component"},{"location":"effects/beam-component/#beam-component","text":"The beam component renders a thick line (a \"beam\") starting at the position of the beam component's owner game object and ending at the position of a selected target object . If the object at either end moves, the beam moves with it. The beam component can be used to implement laser beams, for example from trip mines. The beam component is purely a graphical effect, it has no game play functionality. It also has no logic to decide how long the beam shall be. To adjust the length of the beam, the target object has to be positioned at the desired distance. The raycast placement component works well in conjunction with the beam component, as it uses a raycast to decide where to move a referenced object to. You can attach both components to the same object and let them reference the same (dummy) target object, to get a beam that always stretches towards the closest obstacle.","title":"Beam Component"},{"location":"effects/beam-component/#component-properties","text":"TargetObject : The beam geometry starts at the beam component position and goes towards the position of the referenced object . If the target object moves, the beam follows. Material : The material to use to render the beam geometry with. Color : The tint color for the material. Width : The thickness of the beam geometry. UVUnitsPerWorldUnit : How to stretch the material across the geometry.","title":"Component Properties"},{"location":"effects/beam-component/#see-also","text":"Raycast Placement Component","title":"See Also"},{"location":"effects/cloth-sheet-component/","text":"Cloth Sheet Component The cloth sheet component simulates a square patch of cloth as it hangs and swings in the wind. It is meant for decorative purposes such as flags. Cloth sheets are affected by wind and movement of the owner object. They do not interact with physics objects and they don't collide with scene geometry. Component Properties Size : The physical size of the cloth sheet in the world. Slack : How much slack the cloth has along the X and Y axis. A value of zero means it is hung perfectly straight between its anchors. Positive values make it sag downwards. Segments : How detailed to simulate the cloth. Use as low values as possible, the simulation quickly becomes prohibitively expensive with higher tesselations. Damping : How quickly the cloth loses energy while swinging. Higher values make it come to rest more quickly, low values make it swing for a longer time. Once it comes to rest, it takes significantly less processing power. WindInfluence : How strongly wind should make the cloth swing. Flags : These define at which corners and edges the sheet of cloth is attached to the world. Material : The material used for rendering the cloth. Make sure to set it to two-sided for cloth that can be seen from both sides. Color : An additional tint-color for rendering. See Also Fake Rope Component Wind","title":"Cloth Sheet Component"},{"location":"effects/cloth-sheet-component/#cloth-sheet-component","text":"The cloth sheet component simulates a square patch of cloth as it hangs and swings in the wind. It is meant for decorative purposes such as flags. Cloth sheets are affected by wind and movement of the owner object. They do not interact with physics objects and they don't collide with scene geometry.","title":"Cloth Sheet Component"},{"location":"effects/cloth-sheet-component/#component-properties","text":"Size : The physical size of the cloth sheet in the world. Slack : How much slack the cloth has along the X and Y axis. A value of zero means it is hung perfectly straight between its anchors. Positive values make it sag downwards. Segments : How detailed to simulate the cloth. Use as low values as possible, the simulation quickly becomes prohibitively expensive with higher tesselations. Damping : How quickly the cloth loses energy while swinging. Higher values make it come to rest more quickly, low values make it swing for a longer time. Once it comes to rest, it takes significantly less processing power. WindInfluence : How strongly wind should make the cloth swing. Flags : These define at which corners and edges the sheet of cloth is attached to the world. Material : The material used for rendering the cloth. Make sure to set it to two-sided for cloth that can be seen from both sides. Color : An additional tint-color for rendering.","title":"Component Properties"},{"location":"effects/cloth-sheet-component/#see-also","text":"Fake Rope Component Wind","title":"See Also"},{"location":"effects/decals/","text":"Decals Decals are textures that are projected onto the underlying geometry. Decals can be used to to apply text and other signs to geometry. The most common use-case, though, is to make scenes look more natural by simulating wear and tear, such as dirt and scratches. Decals can also be used to simulate dynamic surface imperfections like bullet holes, soot and blood spatters. Decal Asset Before being able to place a decal component , you must create a decal asset . Decal Asset Properties Mode: The mode specifies which surface properties (color, normal, occlusion/roughness/normal) the decal will affect. If the mode is set to BaseColor only, it will change the geometry's underlying color, but nothing else. If it is set to BaseColor, Normal , it will also modify the surface's normal, etc. BlendModeColorize: If this is disabled, the decal's color texture will be applied 1:1 and the decal's alpha channel specifies the blend factor. If BlendModeColorize is true, the decal's color texture is used to 'change' the color of the underlying geometry, but not 'overwrite' it. A middle-grey value in the decal color texture means the decal will not change the underlying geometry color, at all. A darker value will darken the underlying geometry and a lighter value will lighten up the underlying color. This mode is useful for decals that should always darken or brighten the underlying geometry slightly, instead of overwriting the existing color and thus resulting in some fixed brightness. AlphaMask: An optional separate texture to specify the decal's alpha channel. If this is not specified, the decal's opacity is taken from the alpha channel of the Base Color texture. This can be used to combine a dedicated grey-scale texture to specify the decal's shape, and combine it with some arbitrary texture to specify the color pattern. Note: If an AlphaMask texture is given, all other textures are resized to be no larger than this. Base Color: The texture that defines the decal's color. If no separate AlphaMask texture is given, the alpha channel of this texture will also define the shape of the decal. Normal, ORM, Emissive: If the respective Mode is selected, these settings show up for you to specify which textures to use to modify the normal and/or occlusion/roughness/metalness . If the mode is BaseColor, Emissive , a dedicated Emissive texture can be used to specify which pixels will glow with which color, though in that case you cannot overwrite the normal or ORM values. Decal Component Each decal component represents a single instance of a decal. Its position, rotation and scale define where the decal appears. Decal Component Properties Decals: An array of decal asset references. When the game starts, a random decal from this list is chosen for display. ProjectionAxis: The axis along which to project the decal. Extents: The size of the decal along each axis. SizeVariance: If this value is non-zero, the decal's Extents will be randomized between Extents - Extents*Variance and Extents + Extents*Variance using a Normal Distribution. See Variance Values . Color: A tint color for the decal. EmissiveColor: If set to anything other than black, the decal will glow with this color. SortOrder: A float value to adjust whether this decal will appear before or behind other decals. Default is zero. Decals with higher values will be rendered on top of decals with lower values. WrapAround: If disabled, the decal is simply projected onto the geometry along the selected axis. This can result in visible stretching along orthogonal axes. When WrapAround is enabled, the depth along the projection axis is used to modify the decal's UV coordinates. This trades less stretching, for other distortions. Enabling WrapAround may be useful for 'organic' decals, such as dirt and fluid spatters. For 'mechanical' decals, such as road signs, it should be disabled. MapNormalToGeometry: If enabled, the normal of the decal is considered to be relative to the normal of the underlying geometry. Thus it will 'adjust' the normal of the geometry and the direction from where the decal is projected has no influence on the final pixel normal. This is useful for decals that should act like a layer on top of some geometry, for example fluids and scratches. If disabled, the decal will completely overwrite the normal of the underlying geometry and thus the direction from where the decal is projected has a significant influence. This is useful for decals that should show exactly from where they were projected, for example bullet holes. InnerFadeAngle, OuterFadeAngle: When a decal is projected onto geometry at an angle (not straight down), the inner fade angle specifies at what angle the decal starts to fade out and the outer fade angle specifies at what angle the decal will be completely invisible. FadeOutDelay, FadeOutDuration: If these are non-zero, the decal will fade out over FadeOutDuration seconds starting after FadeOutDelay seconds. OnFinishedAction: If the decal component is set to fade out, it may delete itself or its entire owner object afterwards. ApplyToDynamic: By default, decals apply to static geometry but not to dynamic geometry . If it is desired for a decal to be projected onto a dynamic object, this property should be used to select exactly to which dynamic object the decal should be applied to. Note that decals can only be applied to a single dynamic game object. If the selected object turns out to be static , though, the decal will be invisible. See Also Particle Effects","title":"Decals"},{"location":"effects/decals/#decals","text":"Decals are textures that are projected onto the underlying geometry. Decals can be used to to apply text and other signs to geometry. The most common use-case, though, is to make scenes look more natural by simulating wear and tear, such as dirt and scratches. Decals can also be used to simulate dynamic surface imperfections like bullet holes, soot and blood spatters.","title":"Decals"},{"location":"effects/decals/#decal-asset","text":"Before being able to place a decal component , you must create a decal asset .","title":"Decal Asset"},{"location":"effects/decals/#decal-asset-properties","text":"Mode: The mode specifies which surface properties (color, normal, occlusion/roughness/normal) the decal will affect. If the mode is set to BaseColor only, it will change the geometry's underlying color, but nothing else. If it is set to BaseColor, Normal , it will also modify the surface's normal, etc. BlendModeColorize: If this is disabled, the decal's color texture will be applied 1:1 and the decal's alpha channel specifies the blend factor. If BlendModeColorize is true, the decal's color texture is used to 'change' the color of the underlying geometry, but not 'overwrite' it. A middle-grey value in the decal color texture means the decal will not change the underlying geometry color, at all. A darker value will darken the underlying geometry and a lighter value will lighten up the underlying color. This mode is useful for decals that should always darken or brighten the underlying geometry slightly, instead of overwriting the existing color and thus resulting in some fixed brightness. AlphaMask: An optional separate texture to specify the decal's alpha channel. If this is not specified, the decal's opacity is taken from the alpha channel of the Base Color texture. This can be used to combine a dedicated grey-scale texture to specify the decal's shape, and combine it with some arbitrary texture to specify the color pattern. Note: If an AlphaMask texture is given, all other textures are resized to be no larger than this. Base Color: The texture that defines the decal's color. If no separate AlphaMask texture is given, the alpha channel of this texture will also define the shape of the decal. Normal, ORM, Emissive: If the respective Mode is selected, these settings show up for you to specify which textures to use to modify the normal and/or occlusion/roughness/metalness . If the mode is BaseColor, Emissive , a dedicated Emissive texture can be used to specify which pixels will glow with which color, though in that case you cannot overwrite the normal or ORM values.","title":"Decal Asset Properties"},{"location":"effects/decals/#decal-component","text":"Each decal component represents a single instance of a decal. Its position, rotation and scale define where the decal appears.","title":"Decal Component"},{"location":"effects/decals/#decal-component-properties","text":"Decals: An array of decal asset references. When the game starts, a random decal from this list is chosen for display. ProjectionAxis: The axis along which to project the decal. Extents: The size of the decal along each axis. SizeVariance: If this value is non-zero, the decal's Extents will be randomized between Extents - Extents*Variance and Extents + Extents*Variance using a Normal Distribution. See Variance Values . Color: A tint color for the decal. EmissiveColor: If set to anything other than black, the decal will glow with this color. SortOrder: A float value to adjust whether this decal will appear before or behind other decals. Default is zero. Decals with higher values will be rendered on top of decals with lower values. WrapAround: If disabled, the decal is simply projected onto the geometry along the selected axis. This can result in visible stretching along orthogonal axes. When WrapAround is enabled, the depth along the projection axis is used to modify the decal's UV coordinates. This trades less stretching, for other distortions. Enabling WrapAround may be useful for 'organic' decals, such as dirt and fluid spatters. For 'mechanical' decals, such as road signs, it should be disabled. MapNormalToGeometry: If enabled, the normal of the decal is considered to be relative to the normal of the underlying geometry. Thus it will 'adjust' the normal of the geometry and the direction from where the decal is projected has no influence on the final pixel normal. This is useful for decals that should act like a layer on top of some geometry, for example fluids and scratches. If disabled, the decal will completely overwrite the normal of the underlying geometry and thus the direction from where the decal is projected has a significant influence. This is useful for decals that should show exactly from where they were projected, for example bullet holes. InnerFadeAngle, OuterFadeAngle: When a decal is projected onto geometry at an angle (not straight down), the inner fade angle specifies at what angle the decal starts to fade out and the outer fade angle specifies at what angle the decal will be completely invisible. FadeOutDelay, FadeOutDuration: If these are non-zero, the decal will fade out over FadeOutDuration seconds starting after FadeOutDelay seconds. OnFinishedAction: If the decal component is set to fade out, it may delete itself or its entire owner object afterwards. ApplyToDynamic: By default, decals apply to static geometry but not to dynamic geometry . If it is desired for a decal to be projected onto a dynamic object, this property should be used to select exactly to which dynamic object the decal should be applied to. Note that decals can only be applied to a single dynamic game object. If the selected object turns out to be static , though, the decal will be invisible.","title":"Decal Component Properties"},{"location":"effects/decals/#see-also","text":"Particle Effects","title":"See Also"},{"location":"effects/fake-rope-component/","text":"Fake Rope Component The fake rope component is used to simulate simple cables, ropes and wires for decorative purposes. These ropes are not able to pull on another object and thus can't be used as a gameplay element. Use the rope component for such use cases. On the other hand, the fake rope component is more lightweight to simulate and is optimized to have very little overhead when it has reached a resting state (doesn't swing anymore). Therefore it can be used in larger quantities for decorative purposes. Setting Up a Rope A rope requires two anchor points between which it hangs. One anchor point is the rope object position itself, for the other one typically uses a dummy game object. The Anchor object reference is used to select which one to use. In the object hierarchy it typically looks like this: The position of the anchors can be moved in the 3D viewport to position the rope as desired. The shape of the simulated rope will be shown as a preview. Use the Slack property to make the rope sag. Run the scene to see the final shape and behavior. Rendering With just the rope simulation component, you won't be able to see the rope, at all. You also need to attach a rope render component to the same game object. Component Properties Anchor : A reference to an object whose position determines where the rope ends. AttachToOrigin , AttachToAnchor : Whether the rope is fixed at the origin or anchor location. If the rope is not attached at one or both ends it is free to move away from there. Pieces : How many individual pieces the rope is made up of. More pieces look prettier, but cost more performance and may decrease the simulation stability. Slack : How much slack the rope has. A value of zero means the rope is hung perfectly straight between its anchors. Positive values make the rope sag downwards. Damping : How quickly the rope loses energy while swinging. Higher values make the rope come to rest more quickly, low values make the rope swing for a long time. Once a rope comes to rest, it takes significantly less processing power. WindInfluence : How strongly wind should make the rope swing. Be aware that having many swinging ropes costs a lot of performance. See Also Jolt Rope Component Rope Render Component","title":"Fake Rope Component"},{"location":"effects/fake-rope-component/#fake-rope-component","text":"The fake rope component is used to simulate simple cables, ropes and wires for decorative purposes. These ropes are not able to pull on another object and thus can't be used as a gameplay element. Use the rope component for such use cases. On the other hand, the fake rope component is more lightweight to simulate and is optimized to have very little overhead when it has reached a resting state (doesn't swing anymore). Therefore it can be used in larger quantities for decorative purposes.","title":"Fake Rope Component"},{"location":"effects/fake-rope-component/#setting-up-a-rope","text":"A rope requires two anchor points between which it hangs. One anchor point is the rope object position itself, for the other one typically uses a dummy game object. The Anchor object reference is used to select which one to use. In the object hierarchy it typically looks like this: The position of the anchors can be moved in the 3D viewport to position the rope as desired. The shape of the simulated rope will be shown as a preview. Use the Slack property to make the rope sag. Run the scene to see the final shape and behavior.","title":"Setting Up a Rope"},{"location":"effects/fake-rope-component/#rendering","text":"With just the rope simulation component, you won't be able to see the rope, at all. You also need to attach a rope render component to the same game object.","title":"Rendering"},{"location":"effects/fake-rope-component/#component-properties","text":"Anchor : A reference to an object whose position determines where the rope ends. AttachToOrigin , AttachToAnchor : Whether the rope is fixed at the origin or anchor location. If the rope is not attached at one or both ends it is free to move away from there. Pieces : How many individual pieces the rope is made up of. More pieces look prettier, but cost more performance and may decrease the simulation stability. Slack : How much slack the rope has. A value of zero means the rope is hung perfectly straight between its anchors. Positive values make the rope sag downwards. Damping : How quickly the rope loses energy while swinging. Higher values make the rope come to rest more quickly, low values make the rope swing for a long time. Once a rope comes to rest, it takes significantly less processing power. WindInfluence : How strongly wind should make the rope swing. Be aware that having many swinging ropes costs a lot of performance.","title":"Component Properties"},{"location":"effects/fake-rope-component/#see-also","text":"Jolt Rope Component Rope Render Component","title":"See Also"},{"location":"effects/fog/","text":"Fog By default a scene doesn't have any fog. To enable fog, you need to place a game object in the scene and attach a fog component. Fog Component The fog component is used to apply simple depth-fog to the entire scene. The image below shows a scene without fog (left) with depth-fog (middle) and with an additional height-falloff (right): The rotation and scale of the game object has no effect on the fog. The position is mostly irrelevant, except for the z-coordinate, which is used as the threshold if HeightFalloff is used. Color: The overall color of the fog. Density: The density of the fog. The higher this value, the thicker the fog will be and thus it will also become noticeable at a closer distance. HeightFalloff: If set to zero, the fog is applied equally everywhere in the scene. Otherwise, the fog will only be applied to objects below the fog object. Thus, in this case, the position of the fog object defines which objects will be inside the fog and which are outside. The HeightFalloff value defines the distance over which the height fog transitions from foggy ground to clear sky. For example, a value of 1 means the fog changes from fully foggy to non-foggy over one meter and thus gives a relatively sharp transition. A value of 10 results in a much larger and softer transition. See Also Sky Lighting Particle Effects","title":"Fog"},{"location":"effects/fog/#fog","text":"By default a scene doesn't have any fog. To enable fog, you need to place a game object in the scene and attach a fog component.","title":"Fog"},{"location":"effects/fog/#fog-component","text":"The fog component is used to apply simple depth-fog to the entire scene. The image below shows a scene without fog (left) with depth-fog (middle) and with an additional height-falloff (right): The rotation and scale of the game object has no effect on the fog. The position is mostly irrelevant, except for the z-coordinate, which is used as the threshold if HeightFalloff is used. Color: The overall color of the fog. Density: The density of the fog. The higher this value, the thicker the fog will be and thus it will also become noticeable at a closer distance. HeightFalloff: If set to zero, the fog is applied equally everywhere in the scene. Otherwise, the fog will only be applied to objects below the fog object. Thus, in this case, the position of the fog object defines which objects will be inside the fog and which are outside. The HeightFalloff value defines the distance over which the height fog transitions from foggy ground to clear sky. For example, a value of 1 means the fog changes from fully foggy to non-foggy over one meter and thus gives a relatively sharp transition. A value of 10 results in a much larger and softer transition.","title":"Fog Component"},{"location":"effects/fog/#see-also","text":"Sky Lighting Particle Effects","title":"See Also"},{"location":"effects/rope-render-component/","text":"Rope Render Component The rope render component is used to render a rope or cable. ![[fake-rope-config.jpg]] The rope simulation is done by other components, such as the rope component or the fake rope component . The rope render component has to be attached to the same object as the simulation component. Component Properties Material : The material to use for rendering. Color : The object color. This is typically multiplied into the diffuse part of the material, but the shader may use the color in different ways, as well. Thickness : The thickness of the rope mesh. Detail : How many polygons the rope mesh uses to appear round. More detail costs more rendering performance. Note that this has no effect on how many segments the rope is made up of, that is a property of the simulation component. Subdivide : Whether the rope mesh should have an extra segment subdivision to make it look smoother at strong bends. This doubles the amount of triangles in the mesh. Enable this for ropes that still should look as good as possible even under strong curvature. UScale : The texture is always wrapped exactly once around the rope. Howver, along the rope's length, this option defines how often it will be repeated. See Also Jolt Rope Component Fake Rope Component","title":"Rope Render Component"},{"location":"effects/rope-render-component/#rope-render-component","text":"The rope render component is used to render a rope or cable. ![[fake-rope-config.jpg]] The rope simulation is done by other components, such as the rope component or the fake rope component . The rope render component has to be attached to the same object as the simulation component.","title":"Rope Render Component"},{"location":"effects/rope-render-component/#component-properties","text":"Material : The material to use for rendering. Color : The object color. This is typically multiplied into the diffuse part of the material, but the shader may use the color in different ways, as well. Thickness : The thickness of the rope mesh. Detail : How many polygons the rope mesh uses to appear round. More detail costs more rendering performance. Note that this has no effect on how many segments the rope is made up of, that is a property of the simulation component. Subdivide : Whether the rope mesh should have an extra segment subdivision to make it look smoother at strong bends. This doubles the amount of triangles in the mesh. Enable this for ropes that still should look as good as possible even under strong curvature. UScale : The texture is always wrapped exactly once around the rope. Howver, along the rope's length, this option defines how often it will be repeated.","title":"Component Properties"},{"location":"effects/rope-render-component/#see-also","text":"Jolt Rope Component Fake Rope Component","title":"See Also"},{"location":"effects/simple-wind-component/","text":"Simple Wind Component The simple wind component implements a very basic wind system . By placing a component of this type in a scene, the simple wind world module is created. Things that can react to wind are then able to retrieve a wind value at any location. The simple wind system provides one global wind value, which changes randomly. Additionally, it supports wind volume components . If any such shape is in a scene, its contribution is added to the global wind value, for objects that are inside such a volume. Component Properties MinWindStrength , MaxWindStrength : The minimum and maximum strength with which the wind shall blow. A random value in between will be chosen every couple of seconds. To make it easier to get different things working well with each other, the wind values are hard-coded. They are inspired by the Beaufort scale . Most things that can react to wind (for example particle effects or ropes ) also have a wind influence parameter for tweaking how strongly they react to wind. The Beaufort scale enables you to get an idea what reaction to expect, e.g. you know how an effect should look under a light breple or under storm conditions, and can then tweak the wind influence value accordingly. MaxDeviation : How much the wind direction may deviate from the local x-axis. Set this to the maximum value, if the wind is allowed to come from anywhere. See Also Wind Wind Volume Components","title":"Simple Wind Component"},{"location":"effects/simple-wind-component/#simple-wind-component","text":"The simple wind component implements a very basic wind system . By placing a component of this type in a scene, the simple wind world module is created. Things that can react to wind are then able to retrieve a wind value at any location. The simple wind system provides one global wind value, which changes randomly. Additionally, it supports wind volume components . If any such shape is in a scene, its contribution is added to the global wind value, for objects that are inside such a volume.","title":"Simple Wind Component"},{"location":"effects/simple-wind-component/#component-properties","text":"MinWindStrength , MaxWindStrength : The minimum and maximum strength with which the wind shall blow. A random value in between will be chosen every couple of seconds. To make it easier to get different things working well with each other, the wind values are hard-coded. They are inspired by the Beaufort scale . Most things that can react to wind (for example particle effects or ropes ) also have a wind influence parameter for tweaking how strongly they react to wind. The Beaufort scale enables you to get an idea what reaction to expect, e.g. you know how an effect should look under a light breple or under storm conditions, and can then tweak the wind influence value accordingly. MaxDeviation : How much the wind direction may deviate from the local x-axis. Set this to the maximum value, if the wind is allowed to come from anywhere.","title":"Component Properties"},{"location":"effects/simple-wind-component/#see-also","text":"Wind Wind Volume Components","title":"See Also"},{"location":"effects/sky/","text":"Sky By default the background of a rendered scene is black. To change this, you need to create a game object in the scene and attach a sky component. SkyBox component The SkyBox component implements a simply sky, which displays a cubemap texture as a static background. The position and scale of the game object has no effect on the sky, it will always appear behind all other geometry. The rotation, however, can be used to orient the sky as desired. CubeMap : The cubemap texture asset to use. ExposureBias : This specifies how bright the sky will appear. A higher value results in a brighter sky. InverseTonemap : Switches the tonemapping mode. For HDR skyboxes this should stay off. For skyboxes that do not have high-dynamic range values, enabling this mode will improve brightness and contrast of the colors. UseFog, VirtualDistance : If enabled, fog will be applied to the sky. In that case VirtualDistance is being used to compute how foggy the sky should appear. See Also Fog Textures Lighting","title":"Sky"},{"location":"effects/sky/#sky","text":"By default the background of a rendered scene is black. To change this, you need to create a game object in the scene and attach a sky component.","title":"Sky"},{"location":"effects/sky/#skybox-component","text":"The SkyBox component implements a simply sky, which displays a cubemap texture as a static background. The position and scale of the game object has no effect on the sky, it will always appear behind all other geometry. The rotation, however, can be used to orient the sky as desired. CubeMap : The cubemap texture asset to use. ExposureBias : This specifies how bright the sky will appear. A higher value results in a brighter sky. InverseTonemap : Switches the tonemapping mode. For HDR skyboxes this should stay off. For skyboxes that do not have high-dynamic range values, enabling this mode will improve brightness and contrast of the colors. UseFog, VirtualDistance : If enabled, fog will be applied to the sky. In that case VirtualDistance is being used to compute how foggy the sky should appear.","title":"SkyBox component"},{"location":"effects/sky/#see-also","text":"Fog Textures Lighting","title":"See Also"},{"location":"effects/wind-volume-components/","text":"Wind Volume Components Wind volume components are used to define areas in a scene where wind should blow in a certain way. Note: On their own these components won't have any effect in a scene. A wind system additionally has to be set up. Placing a simple wind component in a scene creates a basic wind system. Shared Component Properties All wind volume components have these properties: Strength : The strength with which the wind shall blow. ReverseDirection : If set, the wind direction is reversed. This can be used to pull things inwards, instead of pushing them. BurstDuration : If this is set to zero, the wind blows continuously. Otherwise it blows for the specified amount of time and then stops. OnFinishedAction : If BurstDuration is non-zero, the component will deactivate itself once the burst is done. Additionally, the component may delete itself or the entire object. Note that if None is selected, the wind burst can be restarted, simply by reactivating the component. Sphere Wind Volume Properties Radius : The radius of the wind volume. Cylinder Wind Volume Properties Length , Radius : The size of the cylinder. Mode : How the wind force is computed. Directional : The wind force is along the main cylinder axis. Vortex : The wind whirls around the cylinder's axis. See the video below. Cone Wind Volume Properties Angle , Length : Angle and length of the cone shape. See Also Wind Simple Wind Component","title":"Wind Volume Components"},{"location":"effects/wind-volume-components/#wind-volume-components","text":"Wind volume components are used to define areas in a scene where wind should blow in a certain way. Note: On their own these components won't have any effect in a scene. A wind system additionally has to be set up. Placing a simple wind component in a scene creates a basic wind system.","title":"Wind Volume Components"},{"location":"effects/wind-volume-components/#shared-component-properties","text":"All wind volume components have these properties: Strength : The strength with which the wind shall blow. ReverseDirection : If set, the wind direction is reversed. This can be used to pull things inwards, instead of pushing them. BurstDuration : If this is set to zero, the wind blows continuously. Otherwise it blows for the specified amount of time and then stops. OnFinishedAction : If BurstDuration is non-zero, the component will deactivate itself once the burst is done. Additionally, the component may delete itself or the entire object. Note that if None is selected, the wind burst can be restarted, simply by reactivating the component.","title":"Shared Component Properties"},{"location":"effects/wind-volume-components/#sphere-wind-volume-properties","text":"Radius : The radius of the wind volume.","title":"Sphere Wind Volume Properties"},{"location":"effects/wind-volume-components/#cylinder-wind-volume-properties","text":"Length , Radius : The size of the cylinder. Mode : How the wind force is computed. Directional : The wind force is along the main cylinder axis. Vortex : The wind whirls around the cylinder's axis. See the video below.","title":"Cylinder Wind Volume Properties"},{"location":"effects/wind-volume-components/#cone-wind-volume-properties","text":"Angle , Length : Angle and length of the cone shape.","title":"Cone Wind Volume Properties"},{"location":"effects/wind-volume-components/#see-also","text":"Wind Simple Wind Component","title":"See Also"},{"location":"effects/wind/","text":"Wind Some components can be animated by wind. For instance particle effects and ropes will react to wind. Usually these animations are for decorative purposes. Wind is implemented as a world module . Thus, it is possible to have different wind system implementations, and choose the most suitable for each scene. For example, one system may do a full volumetric fluid simulation, whereas another does not. You instantiate a specific wind system by adding the respective component to a scene. At this time, Plasma only ships with a basic implementation. You instantiate it with the simple wind component . As long as there is no such component in a scene, there won't be any wind. Querying Wind Values At runtime you query the wind value by location. First you need to retrieve the wind world module: const plWindWorldModuleInterface* pWindInterface = GetWorld()->GetModuleReadOnly<plWindWorldModuleInterface>(); Make sure to check the pointer for nullptr , which happens when there is no wind system set up for a scene. Then the wind can be queried by location: plVec3 wind = pWindInterface->GetWindAt(position); This returns a vector with the direction and strength of the wind at the queried position. To react properly to wind, this value must be polled every frame. However, be careful to query only few values. Depending on the active system, this can be a very fast or a rather slow operation. However, usually wind doesn't change drastically within short distances. Note: The wind system returns a vector of wind direction and strength. This alone often does not yield a convincing wind effect though. For example a tree or a piece of cloth would only be pushed to one side, but that looks very unnatural. Instead objects should flutter in the wind, e.g. wildly swing up and down or sideways. Such behavior is very object specific and must be implemented on top of the general wind value. The utility function plWindWorldModuleInterface::ComputeWindFlutter() might be sufficient to get you started. Controlling Wind To add wind locally, have a look at the wind volume components . These can be used both for static wind fields, for example to make a flag blow in the wind nicely, as well as for short lived dynamic effects, such as the shockwave of an explosion. Affecting Physics Objects Be aware that wind does not affect any physics objects . Such behavior could be implemented, but it would be difficult to not have a serious performance impact, since it would keep the physics engine constantly busy (usually objects go to sleep when no forces act upon them, but wind would be a constantly active force). Instead, explosions and such rather use a physics shape query to determine objects in range, and then apply a short impulse to only those objects once. See the area damage component as an example. Custom Wind Systems It is possible to write your own wind system. Just implement a new world module , derive it from plWindWorldModuleInterface and override the GetWindAt() function. Put your code into a custom engine plugin and also add a custom component type to instantiate your wind world module, and make it configurable. For inspiration, just have a look at plSimpleWindWorldModule and plSimpleWindComponent . See Also Wind Volume Components Simple Wind Component","title":"Wind"},{"location":"effects/wind/#wind","text":"Some components can be animated by wind. For instance particle effects and ropes will react to wind. Usually these animations are for decorative purposes. Wind is implemented as a world module . Thus, it is possible to have different wind system implementations, and choose the most suitable for each scene. For example, one system may do a full volumetric fluid simulation, whereas another does not. You instantiate a specific wind system by adding the respective component to a scene. At this time, Plasma only ships with a basic implementation. You instantiate it with the simple wind component . As long as there is no such component in a scene, there won't be any wind.","title":"Wind"},{"location":"effects/wind/#querying-wind-values","text":"At runtime you query the wind value by location. First you need to retrieve the wind world module: const plWindWorldModuleInterface* pWindInterface = GetWorld()->GetModuleReadOnly<plWindWorldModuleInterface>(); Make sure to check the pointer for nullptr , which happens when there is no wind system set up for a scene. Then the wind can be queried by location: plVec3 wind = pWindInterface->GetWindAt(position); This returns a vector with the direction and strength of the wind at the queried position. To react properly to wind, this value must be polled every frame. However, be careful to query only few values. Depending on the active system, this can be a very fast or a rather slow operation. However, usually wind doesn't change drastically within short distances. Note: The wind system returns a vector of wind direction and strength. This alone often does not yield a convincing wind effect though. For example a tree or a piece of cloth would only be pushed to one side, but that looks very unnatural. Instead objects should flutter in the wind, e.g. wildly swing up and down or sideways. Such behavior is very object specific and must be implemented on top of the general wind value. The utility function plWindWorldModuleInterface::ComputeWindFlutter() might be sufficient to get you started.","title":"Querying Wind Values"},{"location":"effects/wind/#controlling-wind","text":"To add wind locally, have a look at the wind volume components . These can be used both for static wind fields, for example to make a flag blow in the wind nicely, as well as for short lived dynamic effects, such as the shockwave of an explosion.","title":"Controlling Wind"},{"location":"effects/wind/#affecting-physics-objects","text":"Be aware that wind does not affect any physics objects . Such behavior could be implemented, but it would be difficult to not have a serious performance impact, since it would keep the physics engine constantly busy (usually objects go to sleep when no forces act upon them, but wind would be a constantly active force). Instead, explosions and such rather use a physics shape query to determine objects in range, and then apply a short impulse to only those objects once. See the area damage component as an example.","title":"Affecting Physics Objects"},{"location":"effects/wind/#custom-wind-systems","text":"It is possible to write your own wind system. Just implement a new world module , derive it from plWindWorldModuleInterface and override the GetWindAt() function. Put your code into a custom engine plugin and also add a custom component type to instantiate your wind world module, and make it configurable. For inspiration, just have a look at plSimpleWindWorldModule and plSimpleWindComponent .","title":"Custom Wind Systems"},{"location":"effects/wind/#see-also","text":"Wind Volume Components Simple Wind Component","title":"See Also"},{"location":"effects/particle-effects/how-particle-effects-work/","text":"How Particle Effects Work This article gives a broad introduction how particle effects work. It is meant for people completely new to this topic. The information here is not very engine specific, as particle effects conceptually work the same in all engines. Particles A particle is the smallest unit that a particle effect is made up of. Each particle has a small number of properties. Every particle has a position, a duration how long it lives, and typically also velocity (speed and direction), size and color. It may have additional properties, when needed, but those are the most common ones. In a particle effect we often have hundreds, sometimes even thousands of particles. Each particle is small, but by having hundreds of particles scattered throughout a volume, the end result is something that looks volumetric and behaves in complex patterns. There a multiple ways a single particle may get rendered. The most common method are so called billboards . A billboard is a quad which always rotates such that it faces the viewer. By using a texture with a circular image (for instance a flare) and making the quad transparent, the particle will appear volumetric (it appears circular from all directions), although it is just rendered with a simple polygon that is a flat plane. The reason this is the preferred method for rendering particles is that it is otherwise quite difficult to render volumetric, transparent objects with triangles (the only thing GPUs can render). Billboards are an effective illusion. Particles can represent other things, as well, for example small meshes, light sources or even sounds, but billboards are by far the most common. A large part of building a particle effect is about configuring how the different properties of all those particles evolve over time. For example, if you configure particles to have a lifespan of two seconds, rise up with medium speed, change their color from red to yellow and fade out shortly before they die, then you get an effect that looks a lot like fire. Particle Systems and Effects You never work or configure individual particles. Instead, you mostly work with particle systems . A particle system represents a large amount of particles that all behave according to the same rules. A complete particle effect often consists of multiple particle systems, but always at least one. Each particle system defines different rules how the particles of that system behave. So in the fire example, you would have one particle system which is configured to spawn five particles every tenth of a second. All these particles are rendered as billboards, use the same flare texture, rise up, and change their color according to some fire gradient over their lifetime. This system represents the flames. To add smoke above the flame, you would add a second particle system, which may only spawn one particle every tenth of a second, use a smoke-like texture, rise up more slowly and start with a zero size at the beginning, slowly growing larger and larger, such that it becomes visible just above the flame. The flame particles and the smoke particles have no relation, whatsoever, but together they form a better illusion of fire. Evolving Properties The code that updates a particle system mostly handles every property of the particles in isolation. And that is also how you need to think about each property, when you want to create an effect. The way a property, such as position, changes, is called the behavior . So for example, a particle may rise up slowly, or it may fall down according to gravity. Whether the position behaves one way or the other results in a drastically different effect. The same is true for all other properties. A particle may have a constant size or it may start small but grow over time. Its color may be just white, or some random blue-ish tone or it may change its color such that it appears to be burning up or fading out. Every property has its own rule, how it behaves. Put it all together and you can build an infinite amount of different effects. Building Blocks What the particle editor presents to you, are a number of building blocks that you choose and configure. For example, there are a few behaviors how the position of a particle should be calculated. There are a few building blocks for determining a particle's color, its size, and how to render it. Many behaviors are mutually exclusive. So if you already chose the \"gravity\" building block, which lets particles fall down, then you can't choose a second behavior that also affects particle positions. Most building blocks expose options for you to tweak. For example the \"gravity\" behavior allows you to tweak the strength of the applied gravity. To create an effect, you create multiple particle systems, and for each one you select and configure the desired behaviors. Spawning and Lifetime We already mentioned that particles have a limited (usually very short) lifespan. Generally you can separate particle effects into two types: short one shot type of effects, and long lasting or even unending continuous effects. Explosions, water splashes and bullet impacts are all of the former type. They typically spawn all of their particles in one big burst. Those particles live for a second or so and then the effect is over. Fire, smoke and mist are of the latter type. Those effects typically spawn particles continuously. Each particle lives for several seconds and by the time it dies, many other particles have already been spawned to take its place. Continuous effects can be configured such that they are not endless, for example a smoke effect may stop by itself after 10 seconds. However, often it is more convenient to build such effects as endless effects, that never stop, and then use custom (script) code to make a particle effect stop spawning new particles at the desired time. This way the desired duration of an effect can be dynamically adjusted. Whether an effect acts one way or the other, is determined by the selected type of emitter . The emitter building block specifies how often and how many new particles get spawned. Emitters can be configured to be smooth or erratic, doing short bursts or long intervals. Most of the time one uses one of two emitter types, but there are also emitters that only spawn particles when some event happens, which can be used for even more complex effects. How long a particle will live is decided randomly when it gets spawned (within a range). How much time a particle has left to live, is often used to look up other properties. For example the color of a particle often depends on its lifetime, such that particles will fade out towards their end. See Also Particle Effects","title":"How Particle Effects Work"},{"location":"effects/particle-effects/how-particle-effects-work/#how-particle-effects-work","text":"This article gives a broad introduction how particle effects work. It is meant for people completely new to this topic. The information here is not very engine specific, as particle effects conceptually work the same in all engines.","title":"How Particle Effects Work"},{"location":"effects/particle-effects/how-particle-effects-work/#particles","text":"A particle is the smallest unit that a particle effect is made up of. Each particle has a small number of properties. Every particle has a position, a duration how long it lives, and typically also velocity (speed and direction), size and color. It may have additional properties, when needed, but those are the most common ones. In a particle effect we often have hundreds, sometimes even thousands of particles. Each particle is small, but by having hundreds of particles scattered throughout a volume, the end result is something that looks volumetric and behaves in complex patterns. There a multiple ways a single particle may get rendered. The most common method are so called billboards . A billboard is a quad which always rotates such that it faces the viewer. By using a texture with a circular image (for instance a flare) and making the quad transparent, the particle will appear volumetric (it appears circular from all directions), although it is just rendered with a simple polygon that is a flat plane. The reason this is the preferred method for rendering particles is that it is otherwise quite difficult to render volumetric, transparent objects with triangles (the only thing GPUs can render). Billboards are an effective illusion. Particles can represent other things, as well, for example small meshes, light sources or even sounds, but billboards are by far the most common. A large part of building a particle effect is about configuring how the different properties of all those particles evolve over time. For example, if you configure particles to have a lifespan of two seconds, rise up with medium speed, change their color from red to yellow and fade out shortly before they die, then you get an effect that looks a lot like fire.","title":"Particles"},{"location":"effects/particle-effects/how-particle-effects-work/#particle-systems-and-effects","text":"You never work or configure individual particles. Instead, you mostly work with particle systems . A particle system represents a large amount of particles that all behave according to the same rules. A complete particle effect often consists of multiple particle systems, but always at least one. Each particle system defines different rules how the particles of that system behave. So in the fire example, you would have one particle system which is configured to spawn five particles every tenth of a second. All these particles are rendered as billboards, use the same flare texture, rise up, and change their color according to some fire gradient over their lifetime. This system represents the flames. To add smoke above the flame, you would add a second particle system, which may only spawn one particle every tenth of a second, use a smoke-like texture, rise up more slowly and start with a zero size at the beginning, slowly growing larger and larger, such that it becomes visible just above the flame. The flame particles and the smoke particles have no relation, whatsoever, but together they form a better illusion of fire.","title":"Particle Systems and Effects"},{"location":"effects/particle-effects/how-particle-effects-work/#evolving-properties","text":"The code that updates a particle system mostly handles every property of the particles in isolation. And that is also how you need to think about each property, when you want to create an effect. The way a property, such as position, changes, is called the behavior . So for example, a particle may rise up slowly, or it may fall down according to gravity. Whether the position behaves one way or the other results in a drastically different effect. The same is true for all other properties. A particle may have a constant size or it may start small but grow over time. Its color may be just white, or some random blue-ish tone or it may change its color such that it appears to be burning up or fading out. Every property has its own rule, how it behaves. Put it all together and you can build an infinite amount of different effects.","title":"Evolving Properties"},{"location":"effects/particle-effects/how-particle-effects-work/#building-blocks","text":"What the particle editor presents to you, are a number of building blocks that you choose and configure. For example, there are a few behaviors how the position of a particle should be calculated. There are a few building blocks for determining a particle's color, its size, and how to render it. Many behaviors are mutually exclusive. So if you already chose the \"gravity\" building block, which lets particles fall down, then you can't choose a second behavior that also affects particle positions. Most building blocks expose options for you to tweak. For example the \"gravity\" behavior allows you to tweak the strength of the applied gravity. To create an effect, you create multiple particle systems, and for each one you select and configure the desired behaviors.","title":"Building Blocks"},{"location":"effects/particle-effects/how-particle-effects-work/#spawning-and-lifetime","text":"We already mentioned that particles have a limited (usually very short) lifespan. Generally you can separate particle effects into two types: short one shot type of effects, and long lasting or even unending continuous effects. Explosions, water splashes and bullet impacts are all of the former type. They typically spawn all of their particles in one big burst. Those particles live for a second or so and then the effect is over. Fire, smoke and mist are of the latter type. Those effects typically spawn particles continuously. Each particle lives for several seconds and by the time it dies, many other particles have already been spawned to take its place. Continuous effects can be configured such that they are not endless, for example a smoke effect may stop by itself after 10 seconds. However, often it is more convenient to build such effects as endless effects, that never stop, and then use custom (script) code to make a particle effect stop spawning new particles at the desired time. This way the desired duration of an effect can be dynamically adjusted. Whether an effect acts one way or the other, is determined by the selected type of emitter . The emitter building block specifies how often and how many new particles get spawned. Emitters can be configured to be smooth or erratic, doing short bursts or long intervals. Most of the time one uses one of two emitter types, but there are also emitters that only spawn particles when some event happens, which can be used for even more complex effects. How long a particle will live is decided randomly when it gets spawned (within a range). How much time a particle has left to live, is often used to look up other properties. For example the color of a particle often depends on its lifetime, such that particles will fade out towards their end.","title":"Spawning and Lifetime"},{"location":"effects/particle-effects/how-particle-effects-work/#see-also","text":"Particle Effects","title":"See Also"},{"location":"effects/particle-effects/particle-behaviors/","text":"Particle Behaviors This page lists and describes all particle behaviors . Bounds Behavior This behavior can be used for atmospheric effects that should be centered around the player, such as rain, snow or mist. The bounds behavior specifies an area in which particles are allowed. When the player moves, and thus the particle effect is moved to a new location, particles would usually stay behind although not being needed anymore. The bounds behavior can make sure to delete those particles. For some effects it is also vital to fill up the new space quickly. This can be achieved with a very high rate of spawning new particles, though this is often not feasible for atmospheric effects. Instead, the bounds behavior can also just teleport the particles that were left behind, to the new area. PositionOffset, Extents: These values define the size and position of the box, relative to the origin of the particle system. With a position offset of (0, 0, 0) , the box will be centered around the system's origin. OutOfBoundsMode: Defines what happens for particles that leave the bounding area. Die: Particles outside the area will be killed right away. Teleport: Particles leaving one side of the bounding box will be teleported to the other end of the box. This allows the effect to keep a constant density of particles and is therefore useful for effects that should happen around a player, without being simulated completely in the local space of the player, which would prevent things like using the raycast behavior . Instead, particles can simulate in global space, and only be teleported on demand. Be aware that this teleportation can still break the effect in various ways, because only the position and last position of each particle is relocated. Behaviors and particle renderers that use additional positional data may not work well with this. For example, the trail renderer's position history is not relocated and therefore trails will suddenly stretch through the entire bounding area after a relocation. Similarly, an effect that uses the raycast behavior to prevent tunneling through geometry, may be able to tunnel through walls, if it is being relocated from an unobstructed area to a position where it should not have been able to get to without the teleportation. Color Gradient Behavior This behavior changes a particle's color during the update step. A color gradient is used as the color source, and a mode specifies how to look up the color from the gradient. Gradient: The color gradient to use as the source. TintColor: An additional color to be multiplied into the gradient, for tweaking the final result. ColorFrom: This mode specifies how the color is looked up from the gradient: Age - In this mode the particle's color depends on its age and remaining lifetime. That means it starts out with the leftmost color from the gradient and transitions towards the rightmost color. Optimally, the color gradient should include alpha values, such that the particles can fade out towards the end. Speed - In this mode the particle's color is determined from its current speed. Slow particles are assigned colors from the left side of the gradient, fast particles that from the right side. This mode only makes sense when either every particle gets a random speed assigned, or when its speed is able to change over time, due to friction, gravity or other factors. MaxSpeed: When using ColorFrom = Speed , this value specifies the maximum expected speed of any particle. That speed is then mapped to the rightmost side of the color gradient. ![[color-gradient.gif]] Fade Out Behavior This behavior changes a particle's alpha value to gradually fade out over its lifetime. This behavior can also be achieved using a color gradient behavior , however, the fade out behavior is easier to set up and more efficient at runtime. StartAlpha: The alpha value to begin with when the particle has just spawned. Exponent: How quickly to fade the alpha value from StartAlpha towards 0 over the particle's lifespan. An exponent of 1 results in a linear fade. An exponent of 2 will make it fade out much earlier, a value of 0.5 will make it fade out very slowly at first and then quite abruptly at the end. Flies Behavior This behavior moves particles around the emitter center in erratic patterns, similar to a swarm of flies circling something. FlySpeed: The speed with which the particles move. PathLength: The distance that the particles move into some direction before making another turn. The shorter this is, the more often the particles can change direction and thus the smoother the motion becomes. They will also clump up more and stay within the MaxEmitterDistance , if the particles can correct their course more often. With a long PathLength they may spread out more. MaxEmitterDistance: The maximum distance that the particles will fly away from the effect's center before turning back. If they travel further, they will always steer back towards the emitter. How quickly that is possible though, depends on PathLength and MaxSteeringAngle . MaxSteeringAngle: Every time a particle has traveled a distance of PathLength , it will make a random turn. This value specifies how large that turn may be. A small value results in very slow and wide turns, whereas a large value results in quick and erratic behavior. ![[flies.gif]] Gravity Behavior This behavior lets particles fall downwards. GravityFactor: Scales gravity before applying it to the particles' velocity. ![[gravity.gif]] Pull Along Behavior Typically once a particle has been spawned, its position is unaffected by changes to the particle effect position. That means when an effect moves around quickly, it may leave a trail of particles behind it, but that trail will be very choppy, unless you have an extremely high particle spawn count and frequency. Thus making something like a rocket exhaust look convincing for a fast moving object can be difficult. The pull along behavior helps to solve this problem by keeping track of any position changes of the particle effect node and applying a fraction of those movements to all the particles' positions as well. This way, if the effect moves a meter, all particles may move 0.8 meters as well. One typically only applies a fraction, such that when the effect moves fast, the particles will be stretched long behind it and not move in perfect unison with the effect node, yielding a more convincing effect. Strength: How much of the effect node's movement should be carried over to the particle positions. The video below shows two effects beside each other. The left one does not use the pull along behavior, the right one does. As can be seen, the particles on the right stay closer to the moving emitter position. ![[pull-along-behavior.gif]] Raycast Behavior This behavior uses raycasts to detect collisions along the trajectory of a particle. If a particle would collide with geometry, the behavior can either adjust the its velocity, or terminate the particle early, potentially raising an event , which could in turn lead to other effects or being spawned. Reaction: Specifies how the particle should react to a collision. Bounce: The particle's velocity will be adjusted such that it bounces off the hit surface. Die: The particle will be killed early. Stop: The particle's current velocity will be set to zero, thus stopping it in its tracks. If other position affecting behaviors are active, for example the gravity behavior , it will start moving again, but without its previous momentum. BounceFactor: How much of the current speed should be preserved after the bounce. CollisionLayer: The physics collision layer to use. Affects with which geometry the particle will collide and which it will pass through. OnCollideEvent: An optional name of an event to raise. If set, other effects or prefabs can be spawned at the location of impact. ![[raycast.gif]] Size Curve Behavior This behavior changes a particle's size over the course of its lifetime. SizeCurve: A curve which is used to look up the size of the particle. The current fraction of the particle's lifespan is used for the lookup along the X axis. The absolute X and Y values in the curve don't matter, the curve is normalized to [0; 1] range. BaseSize: The particles will always have at least this size, the rest is added on top. CurveScale: Specifies what value the largest value in the curve maps to. That means at the peak of a curve, the particle's size will be BaseSize + CurveScale . ![[size-curve.gif]] Velocity Behavior This behavior affects particle position and velocity. It can be used to gradually dampen the starting velocity through 'friction' and it may apply a constant upwards movement. If a scene contains wind , this behavior can also apply a fraction of the wind force to the particle's position. RiseSpeed: If non-zero, the particles will move upwards with at least this constant speed. This is added to the particle position independent from its velocity, so if the current velocity points downward, the two may cancel each other out. Friction: This value imitates air friction. If it is non-zero, the particle's velocity will be dampened over time. The value's range is [0; infinity] . To achieve an effect as in the animation below, the particles must have a very large starting velocity (here: 10). The friction here is set to 6. This way the particles will appear to be quite fast, but will also get slowed down almost to a standstill within a fraction of a second. WindInfluence: If the scene has wind, this value specifies how much the wind should be able to push the particles around. ![[velocity.gif]] See Also Particle Effects Particle Initializers Particle Renderers","title":"Particle Behaviors"},{"location":"effects/particle-effects/particle-behaviors/#particle-behaviors","text":"This page lists and describes all particle behaviors .","title":"Particle Behaviors"},{"location":"effects/particle-effects/particle-behaviors/#bounds-behavior","text":"This behavior can be used for atmospheric effects that should be centered around the player, such as rain, snow or mist. The bounds behavior specifies an area in which particles are allowed. When the player moves, and thus the particle effect is moved to a new location, particles would usually stay behind although not being needed anymore. The bounds behavior can make sure to delete those particles. For some effects it is also vital to fill up the new space quickly. This can be achieved with a very high rate of spawning new particles, though this is often not feasible for atmospheric effects. Instead, the bounds behavior can also just teleport the particles that were left behind, to the new area. PositionOffset, Extents: These values define the size and position of the box, relative to the origin of the particle system. With a position offset of (0, 0, 0) , the box will be centered around the system's origin. OutOfBoundsMode: Defines what happens for particles that leave the bounding area. Die: Particles outside the area will be killed right away. Teleport: Particles leaving one side of the bounding box will be teleported to the other end of the box. This allows the effect to keep a constant density of particles and is therefore useful for effects that should happen around a player, without being simulated completely in the local space of the player, which would prevent things like using the raycast behavior . Instead, particles can simulate in global space, and only be teleported on demand. Be aware that this teleportation can still break the effect in various ways, because only the position and last position of each particle is relocated. Behaviors and particle renderers that use additional positional data may not work well with this. For example, the trail renderer's position history is not relocated and therefore trails will suddenly stretch through the entire bounding area after a relocation. Similarly, an effect that uses the raycast behavior to prevent tunneling through geometry, may be able to tunnel through walls, if it is being relocated from an unobstructed area to a position where it should not have been able to get to without the teleportation.","title":"Bounds Behavior"},{"location":"effects/particle-effects/particle-behaviors/#color-gradient-behavior","text":"This behavior changes a particle's color during the update step. A color gradient is used as the color source, and a mode specifies how to look up the color from the gradient. Gradient: The color gradient to use as the source. TintColor: An additional color to be multiplied into the gradient, for tweaking the final result. ColorFrom: This mode specifies how the color is looked up from the gradient: Age - In this mode the particle's color depends on its age and remaining lifetime. That means it starts out with the leftmost color from the gradient and transitions towards the rightmost color. Optimally, the color gradient should include alpha values, such that the particles can fade out towards the end. Speed - In this mode the particle's color is determined from its current speed. Slow particles are assigned colors from the left side of the gradient, fast particles that from the right side. This mode only makes sense when either every particle gets a random speed assigned, or when its speed is able to change over time, due to friction, gravity or other factors. MaxSpeed: When using ColorFrom = Speed , this value specifies the maximum expected speed of any particle. That speed is then mapped to the rightmost side of the color gradient. ![[color-gradient.gif]]","title":"Color Gradient Behavior"},{"location":"effects/particle-effects/particle-behaviors/#fade-out-behavior","text":"This behavior changes a particle's alpha value to gradually fade out over its lifetime. This behavior can also be achieved using a color gradient behavior , however, the fade out behavior is easier to set up and more efficient at runtime. StartAlpha: The alpha value to begin with when the particle has just spawned. Exponent: How quickly to fade the alpha value from StartAlpha towards 0 over the particle's lifespan. An exponent of 1 results in a linear fade. An exponent of 2 will make it fade out much earlier, a value of 0.5 will make it fade out very slowly at first and then quite abruptly at the end.","title":"Fade Out Behavior"},{"location":"effects/particle-effects/particle-behaviors/#flies-behavior","text":"This behavior moves particles around the emitter center in erratic patterns, similar to a swarm of flies circling something. FlySpeed: The speed with which the particles move. PathLength: The distance that the particles move into some direction before making another turn. The shorter this is, the more often the particles can change direction and thus the smoother the motion becomes. They will also clump up more and stay within the MaxEmitterDistance , if the particles can correct their course more often. With a long PathLength they may spread out more. MaxEmitterDistance: The maximum distance that the particles will fly away from the effect's center before turning back. If they travel further, they will always steer back towards the emitter. How quickly that is possible though, depends on PathLength and MaxSteeringAngle . MaxSteeringAngle: Every time a particle has traveled a distance of PathLength , it will make a random turn. This value specifies how large that turn may be. A small value results in very slow and wide turns, whereas a large value results in quick and erratic behavior. ![[flies.gif]]","title":"Flies Behavior"},{"location":"effects/particle-effects/particle-behaviors/#gravity-behavior","text":"This behavior lets particles fall downwards. GravityFactor: Scales gravity before applying it to the particles' velocity. ![[gravity.gif]]","title":"Gravity Behavior"},{"location":"effects/particle-effects/particle-behaviors/#pull-along-behavior","text":"Typically once a particle has been spawned, its position is unaffected by changes to the particle effect position. That means when an effect moves around quickly, it may leave a trail of particles behind it, but that trail will be very choppy, unless you have an extremely high particle spawn count and frequency. Thus making something like a rocket exhaust look convincing for a fast moving object can be difficult. The pull along behavior helps to solve this problem by keeping track of any position changes of the particle effect node and applying a fraction of those movements to all the particles' positions as well. This way, if the effect moves a meter, all particles may move 0.8 meters as well. One typically only applies a fraction, such that when the effect moves fast, the particles will be stretched long behind it and not move in perfect unison with the effect node, yielding a more convincing effect. Strength: How much of the effect node's movement should be carried over to the particle positions. The video below shows two effects beside each other. The left one does not use the pull along behavior, the right one does. As can be seen, the particles on the right stay closer to the moving emitter position. ![[pull-along-behavior.gif]]","title":"Pull Along Behavior"},{"location":"effects/particle-effects/particle-behaviors/#raycast-behavior","text":"This behavior uses raycasts to detect collisions along the trajectory of a particle. If a particle would collide with geometry, the behavior can either adjust the its velocity, or terminate the particle early, potentially raising an event , which could in turn lead to other effects or being spawned. Reaction: Specifies how the particle should react to a collision. Bounce: The particle's velocity will be adjusted such that it bounces off the hit surface. Die: The particle will be killed early. Stop: The particle's current velocity will be set to zero, thus stopping it in its tracks. If other position affecting behaviors are active, for example the gravity behavior , it will start moving again, but without its previous momentum. BounceFactor: How much of the current speed should be preserved after the bounce. CollisionLayer: The physics collision layer to use. Affects with which geometry the particle will collide and which it will pass through. OnCollideEvent: An optional name of an event to raise. If set, other effects or prefabs can be spawned at the location of impact. ![[raycast.gif]]","title":"Raycast Behavior"},{"location":"effects/particle-effects/particle-behaviors/#size-curve-behavior","text":"This behavior changes a particle's size over the course of its lifetime. SizeCurve: A curve which is used to look up the size of the particle. The current fraction of the particle's lifespan is used for the lookup along the X axis. The absolute X and Y values in the curve don't matter, the curve is normalized to [0; 1] range. BaseSize: The particles will always have at least this size, the rest is added on top. CurveScale: Specifies what value the largest value in the curve maps to. That means at the peak of a curve, the particle's size will be BaseSize + CurveScale . ![[size-curve.gif]]","title":"Size Curve Behavior"},{"location":"effects/particle-effects/particle-behaviors/#velocity-behavior","text":"This behavior affects particle position and velocity. It can be used to gradually dampen the starting velocity through 'friction' and it may apply a constant upwards movement. If a scene contains wind , this behavior can also apply a fraction of the wind force to the particle's position. RiseSpeed: If non-zero, the particles will move upwards with at least this constant speed. This is added to the particle position independent from its velocity, so if the current velocity points downward, the two may cancel each other out. Friction: This value imitates air friction. If it is non-zero, the particle's velocity will be dampened over time. The value's range is [0; infinity] . To achieve an effect as in the animation below, the particles must have a very large starting velocity (here: 10). The friction here is set to 6. This way the particles will appear to be quite fast, but will also get slowed down almost to a standstill within a fraction of a second. WindInfluence: If the scene has wind, this value specifies how much the wind should be able to push the particles around. ![[velocity.gif]]","title":"Velocity Behavior"},{"location":"effects/particle-effects/particle-behaviors/#see-also","text":"Particle Effects Particle Initializers Particle Renderers","title":"See Also"},{"location":"effects/particle-effects/particle-effect-component/","text":"Particle Effect Component The particle effect component is used to instantiate and control particle effects in a scene. Each component handles one effect. When the owner game object is moved, the particle effect will move accordingly. Particles are emitted along the up axis (positive Z) of the game object. Effect: The particle effect to spawn. SpawnAtStart: If true, the effect will be spawned once the component becomes active. Otherwise, nothing will happen, and the component must be triggered manually via custom code. OnFinishedAction: Specifies what happens when a non-continuous effect finishes: None - The effect stays off, and the particle effect component and its owner object stay as they are. Delete Component - The particle effect component gets automatically deleted to clean up unused components. Delete Object - The game object that the component is attached to is deleted including all attached components and child objects. This can be very useful to clean up entire effect objects, once the effect is finished. Note: This mode can be combined with other components that also have an OnFinishedAction . If multiple such components are set to delete themselves or the owning object, the last one that finishes will delete the object hierarchy. All components that finish earlier will only delete themselves (as if Delete Component was selected on them). This way you can attach for example a particle effect, a decal and a sound source to the same game object, select an OnFinishedAction on all of them, and get the correct behavior, no matter which one finishes first. Restart - The effect will be restarted after an optional restart delay . MinRestartDelay, RestartDelayRange: If OnFinishedAction is set to Restart , a random time between MinRestartDelay and MinRestartDelay + RestartDelayRange has to pass before the effect will be restarted. RandomSeed: If set to zero, the effect will use random values and look slightly different every time. If set to any other value, the effect will look identical every time it is restarted. SpawnDirection: The direction along which the effect should be spawned (in local space). The default is 'positive Z' which means 'up', but to align this with other things, such as decals or lights , it can be useful to use a different axis. Note that interactions with surfaces (e.g. an impact effect that is spawned when a bullet hits a wall) are always spawned such that the spawned prefab's positive X axis aligns with the surface interaction axis (e.g. it's normal). For such cases it therefore makes sense to spawn a particle effect along 'positive X'. IgnoreOwnerRotation: By default the SpawnDirection is local to the owning game object, meaning when the owning object is tipped over, the effect will also spawn sideways. For some effects it can be desireable to ignore the rotation of the owner, and always spawn in global space , though. For instance, when an effect has a strong directionality, such as debris flying away in a cone, it may look best when it is always spawned upwards. SharedInstanceName: If non-empty, this instance will use a shared effect . Parameters: If the chosen effect exposed effect parameters , they will be listed here and can be modified. See Also Particle Effects","title":"Particle Effect Component"},{"location":"effects/particle-effects/particle-effect-component/#particle-effect-component","text":"The particle effect component is used to instantiate and control particle effects in a scene. Each component handles one effect. When the owner game object is moved, the particle effect will move accordingly. Particles are emitted along the up axis (positive Z) of the game object. Effect: The particle effect to spawn. SpawnAtStart: If true, the effect will be spawned once the component becomes active. Otherwise, nothing will happen, and the component must be triggered manually via custom code. OnFinishedAction: Specifies what happens when a non-continuous effect finishes: None - The effect stays off, and the particle effect component and its owner object stay as they are. Delete Component - The particle effect component gets automatically deleted to clean up unused components. Delete Object - The game object that the component is attached to is deleted including all attached components and child objects. This can be very useful to clean up entire effect objects, once the effect is finished. Note: This mode can be combined with other components that also have an OnFinishedAction . If multiple such components are set to delete themselves or the owning object, the last one that finishes will delete the object hierarchy. All components that finish earlier will only delete themselves (as if Delete Component was selected on them). This way you can attach for example a particle effect, a decal and a sound source to the same game object, select an OnFinishedAction on all of them, and get the correct behavior, no matter which one finishes first. Restart - The effect will be restarted after an optional restart delay . MinRestartDelay, RestartDelayRange: If OnFinishedAction is set to Restart , a random time between MinRestartDelay and MinRestartDelay + RestartDelayRange has to pass before the effect will be restarted. RandomSeed: If set to zero, the effect will use random values and look slightly different every time. If set to any other value, the effect will look identical every time it is restarted. SpawnDirection: The direction along which the effect should be spawned (in local space). The default is 'positive Z' which means 'up', but to align this with other things, such as decals or lights , it can be useful to use a different axis. Note that interactions with surfaces (e.g. an impact effect that is spawned when a bullet hits a wall) are always spawned such that the spawned prefab's positive X axis aligns with the surface interaction axis (e.g. it's normal). For such cases it therefore makes sense to spawn a particle effect along 'positive X'. IgnoreOwnerRotation: By default the SpawnDirection is local to the owning game object, meaning when the owning object is tipped over, the effect will also spawn sideways. For some effects it can be desireable to ignore the rotation of the owner, and always spawn in global space , though. For instance, when an effect has a strong directionality, such as debris flying away in a cone, it may look best when it is always spawned upwards. SharedInstanceName: If non-empty, this instance will use a shared effect . Parameters: If the chosen effect exposed effect parameters , they will be listed here and can be modified.","title":"Particle Effect Component"},{"location":"effects/particle-effects/particle-effect-component/#see-also","text":"Particle Effects","title":"See Also"},{"location":"effects/particle-effects/particle-effects-overview/","text":"Particle Effects Particle effects are used to create the visual part of things like explosions, smoke, fire, water splashes and much more. They are randomized to have slight variations every time. To create a full effect, like an explosion, with sound and physical properties such as pushing nearby objects away or damaging objects and creatures, a particle effect is typically put into a prefab , which contains additional components for sound and game play logic (e.g. through scripts ). To create a new particle effect, use Editor > Create Document and select Particle Effect as the file type. Or alternatively, right-click on any asset in the asset browser and select New > Particle Effect . Particle Editor UI This is an overview screenshot of the particle editor: The 3D viewport plays the effect in a loop. Using the toolbar buttons you can pause, reset, slow down or speed up the playback. On the right hand side there are multiple tabs which hold the various settings of the effect. If you are not too familiar with particle effects yet, please read how particle effects work . On the right hand side you see multiple tabbed panels: Systems The Systems panel is very central. Here you add new particle systems to the effect. However, this is also where you select which particle system to edit . The combo box specifies which particle system is currently active. All panels below ( Emitter , Initializers , Behaviors and Renderers ) show only the settings of the active particle system . When you add a new particle system with the green '+' button, you get a new system that uses a default configuration. Effect The Effect panel lists options for the overall effect, independent of the individual particle systems. Adjusting these options is typically only necessary once an effect is working well and you need to tweak its performance or allow users to adjust details through exposed parameters . Emitter, Initializers, Behaviors and Renderers These panels show the various options for the active particle system . When you select a different particle system from the combo box in the Systems panel, these panels will show different options. Event Reactions See events below. Particle System Configuration Every particle system has exactly one emitter , usually multiple initializers and behaviors , and typically one renderer . Most parameters are configured on those parts. Additionally, every particle system has these properties: Visible: This is an option for testing. If Visible is deactivated, the particle system is not simulated or rendered. Use this when you need to focus on editing other systems, or when a particle system is not yet good enough to be used. Invisible particle systems don't cost performance. Life: This is a value with variance . It specifies how long each particle will be simulated and rendered before it is removed from the system. A low variance means all particles live equally long, a high variance means some will have a short lifespan, others a much longer one. Note: A life span of zero seconds guarantees that a particle lives for exactly one frame , independent of frame rate. This is useful for effects where something visually striking (e.g. a flash) should pop up for the shortest possible time. LifeScaleParam: An optional effect parameter which can be used to scale the particle lifespan. OnDeathEvent: An optional name for the event when a particle dies. This can be used to spawn other effects. Emitter The emitter is what defines how new particles in this system get spawned. It mostly specifies when and how many particles are spawned. Typically particles are either spawned in one big burst or continuously. However, for advanced use cases the emitter may only spawn particles as a reaction to some event or when the particle effect node was moved a certain distance. For details about all available emitter types, see Particle Emitters . Initializers Every particle has a number of properties, such as position, velocity, color, size and rotation speed. For a newly spawned particle, these values must get a starting value. Initializers allow you to affect the starting value. For example, a particle's position is by default (0, 0, 0) , but using a Sphere Position Initializer , the starting position will be set to a random position inside a sphere, or even just on its surface. Initializers are executed exactly once for each new particle, thus they cost little performance. However, they only have an effect, if the starting value isn't subsequently overwritten by Behaviors . For instance if the Random Color Initializer is used, it will set the color of new particles. If, however, the Color Gradient Behavior is also used, the behavior will set the particle's color to a new value in every update, thus making the initializer pointless. Prefer to use initializers over behaviors, if the desired result can be achieved with either. For details about all available initializer types, see Particle Initializers . Behaviors Behaviors are the core particle effect update routines. Every time a particle moves, changes color, grows, shrinks or rotates, this is implemented by a behavior. Behaviors are executed for every particle, in every update step. For performance reasons, you should strive to use as few behaviors as possible. Every behavior reads some particle properties and writes one or two properties. For example the Size Curve Behavior reads a particles age and maximum lifespan and then looks up its new size from the provided size curve. Therefore it overwrites the particle's size property in every update. Initializers set a particle's property once when it is spawned, behaviors set (or update) a property continuously. Consequently a behavior may overwrite an initial value, making it redundant. Or it may build on top of the starting value. For example the Velocity Behavior can be used to have particles fly upwards (rise). Although the behavior modifies the particle's position property, it still works well together with the various position initializers, as it only adds to the position instead of replacing it. It is common for a particle system to have at least one, but often multiple, behaviors. For details about all available behavior types, see Particle Behaviors . Renderers Conceptually a particle is just a point in space. There are many ways this point can be visualized. Renderers are used to select how to do that. Most particle systems use one renderer, often the Billboard Renderer , which is the most versatile. However, you are free to use multiple renderers. For example to achieve a fire effect with heat haze, you may want to use two renderers. One to render the particles as billboards using a fire texture, and another one to apply the screen space distortion effect. For details about all available renderer types, see Particle Renderers . Effect Parameters Effect parameters are an advanced feature that allows you to make certain parts of an effect configurable from the outside. You add effect parameters in the Effect tab . At the moment only number parameters and color parameters are supported. All parameters that you add there will appear as exposed parameters on particle effect components . Effect parameters can only affect select features. For example the quad renderer has a TintColorParam property. If you type in the name of a color parameter there, the quad renderer will look up the value of the color parameter during every update, and use that to modulate the final color of the rendered particle. You can use this in a static way, as a means to add more variety to multiple instances of the same effect. Or you can use this in a dynamic way, by modifying the exposed parameter through (script) code, for example to visualize how hot something burns. Events and Event Reactions Particles may raise events . The most common one is when a particle dies, but different particle behaviors can raise other events as well. For example when a particle collides with the environment (see raycast behavior ). The Event Reactions tab allows you to configure what happens for a specific event. This is mostly used to chain effects. For example the fireworks effect below has particles that represent the rockets flying up, and when one 'dies', an explosion is spawned at that position, using event reactions. ![[event-reaction-effect.gif]] Instead of spawning other particle effects, you can also spawn entire prefabs, which enables even more complex effects. Every event reaction has a Probability value which should be between 1 and 100. For example, if one reaction has a probability of 50, then for half of all events, that event reaction will be spawned. If there is no other event reaction for the same event type, then nothing is spawned. If however, another event reaction exists, say with a probability value of 30, then it also gets a chance to be spawned. Since 20 probability points are still not assigned, there is an overall 20 percent chance that no reaction is spawned. Be aware that probabilities are not normalized across event reactions. If you have two reactions for the same event type, both with a probability of 100, then in practice the first event reaction will always be spawned, and the second one will never be spawned. Apart from event reactions, it is also possible to react to events using the OnEvent emitter , however, that is less common. Misc Variance Values The particle editor presents many values as values with variance . They appear as a single value with a slider next to it: The input box represents the base value and the slider represents the variance . The variance is between 0 and 1 . Generally, this type represents a random value, centered around the base value using a normal distribution . The variance affects the range from which random values are drawn. The range is always between BaseValue - Variance*BaseValue and BaseValue + Variance*BaseValue . Consequently: If Variance is 0 , the range and thus the result for every 'random' value will be exactly BaseValue . If the variance is 1 , the random value will be anywhere between 0 and 2 * BaseValue . For a variance of 0.5 , the random value will be between BaseValue - 0.5*BaseValue and BaseValue + 0.5*BaseValue However, due to the normal distribution of the random numbers, values close to BaseValue will appear much more often than values far away from it. Such distributions are common in nature and therefore the result looks more natural. For most such values you should use at least some variance (0.2 to 0.4) to make your effects look less repetitive and sterile. However, extremely large variance values (0.7 and up) can result in unexpected outliers. Local Space Simulation Some effects should always behave the same, no matter how the owning game object is rotated, or how fast it moves. This is often the case for rocket exhaust effects, for example. Such behavior is hard to achieve with the way particle effects typically work, though, as each particle would need to have an extremely short life span and move very fast. When enabling SimulateInLocalSpace in the Effect tab , the effect is simulated as if it was positioned at the world origin and with the default orientation. This removes any influence that the effects position, orientation and movement would otherwise have on the effect, as shown below: ![[sim-local-space.gif]] Some behaviors won't properly work for effects that are simulated in local space. For example the raycast behavior will do it's raycasts at the origin of the scene, rendering it pointless. There is no performance benefit to using local space simulation. However, when using shared effects , the shared state must be simulated in local space. Other options to keep particles closer to the owning object are to use the pull along behavior or to inherit the owner velocity . Owner Velocity Inheritance In the Effect tab there is a property ApplyOwnerVelocity which is a value between 0 and 1 . By default the value is zero, which means that all particles are initialized with either a zero velocity or with whatever some initializer decided. In that case, particles will fly away from the emitter position unaffected by the velocity of the effect object itself. However, if the value is set to non-zero, a part of the velocity of the owning game object will be added to newly spawned particles. This can be used for effects that may be spawned from moving objects and that shall retain some of that momentum. However, unless you additionally configure the effect to have some velocity damping (ie. using the friction property of the velocity behavior ), the particles will fly into that direction continuously, which may look weird, especially when the owner object changes direction or brakes, and the spawned particles overtake it. Other options to keep particles closer to the owning object are to use the pull along behavior or to fully simulate in local space . Shared Effects Typically every particle effect in the world is simulated every frame. However, especially 'ambient' effects, such as the fire of wall torches, is often instantiated many, many times. You may only see a few of them at a time, but if all those effects were simulated every single frame, that would cost significant performance. A solution to this problem is to use shared effects . When an effect is set to be shared, it is simulated only once. All instances, that reference the effect will only be used to render the effect at their position. If not a single instance is visible, the simulation of the shared effect will even be paused. There are two ways to make an effect shared. The global option is the AlwaysShared property which can be found in the Effect tab . If this is enabled, then all instances of the effect will always be shared. This should be used for ambient effects which are expected to be instantiated often and where simulating more than one has no benefit. The second option is to set a SharedInstanceName on the particle effect component . All effect components that use the exact same shared instance name , will share one simulation state and thus look identical. With this method, you can use a finite number of simulated effects, which allows you to have the same effect multiple times next to each other, without having them look identical, but still limiting how many effects need to be simulated. Because the various instances have different positions and orientations, shared effects are always simulated in local space . Effect sharing should mainly be used for continuous effects, and the effect should be authored to not be very distinctive. Also note that the rendering cost still has to be paid for every visible instance. Pre-Simulation Some effects are supposed to always look as if they are constantly running. Mostly this is needed for ambient effects, such as torch fire or chimney smoke. The game may only spawn these effects when the player enters an area, but it is not desirable to see them getting started before they reach a stable simulation state. Instead, they should always already be in the state that they reach after a couple of seconds of simulation. For such cases, you can use the PreSimulationDuration option from the Effects tab . When this is set to a couple of seconds, the first time an effect is simulated, it will be simulated multiple times, to reach the desired state. Note: Pre-simulation obviously has a performance cost during the first simulation step. Therefore, keep the pre-simulation duration as low as possible. Also be aware that for many ambient effects, that are instantiated a lot throughout a scene, prefer to use shared effects . Pre-simulation may still be necessary to fix their very first appearance, though. Update Rate When Invisible When a particle effect is not visible, it may still need to be updated, as the way that it changes may make it visible in the first place. For example the smoke of a smoke grenade that is behind the player may become visible when it is blown into the players view by the wind. It may be sufficient, though, to only update the effect ten times, or even just 5 times per second, while invisible. Thus reducing the computational overhead. However, there are also effects which do not need to be updated, at all, when invisible. A waterfall effect, for instance, will always look similar. Thus once it is out of view, it can be simply paused. And there are even effects that can be discarded entirely, when out of view. Bullet impact effects, for example, may be so small and have such a short life span, that there is no value in updating them at all, unless they are visible to begin with. Which update method to use can be chosen from the Effect tab using the WhenInvisible property. See Also How Particle Effects Work Particle Effect Component","title":"Particle Effects"},{"location":"effects/particle-effects/particle-effects-overview/#particle-effects","text":"Particle effects are used to create the visual part of things like explosions, smoke, fire, water splashes and much more. They are randomized to have slight variations every time. To create a full effect, like an explosion, with sound and physical properties such as pushing nearby objects away or damaging objects and creatures, a particle effect is typically put into a prefab , which contains additional components for sound and game play logic (e.g. through scripts ). To create a new particle effect, use Editor > Create Document and select Particle Effect as the file type. Or alternatively, right-click on any asset in the asset browser and select New > Particle Effect .","title":"Particle Effects"},{"location":"effects/particle-effects/particle-effects-overview/#particle-editor-ui","text":"This is an overview screenshot of the particle editor: The 3D viewport plays the effect in a loop. Using the toolbar buttons you can pause, reset, slow down or speed up the playback. On the right hand side there are multiple tabs which hold the various settings of the effect. If you are not too familiar with particle effects yet, please read how particle effects work . On the right hand side you see multiple tabbed panels:","title":"Particle Editor UI"},{"location":"effects/particle-effects/particle-effects-overview/#systems","text":"The Systems panel is very central. Here you add new particle systems to the effect. However, this is also where you select which particle system to edit . The combo box specifies which particle system is currently active. All panels below ( Emitter , Initializers , Behaviors and Renderers ) show only the settings of the active particle system . When you add a new particle system with the green '+' button, you get a new system that uses a default configuration.","title":"Systems"},{"location":"effects/particle-effects/particle-effects-overview/#effect","text":"The Effect panel lists options for the overall effect, independent of the individual particle systems. Adjusting these options is typically only necessary once an effect is working well and you need to tweak its performance or allow users to adjust details through exposed parameters .","title":"Effect"},{"location":"effects/particle-effects/particle-effects-overview/#emitter-initializers-behaviors-and-renderers","text":"These panels show the various options for the active particle system . When you select a different particle system from the combo box in the Systems panel, these panels will show different options.","title":"Emitter, Initializers, Behaviors and Renderers"},{"location":"effects/particle-effects/particle-effects-overview/#event-reactions","text":"See events below.","title":"Event Reactions"},{"location":"effects/particle-effects/particle-effects-overview/#particle-system-configuration","text":"Every particle system has exactly one emitter , usually multiple initializers and behaviors , and typically one renderer . Most parameters are configured on those parts. Additionally, every particle system has these properties: Visible: This is an option for testing. If Visible is deactivated, the particle system is not simulated or rendered. Use this when you need to focus on editing other systems, or when a particle system is not yet good enough to be used. Invisible particle systems don't cost performance. Life: This is a value with variance . It specifies how long each particle will be simulated and rendered before it is removed from the system. A low variance means all particles live equally long, a high variance means some will have a short lifespan, others a much longer one. Note: A life span of zero seconds guarantees that a particle lives for exactly one frame , independent of frame rate. This is useful for effects where something visually striking (e.g. a flash) should pop up for the shortest possible time. LifeScaleParam: An optional effect parameter which can be used to scale the particle lifespan. OnDeathEvent: An optional name for the event when a particle dies. This can be used to spawn other effects.","title":"Particle System Configuration"},{"location":"effects/particle-effects/particle-effects-overview/#emitter","text":"The emitter is what defines how new particles in this system get spawned. It mostly specifies when and how many particles are spawned. Typically particles are either spawned in one big burst or continuously. However, for advanced use cases the emitter may only spawn particles as a reaction to some event or when the particle effect node was moved a certain distance. For details about all available emitter types, see Particle Emitters .","title":"Emitter"},{"location":"effects/particle-effects/particle-effects-overview/#initializers","text":"Every particle has a number of properties, such as position, velocity, color, size and rotation speed. For a newly spawned particle, these values must get a starting value. Initializers allow you to affect the starting value. For example, a particle's position is by default (0, 0, 0) , but using a Sphere Position Initializer , the starting position will be set to a random position inside a sphere, or even just on its surface. Initializers are executed exactly once for each new particle, thus they cost little performance. However, they only have an effect, if the starting value isn't subsequently overwritten by Behaviors . For instance if the Random Color Initializer is used, it will set the color of new particles. If, however, the Color Gradient Behavior is also used, the behavior will set the particle's color to a new value in every update, thus making the initializer pointless. Prefer to use initializers over behaviors, if the desired result can be achieved with either. For details about all available initializer types, see Particle Initializers .","title":"Initializers"},{"location":"effects/particle-effects/particle-effects-overview/#behaviors","text":"Behaviors are the core particle effect update routines. Every time a particle moves, changes color, grows, shrinks or rotates, this is implemented by a behavior. Behaviors are executed for every particle, in every update step. For performance reasons, you should strive to use as few behaviors as possible. Every behavior reads some particle properties and writes one or two properties. For example the Size Curve Behavior reads a particles age and maximum lifespan and then looks up its new size from the provided size curve. Therefore it overwrites the particle's size property in every update. Initializers set a particle's property once when it is spawned, behaviors set (or update) a property continuously. Consequently a behavior may overwrite an initial value, making it redundant. Or it may build on top of the starting value. For example the Velocity Behavior can be used to have particles fly upwards (rise). Although the behavior modifies the particle's position property, it still works well together with the various position initializers, as it only adds to the position instead of replacing it. It is common for a particle system to have at least one, but often multiple, behaviors. For details about all available behavior types, see Particle Behaviors .","title":"Behaviors"},{"location":"effects/particle-effects/particle-effects-overview/#renderers","text":"Conceptually a particle is just a point in space. There are many ways this point can be visualized. Renderers are used to select how to do that. Most particle systems use one renderer, often the Billboard Renderer , which is the most versatile. However, you are free to use multiple renderers. For example to achieve a fire effect with heat haze, you may want to use two renderers. One to render the particles as billboards using a fire texture, and another one to apply the screen space distortion effect. For details about all available renderer types, see Particle Renderers .","title":"Renderers"},{"location":"effects/particle-effects/particle-effects-overview/#effect-parameters","text":"Effect parameters are an advanced feature that allows you to make certain parts of an effect configurable from the outside. You add effect parameters in the Effect tab . At the moment only number parameters and color parameters are supported. All parameters that you add there will appear as exposed parameters on particle effect components . Effect parameters can only affect select features. For example the quad renderer has a TintColorParam property. If you type in the name of a color parameter there, the quad renderer will look up the value of the color parameter during every update, and use that to modulate the final color of the rendered particle. You can use this in a static way, as a means to add more variety to multiple instances of the same effect. Or you can use this in a dynamic way, by modifying the exposed parameter through (script) code, for example to visualize how hot something burns.","title":"Effect Parameters"},{"location":"effects/particle-effects/particle-effects-overview/#events-and-event-reactions","text":"Particles may raise events . The most common one is when a particle dies, but different particle behaviors can raise other events as well. For example when a particle collides with the environment (see raycast behavior ). The Event Reactions tab allows you to configure what happens for a specific event. This is mostly used to chain effects. For example the fireworks effect below has particles that represent the rockets flying up, and when one 'dies', an explosion is spawned at that position, using event reactions. ![[event-reaction-effect.gif]] Instead of spawning other particle effects, you can also spawn entire prefabs, which enables even more complex effects. Every event reaction has a Probability value which should be between 1 and 100. For example, if one reaction has a probability of 50, then for half of all events, that event reaction will be spawned. If there is no other event reaction for the same event type, then nothing is spawned. If however, another event reaction exists, say with a probability value of 30, then it also gets a chance to be spawned. Since 20 probability points are still not assigned, there is an overall 20 percent chance that no reaction is spawned. Be aware that probabilities are not normalized across event reactions. If you have two reactions for the same event type, both with a probability of 100, then in practice the first event reaction will always be spawned, and the second one will never be spawned. Apart from event reactions, it is also possible to react to events using the OnEvent emitter , however, that is less common.","title":"Events and Event Reactions"},{"location":"effects/particle-effects/particle-effects-overview/#misc","text":"","title":"Misc"},{"location":"effects/particle-effects/particle-effects-overview/#variance-values","text":"The particle editor presents many values as values with variance . They appear as a single value with a slider next to it: The input box represents the base value and the slider represents the variance . The variance is between 0 and 1 . Generally, this type represents a random value, centered around the base value using a normal distribution . The variance affects the range from which random values are drawn. The range is always between BaseValue - Variance*BaseValue and BaseValue + Variance*BaseValue . Consequently: If Variance is 0 , the range and thus the result for every 'random' value will be exactly BaseValue . If the variance is 1 , the random value will be anywhere between 0 and 2 * BaseValue . For a variance of 0.5 , the random value will be between BaseValue - 0.5*BaseValue and BaseValue + 0.5*BaseValue However, due to the normal distribution of the random numbers, values close to BaseValue will appear much more often than values far away from it. Such distributions are common in nature and therefore the result looks more natural. For most such values you should use at least some variance (0.2 to 0.4) to make your effects look less repetitive and sterile. However, extremely large variance values (0.7 and up) can result in unexpected outliers.","title":"Variance Values"},{"location":"effects/particle-effects/particle-effects-overview/#local-space-simulation","text":"Some effects should always behave the same, no matter how the owning game object is rotated, or how fast it moves. This is often the case for rocket exhaust effects, for example. Such behavior is hard to achieve with the way particle effects typically work, though, as each particle would need to have an extremely short life span and move very fast. When enabling SimulateInLocalSpace in the Effect tab , the effect is simulated as if it was positioned at the world origin and with the default orientation. This removes any influence that the effects position, orientation and movement would otherwise have on the effect, as shown below: ![[sim-local-space.gif]] Some behaviors won't properly work for effects that are simulated in local space. For example the raycast behavior will do it's raycasts at the origin of the scene, rendering it pointless. There is no performance benefit to using local space simulation. However, when using shared effects , the shared state must be simulated in local space. Other options to keep particles closer to the owning object are to use the pull along behavior or to inherit the owner velocity .","title":"Local Space Simulation"},{"location":"effects/particle-effects/particle-effects-overview/#owner-velocity-inheritance","text":"In the Effect tab there is a property ApplyOwnerVelocity which is a value between 0 and 1 . By default the value is zero, which means that all particles are initialized with either a zero velocity or with whatever some initializer decided. In that case, particles will fly away from the emitter position unaffected by the velocity of the effect object itself. However, if the value is set to non-zero, a part of the velocity of the owning game object will be added to newly spawned particles. This can be used for effects that may be spawned from moving objects and that shall retain some of that momentum. However, unless you additionally configure the effect to have some velocity damping (ie. using the friction property of the velocity behavior ), the particles will fly into that direction continuously, which may look weird, especially when the owner object changes direction or brakes, and the spawned particles overtake it. Other options to keep particles closer to the owning object are to use the pull along behavior or to fully simulate in local space .","title":"Owner Velocity Inheritance"},{"location":"effects/particle-effects/particle-effects-overview/#shared-effects","text":"Typically every particle effect in the world is simulated every frame. However, especially 'ambient' effects, such as the fire of wall torches, is often instantiated many, many times. You may only see a few of them at a time, but if all those effects were simulated every single frame, that would cost significant performance. A solution to this problem is to use shared effects . When an effect is set to be shared, it is simulated only once. All instances, that reference the effect will only be used to render the effect at their position. If not a single instance is visible, the simulation of the shared effect will even be paused. There are two ways to make an effect shared. The global option is the AlwaysShared property which can be found in the Effect tab . If this is enabled, then all instances of the effect will always be shared. This should be used for ambient effects which are expected to be instantiated often and where simulating more than one has no benefit. The second option is to set a SharedInstanceName on the particle effect component . All effect components that use the exact same shared instance name , will share one simulation state and thus look identical. With this method, you can use a finite number of simulated effects, which allows you to have the same effect multiple times next to each other, without having them look identical, but still limiting how many effects need to be simulated. Because the various instances have different positions and orientations, shared effects are always simulated in local space . Effect sharing should mainly be used for continuous effects, and the effect should be authored to not be very distinctive. Also note that the rendering cost still has to be paid for every visible instance.","title":"Shared Effects"},{"location":"effects/particle-effects/particle-effects-overview/#pre-simulation","text":"Some effects are supposed to always look as if they are constantly running. Mostly this is needed for ambient effects, such as torch fire or chimney smoke. The game may only spawn these effects when the player enters an area, but it is not desirable to see them getting started before they reach a stable simulation state. Instead, they should always already be in the state that they reach after a couple of seconds of simulation. For such cases, you can use the PreSimulationDuration option from the Effects tab . When this is set to a couple of seconds, the first time an effect is simulated, it will be simulated multiple times, to reach the desired state. Note: Pre-simulation obviously has a performance cost during the first simulation step. Therefore, keep the pre-simulation duration as low as possible. Also be aware that for many ambient effects, that are instantiated a lot throughout a scene, prefer to use shared effects . Pre-simulation may still be necessary to fix their very first appearance, though.","title":"Pre-Simulation"},{"location":"effects/particle-effects/particle-effects-overview/#update-rate-when-invisible","text":"When a particle effect is not visible, it may still need to be updated, as the way that it changes may make it visible in the first place. For example the smoke of a smoke grenade that is behind the player may become visible when it is blown into the players view by the wind. It may be sufficient, though, to only update the effect ten times, or even just 5 times per second, while invisible. Thus reducing the computational overhead. However, there are also effects which do not need to be updated, at all, when invisible. A waterfall effect, for instance, will always look similar. Thus once it is out of view, it can be simply paused. And there are even effects that can be discarded entirely, when out of view. Bullet impact effects, for example, may be so small and have such a short life span, that there is no value in updating them at all, unless they are visible to begin with. Which update method to use can be chosen from the Effect tab using the WhenInvisible property.","title":"Update Rate When Invisible"},{"location":"effects/particle-effects/particle-effects-overview/#see-also","text":"How Particle Effects Work Particle Effect Component","title":"See Also"},{"location":"effects/particle-effects/particle-emitters/","text":"Particle Emitters This page lists and describes all particle emitters . Burst Emitter This emitter type spawns particles either in one instantaneous burst or over a limited amount of time. It is mainly used for one-off effects like explosions, impacts, etc, which have a short lifespan. Once the burst emitter is finished, the particle effect will only continue to live until all spawned particles have reached the end of their life. For such effects the particle effect components are typically set to auto-delete themselves after the effect is finished. Duration: The timespan over which the emitter will distribute the spawning of the particles. If this is set to zero, all particles spawn at the same instant. StartDelay: An optional delay from when the particle effect is created, until the emitter starts spawning particles. Useful in effects with multiple particle systems, to tweak when one type of particles becomes visible, relative to other types of particles. MinSpawnCount, SpawnCountRange: A random number of particles between MinSpawnCount and MinSpawnCount + SpawnCountRange is emitted over the emitter's duration . SpawnCountScaleParam: An optional name of an effect parameter that can be used to scale the number of emitted particles up or down. Note: At the moment this mostly allows to reduce the number of emitted particles. Increasing the amount of particles may have no visible effect. ![[burst-emitter.gif]] Continuous Emitter This emitter type continuously spawns new particles. Effects which have at least one such emitter type will never stop, unless custom code specifically switches the effect off, or the owning particle effect component is deleted. In both cases all spawned particles will continue to be simulated and rendered, until they reach the end of their life. This emitter type is commonly used for ambient effects such as smoke and fire. By exposing effect parameters , continuous particle effects can be adjusted dynamically to visualize game mechanics, such as how hot something burns or how active some machine is. StartDelay: See the burst emitter . SpawnCountPerSec, SpawnCountPerSecRange: A random number of particles between SpawnCountPerSec and SpawnCountPerSec + SpawnCountPerSecRange is emitted every second. SpawnCountScaleParam: See the burst emitter . CountCurve, CurveDuration: If no CountCurve is specified, particles are spawned in regular intervals. Only a large value for SpawnCountPerSecRange may introduce irregularities. Using a count curve, the distribution of how many particles are spawned at what time can be controlled. If a curve is given, CurveDuration specifies its timespan. For instance, a curve duration of two seconds means, that the count curve is sampled from left to right over a duration of two seconds, before it repeats again. The value of the curve at a given time determines how many particles will get spawned. The curve is only used as a scale factor between zero and one, though (its absolute values don't matter, it is normalized internally). Every time the emitter attempts to spawn particles, SpawnCountPerSec and SpawnCountPerSecRange determine the maximum amount of particles to spawn. Then the curve is sampled and the current value is used to scale the number of particles down. Thus count curves can be used to introduce more elaborate spawn patterns. ![[continuous-emitter.gif]] Distance Emitter This emitter type only spawns new particles when the particle effect is moved for a distance of at least DistanceThreshold units. This can be used when an effect should have a relatively uniform particle density when in motion, without constantly spawning large amounts of particles. When the effect stands still, this emitter will not spawn any particles, so you may want to combine this with another continuous emitter . DistanceThreshold: The distance that the effect has to be moved for the emitter to spawn another set of particles. MinSpawnCount, SpawnCountRange: See the burst emitter . SpawnCountScaleParam: See the burst emitter . ![[distance-emitter.gif]] OnEvent Emitter This emitter type spawns new particles whenever a specific event happens. It does not create the new particles at the position of the event. If that is desired, use an event reaction instead. EventName: The name of the event which shall trigger spawning particles. MinSpawnCount, SpawnCountRange: See the burst emitter . SpawnCountScaleParam: See the burst emitter . In the animation below, the blue particles use a raycast behavior to get removed when a collision is detected. The behavior also sends an event . This is picked up by a second particle system, which then spawns a number of red particles. ![[onevent-emitter.gif]] See Also Particle Effects","title":"Particle Emitters"},{"location":"effects/particle-effects/particle-emitters/#particle-emitters","text":"This page lists and describes all particle emitters .","title":"Particle Emitters"},{"location":"effects/particle-effects/particle-emitters/#burst-emitter","text":"This emitter type spawns particles either in one instantaneous burst or over a limited amount of time. It is mainly used for one-off effects like explosions, impacts, etc, which have a short lifespan. Once the burst emitter is finished, the particle effect will only continue to live until all spawned particles have reached the end of their life. For such effects the particle effect components are typically set to auto-delete themselves after the effect is finished. Duration: The timespan over which the emitter will distribute the spawning of the particles. If this is set to zero, all particles spawn at the same instant. StartDelay: An optional delay from when the particle effect is created, until the emitter starts spawning particles. Useful in effects with multiple particle systems, to tweak when one type of particles becomes visible, relative to other types of particles. MinSpawnCount, SpawnCountRange: A random number of particles between MinSpawnCount and MinSpawnCount + SpawnCountRange is emitted over the emitter's duration . SpawnCountScaleParam: An optional name of an effect parameter that can be used to scale the number of emitted particles up or down. Note: At the moment this mostly allows to reduce the number of emitted particles. Increasing the amount of particles may have no visible effect. ![[burst-emitter.gif]]","title":"Burst Emitter"},{"location":"effects/particle-effects/particle-emitters/#continuous-emitter","text":"This emitter type continuously spawns new particles. Effects which have at least one such emitter type will never stop, unless custom code specifically switches the effect off, or the owning particle effect component is deleted. In both cases all spawned particles will continue to be simulated and rendered, until they reach the end of their life. This emitter type is commonly used for ambient effects such as smoke and fire. By exposing effect parameters , continuous particle effects can be adjusted dynamically to visualize game mechanics, such as how hot something burns or how active some machine is. StartDelay: See the burst emitter . SpawnCountPerSec, SpawnCountPerSecRange: A random number of particles between SpawnCountPerSec and SpawnCountPerSec + SpawnCountPerSecRange is emitted every second. SpawnCountScaleParam: See the burst emitter . CountCurve, CurveDuration: If no CountCurve is specified, particles are spawned in regular intervals. Only a large value for SpawnCountPerSecRange may introduce irregularities. Using a count curve, the distribution of how many particles are spawned at what time can be controlled. If a curve is given, CurveDuration specifies its timespan. For instance, a curve duration of two seconds means, that the count curve is sampled from left to right over a duration of two seconds, before it repeats again. The value of the curve at a given time determines how many particles will get spawned. The curve is only used as a scale factor between zero and one, though (its absolute values don't matter, it is normalized internally). Every time the emitter attempts to spawn particles, SpawnCountPerSec and SpawnCountPerSecRange determine the maximum amount of particles to spawn. Then the curve is sampled and the current value is used to scale the number of particles down. Thus count curves can be used to introduce more elaborate spawn patterns. ![[continuous-emitter.gif]]","title":"Continuous Emitter"},{"location":"effects/particle-effects/particle-emitters/#distance-emitter","text":"This emitter type only spawns new particles when the particle effect is moved for a distance of at least DistanceThreshold units. This can be used when an effect should have a relatively uniform particle density when in motion, without constantly spawning large amounts of particles. When the effect stands still, this emitter will not spawn any particles, so you may want to combine this with another continuous emitter . DistanceThreshold: The distance that the effect has to be moved for the emitter to spawn another set of particles. MinSpawnCount, SpawnCountRange: See the burst emitter . SpawnCountScaleParam: See the burst emitter . ![[distance-emitter.gif]]","title":"Distance Emitter"},{"location":"effects/particle-effects/particle-emitters/#onevent-emitter","text":"This emitter type spawns new particles whenever a specific event happens. It does not create the new particles at the position of the event. If that is desired, use an event reaction instead. EventName: The name of the event which shall trigger spawning particles. MinSpawnCount, SpawnCountRange: See the burst emitter . SpawnCountScaleParam: See the burst emitter . In the animation below, the blue particles use a raycast behavior to get removed when a collision is detected. The behavior also sends an event . This is picked up by a second particle system, which then spawns a number of red particles. ![[onevent-emitter.gif]]","title":"OnEvent Emitter"},{"location":"effects/particle-effects/particle-emitters/#see-also","text":"Particle Effects","title":"See Also"},{"location":"effects/particle-effects/particle-initializers/","text":"Particle Initializers This page lists and describes all particle initializers . Box Position Initializer Initializes a particle's position to a random point within a box shape. PositionOffset, Size: These values define the size and position of the box, relative to the origin of the particle system. With a position offset of (0, 0, 0) , the box will be centered around the system's origin. ScaleXParam, ScaleYParam, ScaleZParam: Optional names of effect parameters . This allows to scale the volume in which particles spawn. Note: Scaling the volume will change particle density. To compensate, the particle system will automatically spawn more or fewer particles. Thus you can author an effect as a 1x1x0 meter sized patch and then let the user decide how large a patch she needs by exposing these parameters. If your 1x1x0 patch requires roughly 100 particles at all times, then scaling it to a 10x5x0 patch will require 5000 particles. ![[box-position-init.gif]] Cylinder Position Initializer Initializes a particle's position to a random point either within a cylinder or on its surface. A cylinder of height 0 initializes the position to a random point on a circle or its circumference. PositionOffset, Radius, Height: These values define the size and position of the cylinder, relative to the origin of the particle system. With a position offset of (0, 0, 0) , the cylinder will be centered around the system's origin. A height of 0 turns the cylinder into a circle. OnSurface: If enabled, particles will only spawn on the surface of the cylinder, not inside it. This also excludes the caps. For a cylinder of height 0 that means the particles will spawn on the circumference of a circle. SetVelocity, Speed: If enabled, the initializer will additionally set the particle's starting velocity. The velocity is always outward from the cylinder's center line. ScaleRadiusParam, ScaleHeightParam: Optional names of effect parameters . This allows to scale the volume in which particles spawn. Note: Scaling the volume will change particle density. See the box position initializer for details. ![[cylinder-position-init.gif]] Sphere Position Initializer Initializes a particle's position to a random point within a sphere shape. PositionOffset, Radius: These values define the size and position of the sphere, relative to the origin of the particle system. With a position offset of (0, 0, 0) , the sphere will be centered around the system's origin. OnSurface: If enabled, particles will only spawn on the surface of the sphere, not inside it. SetVelocity, Speed: If enabled, the initializer will additionally set the particle's starting velocity. The velocity is always outward from the sphere's center. ScaleRadiusParam: Optional name of an effect parameter . This allows to scale the volume in which particles spawn. Note: Scaling the volume will change particle density. See the box position initializer for details. ![[sphere-position-init.gif]] Random Color Initializer Initializes a particle's color to a random color. Gradient: If specified, the random color will be picked from the given color gradient . Color1, Color2: A random interpolated color between the two given colors is used. So if one color is white and the other is black, particles will get a random grey value as their color. If a gradient is set as well, the two colors are combined. ![[random-color-init.gif]] Random Size Initializer Initializes a particle's size to a random value. Size: The base size for the particles to start with. To initialize all particles to have a fixed size, set the variance to zero. SizeCurve: If specified, the curve is sampled at a random location and the normalized value (always between 0 and 1 ) is used to scale the randomly chosen base size. The shape of the curve has no meaning for this use case, it only provides a way to affect the distribution of the random sizes. For example, you could have a curve that sets exactly half of all particles to exactly a tenth of the base size. If you want exactly the same distribution as the curve has, you should set the variance of the base size to zero. ![[random-size-init.gif]] Rotation Speed Initializer Initializes a particle's rotation and rotation speed to a random value. RandomStartAngle: If enabled, the particle will start out with a random rotation. For particles with a distinct texture or shape, this can make the effect look significantly more natural. DegreesPerSecond: If set to a non-zero value, particles will rotate with a constant speed. Each particle gets its own random speed assigned. With a low variance all particles will rotate similarly fast, with a high variance you will see some particles rotate very fast and some very slowly. Half of the particles rotate clockwise, the other half counter-clockwise. ![[rotation-speed-init.gif]] Velocity Cone Initializer Initializes a particle's velocity to a random up vector. Angle: The maximum opening angle of the upside down cone. With a small opening angle, particles will fly straight up. With a wide opening angle, particles will fly in all directions. Speed: The initial speed for the particles. ![[velocity-cone-init.gif]] See Also Particle Effects Particle Behaviors Particle Renderers","title":"Particle Initializers"},{"location":"effects/particle-effects/particle-initializers/#particle-initializers","text":"This page lists and describes all particle initializers .","title":"Particle Initializers"},{"location":"effects/particle-effects/particle-initializers/#box-position-initializer","text":"Initializes a particle's position to a random point within a box shape. PositionOffset, Size: These values define the size and position of the box, relative to the origin of the particle system. With a position offset of (0, 0, 0) , the box will be centered around the system's origin. ScaleXParam, ScaleYParam, ScaleZParam: Optional names of effect parameters . This allows to scale the volume in which particles spawn. Note: Scaling the volume will change particle density. To compensate, the particle system will automatically spawn more or fewer particles. Thus you can author an effect as a 1x1x0 meter sized patch and then let the user decide how large a patch she needs by exposing these parameters. If your 1x1x0 patch requires roughly 100 particles at all times, then scaling it to a 10x5x0 patch will require 5000 particles. ![[box-position-init.gif]]","title":"Box Position Initializer"},{"location":"effects/particle-effects/particle-initializers/#cylinder-position-initializer","text":"Initializes a particle's position to a random point either within a cylinder or on its surface. A cylinder of height 0 initializes the position to a random point on a circle or its circumference. PositionOffset, Radius, Height: These values define the size and position of the cylinder, relative to the origin of the particle system. With a position offset of (0, 0, 0) , the cylinder will be centered around the system's origin. A height of 0 turns the cylinder into a circle. OnSurface: If enabled, particles will only spawn on the surface of the cylinder, not inside it. This also excludes the caps. For a cylinder of height 0 that means the particles will spawn on the circumference of a circle. SetVelocity, Speed: If enabled, the initializer will additionally set the particle's starting velocity. The velocity is always outward from the cylinder's center line. ScaleRadiusParam, ScaleHeightParam: Optional names of effect parameters . This allows to scale the volume in which particles spawn. Note: Scaling the volume will change particle density. See the box position initializer for details. ![[cylinder-position-init.gif]]","title":"Cylinder Position Initializer"},{"location":"effects/particle-effects/particle-initializers/#sphere-position-initializer","text":"Initializes a particle's position to a random point within a sphere shape. PositionOffset, Radius: These values define the size and position of the sphere, relative to the origin of the particle system. With a position offset of (0, 0, 0) , the sphere will be centered around the system's origin. OnSurface: If enabled, particles will only spawn on the surface of the sphere, not inside it. SetVelocity, Speed: If enabled, the initializer will additionally set the particle's starting velocity. The velocity is always outward from the sphere's center. ScaleRadiusParam: Optional name of an effect parameter . This allows to scale the volume in which particles spawn. Note: Scaling the volume will change particle density. See the box position initializer for details. ![[sphere-position-init.gif]]","title":"Sphere Position Initializer"},{"location":"effects/particle-effects/particle-initializers/#random-color-initializer","text":"Initializes a particle's color to a random color. Gradient: If specified, the random color will be picked from the given color gradient . Color1, Color2: A random interpolated color between the two given colors is used. So if one color is white and the other is black, particles will get a random grey value as their color. If a gradient is set as well, the two colors are combined. ![[random-color-init.gif]]","title":"Random Color Initializer"},{"location":"effects/particle-effects/particle-initializers/#random-size-initializer","text":"Initializes a particle's size to a random value. Size: The base size for the particles to start with. To initialize all particles to have a fixed size, set the variance to zero. SizeCurve: If specified, the curve is sampled at a random location and the normalized value (always between 0 and 1 ) is used to scale the randomly chosen base size. The shape of the curve has no meaning for this use case, it only provides a way to affect the distribution of the random sizes. For example, you could have a curve that sets exactly half of all particles to exactly a tenth of the base size. If you want exactly the same distribution as the curve has, you should set the variance of the base size to zero. ![[random-size-init.gif]]","title":"Random Size Initializer"},{"location":"effects/particle-effects/particle-initializers/#rotation-speed-initializer","text":"Initializes a particle's rotation and rotation speed to a random value. RandomStartAngle: If enabled, the particle will start out with a random rotation. For particles with a distinct texture or shape, this can make the effect look significantly more natural. DegreesPerSecond: If set to a non-zero value, particles will rotate with a constant speed. Each particle gets its own random speed assigned. With a low variance all particles will rotate similarly fast, with a high variance you will see some particles rotate very fast and some very slowly. Half of the particles rotate clockwise, the other half counter-clockwise. ![[rotation-speed-init.gif]]","title":"Rotation Speed Initializer"},{"location":"effects/particle-effects/particle-initializers/#velocity-cone-initializer","text":"Initializes a particle's velocity to a random up vector. Angle: The maximum opening angle of the upside down cone. With a small opening angle, particles will fly straight up. With a wide opening angle, particles will fly in all directions. Speed: The initial speed for the particles. ![[velocity-cone-init.gif]]","title":"Velocity Cone Initializer"},{"location":"effects/particle-effects/particle-initializers/#see-also","text":"Particle Effects Particle Behaviors Particle Renderers","title":"See Also"},{"location":"effects/particle-effects/particle-renderers/","text":"Particle Renderers Quad Renderer This renderer visualizes each particle as a quad. There are several modes to choose from how this quad is oriented. There are also different modes how to blend the particle with the background. This renderer is very versatile. Orientation: This mode defines how the quad is oriented and around which axis it may rotate. In the 'rotating' modes the quad geometry constantly rotates around some axis that is decided when the particle is spawned. In the 'fixed' modes the quads themselves will not rotate, but have a fixed plane decided when they spawn. If they have a non-zero rotation speed, their texture will rotate around the quads center. In the 'axis' modes, the quads have one fixed axis and one that orients itself into the direction of the camera. Billboard - This is the most common mode. Billboards always face the camera. If the camera moves around the effect, the billboards keep orienting towards it. When billboards should rotate, they always rotate in screen-space, meaning around the current forward axis of the camera. ![[billboards.gif]] Rotating: Ortho Emitter Dir - In this mode the quads rotate around the orthogonal axis between the direction of the emitter and the direction the particle moves into. This mode is particularly useful for simulating debris of concrete for bullet impacts and such. When the particles have a high rotation speed , they will fly off in a spectacular fashion. For this type of effect it is also best to use alpha-masked textures representing debris, and to use the 'Opaque' render mode . ![[quad-rotating-ortho.gif]] Rotating: Emitter Dir - Similar to the mode above, but uses the direction of the emitter as its rotation axis. This can be used for muzzle flashes (the part that shows along the direction of the barrel) or impact effects. ![[quad-rotating-emdir.gif]] Fixed: Emitter Dir - In this mode the quad always uses the emitter direction as its plane normal (with some optional Deviation ). This mode is useful for creating shockwave effects at a point of impact, or things like ripples in water. It can also be used for muzzle flashes (the part sideways out of a barrel). ![[quad-fixed-emdir.gif]] Fixed: World Up - Similar to Fixed: Emitter Dir but the axis used is always the world up direction. This can be useful for effects where the emitter may have an arbitrary direction, but the particles should always face upwards. For example for the ripples of water impact effects. Fixed: Random Dir - In this mode each quad gets a random axis assigned when it is spawned. This can be useful for creating shockwave effects in explosions. ![[random-color-init.gif]] Axis: Emitter Dir - In this mode the quads fixed axis is the direction of the emitter. The quad will rotate around this axis to face the camera as much as possible. Additionally, the quad will not scale around its center, but around one of its edges. Therefore, when scaling up or down, that edge will stay in a fixed position. This can be used to create muzzle flashes and other impact effects which should generally move into the direction of the emitter, but also face the camera as much as possible, to be well visible. This mode won't look too convincing if the particles' movement deviates strongly from the emitter direction. For most common use cases, the particles may not move at all, but only change their size. ![[quad-axis-emdir.gif]] Axis: Particle Dir - In this mode the quads fixed axis is its own fly direction. The quad will rotate around this axis to face the camera as much as possible. This is useful for creating sparks or laser blasts which shall stretch a little while moving into some direction. ![[continuous-emitter.gif]] Render Mode: This mode specifies how the color from the particle will be combined with the scene background. Opaque - The particle will use the alpha channel of the texture as mask. Pixels are either fully transparent or fully opaque. This is mostly useful for debris. Additive - The particle's color will be added to the background. The alpha-channel is not used. This is used for everything that should glow (magic spells, sparks, ...) Blended - The alpha channel of the texture is used to interpolate the particle's color with the background. This is used for everything that should be transparent, but not glowing (smoke and such). It is also the most tricky mode to make look good, as it depends the most on a good texture with a proper alpha channel. Particles rendered with this mode must be sorted by distance by the renderer, which incurs an additional performance cost. Blended Foreground/Background - Same as Blended but when there are multiple particle systems using Blended mode, this allows you to influence in which order the systems are rendered. This is used to fix rendering issues. Distortion - This is used to create a heat haze effect, which distorts the scene behind it. The alpha-channel of Texture is used to determine the shape of the distortion effect. The Distortion Texture and Distortion Strength are used to decide for each pixels how much to distort the background. Any texture can be used as the distortion texture, but the effect works best using a normal map . ![[render-mode-distortion.gif]] Texture: The texture used for rendering. May be a texture atlas or contain flipbook animations. Texture Atlas: Specifies how to interpret the content in Texture : None - The texture contains only a single image. Random Variations - The texture contains NumSpritesX x NumSpritesY images in a regular grid. Each image is independent and each particle uses a random one. Flipbook Animation - The texture contains an animation starting at the top left, going to the right and down. Every particle starts with the first image, and over its lifetime will traverse through the frames to play the animation. Random Row, Animated Column - The texture contains NumSpritesY animations, each with NumSpritesX frames. Each particle plays one random animation over its lifetime. TintColorParam: An optional effect parameter name. If set, the parameter is used to tint the final color of the particle. Particle Stretch: Only available in the 'axis' render modes. Allows to stretch the particles along their fixed axis. Useful to create sparks. Mesh Renderer This renderer visualizes each particle using a mesh . Mesh: The mesh to use for rendering. Material: The material to use on the mesh. TintColorParam: An optional effect parameter name. If set, the parameter is used to tint the color of the meshes. ![[mesh-renderer.gif]] Light Renderer The light renderer treats each particle as a light source and thus illuminates the scene around it. Usually one would add this renderer as a second renderer to a particle system, such that one can easily reuse the behavior and color of for example billboards. Since light sources have a very high performance overhead, and adding as many lights into a scene as billboards would often be way too much, this renderer may visualize only a fraction of all particles in the system. SizeFactor: A factor to scale the particle's size with to determine the light influence radius. If the particles are also visualized as, for instance, billboards, the light source around the billboard should often be three to five times bigger. Intensity: The light intensity. Percentage: How many of the particles in the system should also be used as light sources. Typically you should only use 10% or so of the particles. Fewer lights is not only better for performance, it often also looks better, as there will be more contrast and flickering. Too many lights will result in constant brightness, making the effect less interesting. TintColorParam: An optional effect parameter name. If set, the parameter is used to tint the color of the lights. IntensityScaleParam: An optional effect parameter name. If set, the parameter is used to scale the intensity of the lights. SizeScaleParam: An optional effect parameter name. If set, the parameter is used to scale the light influence radius. ![[light-renderer.gif]] Trail Renderer This renderer visualizes particles as long lines that draw the path that the particle took. Trail particles are made up of a fixed number of segments. The more segments the renderer uses, the longer the trails will be. Also the faster a particle moves, the longer the trail will stretch. More segments cost more performance to update and render. For very short sparks that should just stretch a little, it is better to use billboards with Orientation set the Axis: Particle Dir and StretchFactor set to some value between 2 and 5. Render Mode, Texture, Texture Atlas, TintColorParam: These options are identical to the quad renderer . Segments: The number of segments to use for each particle. More segments cost more performance but also result in longer and more detailed trails. ![[trail-renderer.gif]] Effect Renderer This renderer visualizes each particle using another particle effect. The referenced effect is spawned at the position of each particle and then moved along as the particle moves. This allows you to create effects like fireworks, where each 'rocket' is represented by a single particle flying into the sky, but visualizes as a complete burning particle effect. Effect: The particle effect to spawn and move along for each particle in this system. Usually the other particle system would use at least one continuous emitter, such that the effect will be active for the whole lifetime of the particle that references it. RandomSeeed: An optional random seed to pass into the spawned effect. ![[effect-renderer.gif]] See Also Particle Effects Particle Initializers Particle Behaviors","title":"Particle Renderers"},{"location":"effects/particle-effects/particle-renderers/#particle-renderers","text":"","title":"Particle Renderers"},{"location":"effects/particle-effects/particle-renderers/#quad-renderer","text":"This renderer visualizes each particle as a quad. There are several modes to choose from how this quad is oriented. There are also different modes how to blend the particle with the background. This renderer is very versatile. Orientation: This mode defines how the quad is oriented and around which axis it may rotate. In the 'rotating' modes the quad geometry constantly rotates around some axis that is decided when the particle is spawned. In the 'fixed' modes the quads themselves will not rotate, but have a fixed plane decided when they spawn. If they have a non-zero rotation speed, their texture will rotate around the quads center. In the 'axis' modes, the quads have one fixed axis and one that orients itself into the direction of the camera. Billboard - This is the most common mode. Billboards always face the camera. If the camera moves around the effect, the billboards keep orienting towards it. When billboards should rotate, they always rotate in screen-space, meaning around the current forward axis of the camera. ![[billboards.gif]] Rotating: Ortho Emitter Dir - In this mode the quads rotate around the orthogonal axis between the direction of the emitter and the direction the particle moves into. This mode is particularly useful for simulating debris of concrete for bullet impacts and such. When the particles have a high rotation speed , they will fly off in a spectacular fashion. For this type of effect it is also best to use alpha-masked textures representing debris, and to use the 'Opaque' render mode . ![[quad-rotating-ortho.gif]] Rotating: Emitter Dir - Similar to the mode above, but uses the direction of the emitter as its rotation axis. This can be used for muzzle flashes (the part that shows along the direction of the barrel) or impact effects. ![[quad-rotating-emdir.gif]] Fixed: Emitter Dir - In this mode the quad always uses the emitter direction as its plane normal (with some optional Deviation ). This mode is useful for creating shockwave effects at a point of impact, or things like ripples in water. It can also be used for muzzle flashes (the part sideways out of a barrel). ![[quad-fixed-emdir.gif]] Fixed: World Up - Similar to Fixed: Emitter Dir but the axis used is always the world up direction. This can be useful for effects where the emitter may have an arbitrary direction, but the particles should always face upwards. For example for the ripples of water impact effects. Fixed: Random Dir - In this mode each quad gets a random axis assigned when it is spawned. This can be useful for creating shockwave effects in explosions. ![[random-color-init.gif]] Axis: Emitter Dir - In this mode the quads fixed axis is the direction of the emitter. The quad will rotate around this axis to face the camera as much as possible. Additionally, the quad will not scale around its center, but around one of its edges. Therefore, when scaling up or down, that edge will stay in a fixed position. This can be used to create muzzle flashes and other impact effects which should generally move into the direction of the emitter, but also face the camera as much as possible, to be well visible. This mode won't look too convincing if the particles' movement deviates strongly from the emitter direction. For most common use cases, the particles may not move at all, but only change their size. ![[quad-axis-emdir.gif]] Axis: Particle Dir - In this mode the quads fixed axis is its own fly direction. The quad will rotate around this axis to face the camera as much as possible. This is useful for creating sparks or laser blasts which shall stretch a little while moving into some direction. ![[continuous-emitter.gif]] Render Mode: This mode specifies how the color from the particle will be combined with the scene background. Opaque - The particle will use the alpha channel of the texture as mask. Pixels are either fully transparent or fully opaque. This is mostly useful for debris. Additive - The particle's color will be added to the background. The alpha-channel is not used. This is used for everything that should glow (magic spells, sparks, ...) Blended - The alpha channel of the texture is used to interpolate the particle's color with the background. This is used for everything that should be transparent, but not glowing (smoke and such). It is also the most tricky mode to make look good, as it depends the most on a good texture with a proper alpha channel. Particles rendered with this mode must be sorted by distance by the renderer, which incurs an additional performance cost. Blended Foreground/Background - Same as Blended but when there are multiple particle systems using Blended mode, this allows you to influence in which order the systems are rendered. This is used to fix rendering issues. Distortion - This is used to create a heat haze effect, which distorts the scene behind it. The alpha-channel of Texture is used to determine the shape of the distortion effect. The Distortion Texture and Distortion Strength are used to decide for each pixels how much to distort the background. Any texture can be used as the distortion texture, but the effect works best using a normal map . ![[render-mode-distortion.gif]] Texture: The texture used for rendering. May be a texture atlas or contain flipbook animations. Texture Atlas: Specifies how to interpret the content in Texture : None - The texture contains only a single image. Random Variations - The texture contains NumSpritesX x NumSpritesY images in a regular grid. Each image is independent and each particle uses a random one. Flipbook Animation - The texture contains an animation starting at the top left, going to the right and down. Every particle starts with the first image, and over its lifetime will traverse through the frames to play the animation. Random Row, Animated Column - The texture contains NumSpritesY animations, each with NumSpritesX frames. Each particle plays one random animation over its lifetime. TintColorParam: An optional effect parameter name. If set, the parameter is used to tint the final color of the particle. Particle Stretch: Only available in the 'axis' render modes. Allows to stretch the particles along their fixed axis. Useful to create sparks.","title":"Quad Renderer"},{"location":"effects/particle-effects/particle-renderers/#mesh-renderer","text":"This renderer visualizes each particle using a mesh . Mesh: The mesh to use for rendering. Material: The material to use on the mesh. TintColorParam: An optional effect parameter name. If set, the parameter is used to tint the color of the meshes. ![[mesh-renderer.gif]]","title":"Mesh Renderer"},{"location":"effects/particle-effects/particle-renderers/#light-renderer","text":"The light renderer treats each particle as a light source and thus illuminates the scene around it. Usually one would add this renderer as a second renderer to a particle system, such that one can easily reuse the behavior and color of for example billboards. Since light sources have a very high performance overhead, and adding as many lights into a scene as billboards would often be way too much, this renderer may visualize only a fraction of all particles in the system. SizeFactor: A factor to scale the particle's size with to determine the light influence radius. If the particles are also visualized as, for instance, billboards, the light source around the billboard should often be three to five times bigger. Intensity: The light intensity. Percentage: How many of the particles in the system should also be used as light sources. Typically you should only use 10% or so of the particles. Fewer lights is not only better for performance, it often also looks better, as there will be more contrast and flickering. Too many lights will result in constant brightness, making the effect less interesting. TintColorParam: An optional effect parameter name. If set, the parameter is used to tint the color of the lights. IntensityScaleParam: An optional effect parameter name. If set, the parameter is used to scale the intensity of the lights. SizeScaleParam: An optional effect parameter name. If set, the parameter is used to scale the light influence radius. ![[light-renderer.gif]]","title":"Light Renderer"},{"location":"effects/particle-effects/particle-renderers/#trail-renderer","text":"This renderer visualizes particles as long lines that draw the path that the particle took. Trail particles are made up of a fixed number of segments. The more segments the renderer uses, the longer the trails will be. Also the faster a particle moves, the longer the trail will stretch. More segments cost more performance to update and render. For very short sparks that should just stretch a little, it is better to use billboards with Orientation set the Axis: Particle Dir and StretchFactor set to some value between 2 and 5. Render Mode, Texture, Texture Atlas, TintColorParam: These options are identical to the quad renderer . Segments: The number of segments to use for each particle. More segments cost more performance but also result in longer and more detailed trails. ![[trail-renderer.gif]]","title":"Trail Renderer"},{"location":"effects/particle-effects/particle-renderers/#effect-renderer","text":"This renderer visualizes each particle using another particle effect. The referenced effect is spawned at the position of each particle and then moved along as the particle moves. This allows you to create effects like fireworks, where each 'rocket' is represented by a single particle flying into the sky, but visualizes as a complete burning particle effect. Effect: The particle effect to spawn and move along for each particle in this system. Usually the other particle system would use at least one continuous emitter, such that the effect will be active for the whole lifetime of the particle that references it. RandomSeeed: An optional random seed to pass into the spawned effect. ![[effect-renderer.gif]]","title":"Effect Renderer"},{"location":"effects/particle-effects/particle-renderers/#see-also","text":"Particle Effects Particle Initializers Particle Behaviors","title":"See Also"},{"location":"effects/post-processing/post-processing-component/","text":"Post-Processing Component The post processing component is used to dynamically modify settings of the render pipeline (TODO) . The component relies on volume components to define where in a level which values shall be used. The following image shows a scene without custom post processing values: With post processing, the area can be made to look very different: The level uses a volume component to specify that this area should have a different athmosphere and thus use different values for post processing. To enable custom post processing, attach the post processing component to the same object where the main camera component is attached to. This would typically be inside a player prefab. If, however, the post processing component is attached to an object with a camera component that is configured for render to texture (TODO) , it will only affect that. It is also possible to place this component simply anywhere in a level. In this case it is always active and affects the currently active camera. This can be very useful during testing, since it also affects the editor camera and thus you can test values and volume placement just by moving the editor camera around, without even simulating the scene. Component Properties Volume Type : A spatial category that is used as a filter to determine which volume components to use for looking up values. Mappings : An array of mappings from that configure which values in a render pipeline (TODO) to modify. See the images below for an example. Render Pass : The name of the render pass in the pipeline to change. Property : The name of the property to change. Volume Value : The name of the value to use from the volume . Default Value : The value to use for Property if the camera is currently in no volume. Interpolation Duration : The property is interpolated towards the target value over this time. A duration of 0 means the value changes immediately, anything larger means the change happens more smoothly. Example In this example, the post processing component is used to alter the Mood Color : The image below shows the Tonemapping render pass from the game's render pipeline (TODO) . On the right are it's properties that get modified. Be aware that once post processing component is used, the values for these properties on the render pipeline have no effect anymore, since they are always being overwritten anyway. Constant Overrides If you leave the Volume Value property for a mapping empty, the component overrides the render pipeline with the default value, but never reads a value from a volume. This can be used to just set a value to a different value in a level. This way you can also use this component just to have different values per level. In this case the component should not exist on the player object, but just be added to each level. See Also Volume Components Lighting Fog","title":"Post-Processing Component"},{"location":"effects/post-processing/post-processing-component/#post-processing-component","text":"The post processing component is used to dynamically modify settings of the render pipeline (TODO) . The component relies on volume components to define where in a level which values shall be used. The following image shows a scene without custom post processing values: With post processing, the area can be made to look very different: The level uses a volume component to specify that this area should have a different athmosphere and thus use different values for post processing. To enable custom post processing, attach the post processing component to the same object where the main camera component is attached to. This would typically be inside a player prefab. If, however, the post processing component is attached to an object with a camera component that is configured for render to texture (TODO) , it will only affect that. It is also possible to place this component simply anywhere in a level. In this case it is always active and affects the currently active camera. This can be very useful during testing, since it also affects the editor camera and thus you can test values and volume placement just by moving the editor camera around, without even simulating the scene.","title":"Post-Processing Component"},{"location":"effects/post-processing/post-processing-component/#component-properties","text":"Volume Type : A spatial category that is used as a filter to determine which volume components to use for looking up values. Mappings : An array of mappings from that configure which values in a render pipeline (TODO) to modify. See the images below for an example. Render Pass : The name of the render pass in the pipeline to change. Property : The name of the property to change. Volume Value : The name of the value to use from the volume . Default Value : The value to use for Property if the camera is currently in no volume. Interpolation Duration : The property is interpolated towards the target value over this time. A duration of 0 means the value changes immediately, anything larger means the change happens more smoothly.","title":"Component Properties"},{"location":"effects/post-processing/post-processing-component/#example","text":"In this example, the post processing component is used to alter the Mood Color : The image below shows the Tonemapping render pass from the game's render pipeline (TODO) . On the right are it's properties that get modified. Be aware that once post processing component is used, the values for these properties on the render pipeline have no effect anymore, since they are always being overwritten anyway.","title":"Example"},{"location":"effects/post-processing/post-processing-component/#constant-overrides","text":"If you leave the Volume Value property for a mapping empty, the component overrides the render pipeline with the default value, but never reads a value from a volume. This can be used to just set a value to a different value in a level. This way you can also use this component just to have different values per level. In this case the component should not exist on the player object, but just be added to each level.","title":"Constant Overrides"},{"location":"effects/post-processing/post-processing-component/#see-also","text":"Volume Components Lighting Fog","title":"See Also"},{"location":"effects/post-processing/volume-components/","text":"Volume Components Volume components are used to define custom environmental conditions in areas of a level. By itself, volume components have no functionality and no noticeable effect. They only specify an area and what values to use there. Values are usually specified by referencing a blackboard template asset . Other systems may use this information to implement behavior. One such system is the post-processing component which uses these volumes to modify parameters of the rendering pipeline, for example to have different color grading per area. A custom system could for example also use these volumes to determine whether the player is inside water. There are multiple volume components for different shapes: plVolumeBoxComponent plVolumeSphereComponent They only add options to define their shape, such as extents or radius, but do not differ in functionality. Component Properties All volume components share these properties: Type : A spatial category used for separating volumes that represent different things. This way one volume may be used to configure graphics settings, while other volumes may affect gameplay relevant functionality, and they don't accidentally interfere with each other, since the respective systems only get to see the volumes that are meant to affect them. SortOrder : In case two volumes overlap, the one with a higher sort order value has precedence. Template : A reference to a blackboard template asset to define the key/value pairs. It is usually more convenient to use a blackboard template as a preset for values, than to specify them directly on the volume component. Values : Individually added key/value pairs. Prefer to use a Template , but If the same key is also added here, it overrides the value from the template. Falloff : Volumes may have a soft edge , meaning that the boundary of the volume is not considered to be aprubt. This is used to smoothly fade values from one value to the one inside the volume. For example if a volume represents a foggy area, where the whole color grading is supposed to change, the colors are not supposed to change exactly the moment that the camera enters the volume, but rather the colors should become stronger the farther the camera is inside the volume. The falloff value is a value between 0 and 1 that configures how smooth the edge of the volume is. At 0 the edge is hard and change happens immediately. This is useful for example for water, where you are either inside or outside, but not in between. Any value above 0 is meant for smoother transitions. Be aware that additional to this, some systems may also transition to new values over time. To test the falloff value, it is best to deactivate any time delay. See Also Post-Processing Component","title":"Volume Components"},{"location":"effects/post-processing/volume-components/#volume-components","text":"Volume components are used to define custom environmental conditions in areas of a level. By itself, volume components have no functionality and no noticeable effect. They only specify an area and what values to use there. Values are usually specified by referencing a blackboard template asset . Other systems may use this information to implement behavior. One such system is the post-processing component which uses these volumes to modify parameters of the rendering pipeline, for example to have different color grading per area. A custom system could for example also use these volumes to determine whether the player is inside water. There are multiple volume components for different shapes: plVolumeBoxComponent plVolumeSphereComponent They only add options to define their shape, such as extents or radius, but do not differ in functionality.","title":"Volume Components"},{"location":"effects/post-processing/volume-components/#component-properties","text":"All volume components share these properties: Type : A spatial category used for separating volumes that represent different things. This way one volume may be used to configure graphics settings, while other volumes may affect gameplay relevant functionality, and they don't accidentally interfere with each other, since the respective systems only get to see the volumes that are meant to affect them. SortOrder : In case two volumes overlap, the one with a higher sort order value has precedence. Template : A reference to a blackboard template asset to define the key/value pairs. It is usually more convenient to use a blackboard template as a preset for values, than to specify them directly on the volume component. Values : Individually added key/value pairs. Prefer to use a Template , but If the same key is also added here, it overrides the value from the template. Falloff : Volumes may have a soft edge , meaning that the boundary of the volume is not considered to be aprubt. This is used to smoothly fade values from one value to the one inside the volume. For example if a volume represents a foggy area, where the whole color grading is supposed to change, the colors are not supposed to change exactly the moment that the camera enters the volume, but rather the colors should become stronger the farther the camera is inside the volume. The falloff value is a value between 0 and 1 that configures how smooth the edge of the volume is. At 0 the edge is hard and change happens immediately. This is useful for example for water, where you are either inside or outside, but not in between. Any value above 0 is meant for smoother transitions. Be aware that additional to this, some systems may also transition to new values over time. To test the falloff value, it is best to deactivate any time delay.","title":"Component Properties"},{"location":"effects/post-processing/volume-components/#see-also","text":"Post-Processing Component","title":"See Also"},{"location":"gameplay/area-damage-component/","text":"Area Damage Component The area damage component posts an plMsgDamage to all objects in its vicinity every time it is triggered. It may also send an plMsgPhysicsAddImpulse to push objects away from its location. This is used to implement the effect of explosions and other things that should damage close-by objects. Component Properties OnCreation : If enabled, the component will apply damage the moment it gets activated. Radius : The radius in which objects will receive damage. CollisionLayer : The physics collision layer to use to find objects to which to apply damage. Damage : The maximum amount of damage to apply. Damage is scaled down linearly by distance, so an object further away will receive less damage. Impulse : An optional physical impulse to apply to damaged objects. This will push objects away from this object. The applied impulse is also scaled down linearly by distance. Scripting ApplyAreaDamage() : This function can be called manually to control when and how often this component applies damage. For example a 'dangerous' area can be implemented by repeatedly triggering a component of this type. See Also Spawn Component Timed Death Component","title":"Area Damage Component"},{"location":"gameplay/area-damage-component/#area-damage-component","text":"The area damage component posts an plMsgDamage to all objects in its vicinity every time it is triggered. It may also send an plMsgPhysicsAddImpulse to push objects away from its location. This is used to implement the effect of explosions and other things that should damage close-by objects.","title":"Area Damage Component"},{"location":"gameplay/area-damage-component/#component-properties","text":"OnCreation : If enabled, the component will apply damage the moment it gets activated. Radius : The radius in which objects will receive damage. CollisionLayer : The physics collision layer to use to find objects to which to apply damage. Damage : The maximum amount of damage to apply. Damage is scaled down linearly by distance, so an object further away will receive less damage. Impulse : An optional physical impulse to apply to damaged objects. This will push objects away from this object. The applied impulse is also scaled down linearly by distance.","title":"Component Properties"},{"location":"gameplay/area-damage-component/#scripting","text":"ApplyAreaDamage() : This function can be called manually to control when and how often this component applies damage. For example a 'dangerous' area can be implemented by repeatedly triggering a component of this type.","title":"Scripting"},{"location":"gameplay/area-damage-component/#see-also","text":"Spawn Component Timed Death Component","title":"See Also"},{"location":"gameplay/grabbable-item-component/","text":"Grabbable Item Component The Grabbable Item component is used to define points on an object that are good anchor points to pick the object up. The component has no behavior by itself, it only holds data. Other components, such as the grab object component can utilize this information to improve the experience of picking an object up. Or a game may even only allow objects with this component to be grabbable in the first place. Both the position and rotation of each grab point are important. The rotation defines how the object will be oriented when it is picked up. Note: The component can be attached to any object, but for example the grab object component expects it to be attached to the same object as the dynamic actor component that has been picked through its raycast. Component Properties GrabPoints : An array holding all the grab points . Extend this array to add grab points. Use an item's manipulators to adjust the position and orientation of a grab point. See Also Jolt Grab Object Component","title":"Grabbable Item Component"},{"location":"gameplay/grabbable-item-component/#grabbable-item-component","text":"The Grabbable Item component is used to define points on an object that are good anchor points to pick the object up. The component has no behavior by itself, it only holds data. Other components, such as the grab object component can utilize this information to improve the experience of picking an object up. Or a game may even only allow objects with this component to be grabbable in the first place. Both the position and rotation of each grab point are important. The rotation defines how the object will be oriented when it is picked up. Note: The component can be attached to any object, but for example the grab object component expects it to be attached to the same object as the dynamic actor component that has been picked through its raycast.","title":"Grabbable Item Component"},{"location":"gameplay/grabbable-item-component/#component-properties","text":"GrabPoints : An array holding all the grab points . Extend this array to add grab points. Use an item's manipulators to adjust the position and orientation of a grab point.","title":"Component Properties"},{"location":"gameplay/grabbable-item-component/#see-also","text":"Jolt Grab Object Component","title":"See Also"},{"location":"gameplay/headbone-component/","text":"Head Bone Component This feature is in a usable state, but currently undocumented. See Also","title":"Head Bone Component"},{"location":"gameplay/headbone-component/#head-bone-component","text":"This feature is in a usable state, but currently undocumented.","title":"Head Bone Component"},{"location":"gameplay/headbone-component/#see-also","text":"","title":"See Also"},{"location":"gameplay/marker-component/","text":"Marker Component The marker component can be used to markup objects and locations with gameplay relevant semantical information. To implement game mechanics, especially some form of AI, your code must be able to reason about objects in the game. An AI for an NPC must be able to scan its nearby environment to detect objects that it can interact with, other NPCs, the player and locations that may be of interest. The spatial system is there to provide efficient means to do such queries. Using spatial queries, you can find all objects within an area that belong to some group. For this to work, you obviously need to insert objects into the spatial system. The marker component is a simple and convenient way to do so. All that the marker component does, is to insert a sphere of a given size and category into the spatial system, so that the object that the component is attached to, can be found with spatial queries. Note: Keep in mind that the number of categories available for use is limited to about 25. You should therefore prefer generic categories where possible. Properties Marker : Which spatial data category to use for this marker. Radius : The size of the marker. Examples The marker component can be used for many purposes. Here are a couple of examples: Tag NPCs and players. Tag objects that can be picked up. The position and rotation of the marker node can be used to orient the object when it is picked up. Set up points for visibility checks and for targeting when determining whether an NPC sees another NPC. Each character may have a 'target' node at its head, its torso, each elbow and knee. The AI can then do raycasts against all these points to determine whether a character is visible, and if so, shoot at one of the visible markers. Identify usable objects, such as buttons. The marker should be used to mark up that something is usable at that location, other mechanisms should be used to narrow down what the function is and how an AI could interact with it. Mark useful locations, for example good hiding spots, or sniper positions. Warn of dangerous locations. A grenade may have a large 'danger' marker attached, which informs NPCs to run away. The same can be used in front and behind vehicles, where they are enabled when a car starts driving, such that NPCs will get out of its way. For an example how marker components and spatial queries can be used to find nearby objects, have a look at the Sample Game Plugin . See Also Spatial System Sample Game Plugin Tags","title":"Marker Component"},{"location":"gameplay/marker-component/#marker-component","text":"The marker component can be used to markup objects and locations with gameplay relevant semantical information. To implement game mechanics, especially some form of AI, your code must be able to reason about objects in the game. An AI for an NPC must be able to scan its nearby environment to detect objects that it can interact with, other NPCs, the player and locations that may be of interest. The spatial system is there to provide efficient means to do such queries. Using spatial queries, you can find all objects within an area that belong to some group. For this to work, you obviously need to insert objects into the spatial system. The marker component is a simple and convenient way to do so. All that the marker component does, is to insert a sphere of a given size and category into the spatial system, so that the object that the component is attached to, can be found with spatial queries. Note: Keep in mind that the number of categories available for use is limited to about 25. You should therefore prefer generic categories where possible.","title":"Marker Component"},{"location":"gameplay/marker-component/#properties","text":"Marker : Which spatial data category to use for this marker. Radius : The size of the marker.","title":"Properties"},{"location":"gameplay/marker-component/#examples","text":"The marker component can be used for many purposes. Here are a couple of examples: Tag NPCs and players. Tag objects that can be picked up. The position and rotation of the marker node can be used to orient the object when it is picked up. Set up points for visibility checks and for targeting when determining whether an NPC sees another NPC. Each character may have a 'target' node at its head, its torso, each elbow and knee. The AI can then do raycasts against all these points to determine whether a character is visible, and if so, shoot at one of the visible markers. Identify usable objects, such as buttons. The marker should be used to mark up that something is usable at that location, other mechanisms should be used to narrow down what the function is and how an AI could interact with it. Mark useful locations, for example good hiding spots, or sniper positions. Warn of dangerous locations. A grenade may have a large 'danger' marker attached, which informs NPCs to run away. The same can be used in front and behind vehicles, where they are enabled when a car starts driving, such that NPCs will get out of its way. For an example how marker components and spatial queries can be used to find nearby objects, have a look at the Sample Game Plugin .","title":"Examples"},{"location":"gameplay/marker-component/#see-also","text":"Spatial System Sample Game Plugin Tags","title":"See Also"},{"location":"gameplay/player-start-point/","text":"Player Start Point The Player Start Point component is used to indicate a position in a level from where the game should start playing. The component references a prefab which represents the player object. This prefab must be built such that it handles input and implements the desired player movement and interactions. When the game is run either using Play-the-Game mode or stand-alone , it will execute its game state . The default game state implementation will look for a player start component and instantiate the referenced prefab. This is most useful for games where a specific object represents the player. For games that do not have a player presence, such as RTS games, the custom game state should ignore this type of component and instead implement the player interaction logic itself. When a scene contains a player start point component, you can use the Play From Here feature. See Also Running a Scene plPlayer","title":"Player Start Point"},{"location":"gameplay/player-start-point/#player-start-point","text":"The Player Start Point component is used to indicate a position in a level from where the game should start playing. The component references a prefab which represents the player object. This prefab must be built such that it handles input and implements the desired player movement and interactions. When the game is run either using Play-the-Game mode or stand-alone , it will execute its game state . The default game state implementation will look for a player start component and instantiate the referenced prefab. This is most useful for games where a specific object represents the player. For games that do not have a player presence, such as RTS games, the custom game state should ignore this type of component and instead implement the player interaction logic itself. When a scene contains a player start point component, you can use the Play From Here feature.","title":"Player Start Point"},{"location":"gameplay/player-start-point/#see-also","text":"Running a Scene plPlayer","title":"See Also"},{"location":"gameplay/projectile-component/","text":"Projectile Component The projectile component makes an object move along a straight line (with optional gravity), checks for collisions and triggers surface interactions . It also applies damage and a physical impulse when it hits something. There are many ways projectiles may work in different games. The projectile component only implements the most commonly needed functionality. For some games this may already be sufficient. For more creative games you most certainly need to implement your own component. The built-in projectile component is mostly meant as a showcase and demonstration, how you can generally implement such functionality, especially how to interact with surfaces . Component Properties Speed : The speed (m/sec) at which the projectile will fly along the +X axis. For dramatic effect it is often more interesting for bullets to fly much slower than is realistic, so that one can see them. GravityMultiplier : How strong gravity should affect the bullet. If set to 0, the projectile will fly straight ahead. With values > 0, it will fall downwards. MaxLifetime : If the projectile lives longer than this, it will destroy itself. OnTimeoutSpawn : If the projectile destroys itself because it didn't hit anything before MaxLifetime ended, it will spawn this prefab. Can be used to let rockets 'explode' after a while. CollisionLayer : The physics collision layer to use for raycasting whether the projectile hit something. FallbackSurface : If the projectile hits something that has no surface assigned, it will assume that this type of surface was hit. This just guarantees that you always get any kind of hit response, even when the hit geometry is just dummy or placeholder geometry. Interactions : This array allows you to specify exactly what the projectile will do, when it hits different types of surfaces. Each entry is used to configure the action of the projectile if a certain type of surface is hit. Note that surfaces and surface interactions already work hierarchically. That means for most types of projectiles this array only needs to contain a single entry, with a base surface, and all it needs to specify is what 'interaction' to trigger. The exact type of surface that is hit, will then decide what prefabs to spawn for that kind of interaction. Interaction Properties Surface : The (base) type of surface for which this array element applies. Reaction : How the projectile itself should react. Absorb means it will stop there, Reflect means the bullet will bounce off, Attach means the bullet will stop but attach itself to the target object and Pass Through means it will continue on its path. Interaction : The surface interaction to trigger when the projectile hits this type of surface. Impulse : The amount of physical impulse to exert on the hit object. Damage : The amount of damage to apply to the hit object. See Also Surfaces Collision Layers Particle Effects","title":"Projectile Component"},{"location":"gameplay/projectile-component/#projectile-component","text":"The projectile component makes an object move along a straight line (with optional gravity), checks for collisions and triggers surface interactions . It also applies damage and a physical impulse when it hits something. There are many ways projectiles may work in different games. The projectile component only implements the most commonly needed functionality. For some games this may already be sufficient. For more creative games you most certainly need to implement your own component. The built-in projectile component is mostly meant as a showcase and demonstration, how you can generally implement such functionality, especially how to interact with surfaces .","title":"Projectile Component"},{"location":"gameplay/projectile-component/#component-properties","text":"Speed : The speed (m/sec) at which the projectile will fly along the +X axis. For dramatic effect it is often more interesting for bullets to fly much slower than is realistic, so that one can see them. GravityMultiplier : How strong gravity should affect the bullet. If set to 0, the projectile will fly straight ahead. With values > 0, it will fall downwards. MaxLifetime : If the projectile lives longer than this, it will destroy itself. OnTimeoutSpawn : If the projectile destroys itself because it didn't hit anything before MaxLifetime ended, it will spawn this prefab. Can be used to let rockets 'explode' after a while. CollisionLayer : The physics collision layer to use for raycasting whether the projectile hit something. FallbackSurface : If the projectile hits something that has no surface assigned, it will assume that this type of surface was hit. This just guarantees that you always get any kind of hit response, even when the hit geometry is just dummy or placeholder geometry. Interactions : This array allows you to specify exactly what the projectile will do, when it hits different types of surfaces. Each entry is used to configure the action of the projectile if a certain type of surface is hit. Note that surfaces and surface interactions already work hierarchically. That means for most types of projectiles this array only needs to contain a single entry, with a base surface, and all it needs to specify is what 'interaction' to trigger. The exact type of surface that is hit, will then decide what prefabs to spawn for that kind of interaction.","title":"Component Properties"},{"location":"gameplay/projectile-component/#interaction-properties","text":"Surface : The (base) type of surface for which this array element applies. Reaction : How the projectile itself should react. Absorb means it will stop there, Reflect means the bullet will bounce off, Attach means the bullet will stop but attach itself to the target object and Pass Through means it will continue on its path. Interaction : The surface interaction to trigger when the projectile hits this type of surface. Impulse : The amount of physical impulse to exert on the hit object. Damage : The amount of damage to apply to the hit object.","title":"Interaction Properties"},{"location":"gameplay/projectile-component/#see-also","text":"Surfaces Collision Layers Particle Effects","title":"See Also"},{"location":"gameplay/raycast-placement-component/","text":"Raycast Placement Component The raycast placement component does a ray cast and positions a target object there. The image below shows raycast placement components being used together with a beam components to create laser beams. This component does a ray cast along the forward axis of the game object it is attached to. If this produces a hit, the target object is placed there. If no hit is found the target object is either placed at the maximum distance or deactivated depending on the component configuration. This component can also trigger messages when objects enter the ray. E.g. when a player trips a laser detection beam. To enable this set the trigger collision layer to another layer than the main ray cast and set a trigger message. Sample setup: CollisionLayerEndPoint = Default CollisionLayerTrigger = Player TriggerMessage = \"APlayerEnteredTheBeam\" This will lead to trigger messages being sent when a physics actor on the 'Player' layer comes between the original hit on the default layer and the ray cast origin. Component Properties MaxDistance : The maximum distance to do the raycast. DisableTargetObjectOnNoHit : If set, the RaycastEndObject is set to inactive when the raycast hits nothing within MaxDistance . This can be used to for things like laser pointers, where the target object represents the 'laser dot'. If the laser pointer hits nothing, the laser dot object should temporarily disappear. Once the raycast hits something again, the component will make sure to reactivate the target object again. RaycastEndObject : A referenced object that this component should affect. Every time the placement component determines a different position for the raycast hit, it will move the referenced object there. ForceTargetParentless : If set, the placement component will make sure that the referenced RaycastEndObject will be detached from any parent object. The practical reason for this is, that to prevent multiple objects from modifying the position of the end object, it should have no parent game object, which may pass down its own transform changes. However, when the end object is part of a prefab, it will always have a parent, and that parent may need to move. For example when a weapon is attached to a character controller. Therefore the placement component can take care of detaching the end object at the appropriate time. CollisionLayerEndPoint : The collision layer to use for the raycast to detect where the RaycastEndObject should be placed. CollisionLayerTrigger : An optional, different collision layer to detect whether a specific type of object is closer than the placed end object. If this is the case, the event plMsgTriggerTriggered is raised, using the identifier given in TriggerMessage . This can be used for things like trip mines, where the first collision layer detects how far away the closest wall is, and the second collision layer checks whether any player or NPC has come between the mine and the opposing wall. TriggerMessage : The trigger identifier message to use when CollisionLayerTrigger detects an object. See Also Beam Component Trigger Component","title":"Raycast Placement Component"},{"location":"gameplay/raycast-placement-component/#raycast-placement-component","text":"The raycast placement component does a ray cast and positions a target object there. The image below shows raycast placement components being used together with a beam components to create laser beams. This component does a ray cast along the forward axis of the game object it is attached to. If this produces a hit, the target object is placed there. If no hit is found the target object is either placed at the maximum distance or deactivated depending on the component configuration. This component can also trigger messages when objects enter the ray. E.g. when a player trips a laser detection beam. To enable this set the trigger collision layer to another layer than the main ray cast and set a trigger message. Sample setup: CollisionLayerEndPoint = Default CollisionLayerTrigger = Player TriggerMessage = \"APlayerEnteredTheBeam\" This will lead to trigger messages being sent when a physics actor on the 'Player' layer comes between the original hit on the default layer and the ray cast origin.","title":"Raycast Placement Component"},{"location":"gameplay/raycast-placement-component/#component-properties","text":"MaxDistance : The maximum distance to do the raycast. DisableTargetObjectOnNoHit : If set, the RaycastEndObject is set to inactive when the raycast hits nothing within MaxDistance . This can be used to for things like laser pointers, where the target object represents the 'laser dot'. If the laser pointer hits nothing, the laser dot object should temporarily disappear. Once the raycast hits something again, the component will make sure to reactivate the target object again. RaycastEndObject : A referenced object that this component should affect. Every time the placement component determines a different position for the raycast hit, it will move the referenced object there. ForceTargetParentless : If set, the placement component will make sure that the referenced RaycastEndObject will be detached from any parent object. The practical reason for this is, that to prevent multiple objects from modifying the position of the end object, it should have no parent game object, which may pass down its own transform changes. However, when the end object is part of a prefab, it will always have a parent, and that parent may need to move. For example when a weapon is attached to a character controller. Therefore the placement component can take care of detaching the end object at the appropriate time. CollisionLayerEndPoint : The collision layer to use for the raycast to detect where the RaycastEndObject should be placed. CollisionLayerTrigger : An optional, different collision layer to detect whether a specific type of object is closer than the placed end object. If this is the case, the event plMsgTriggerTriggered is raised, using the identifier given in TriggerMessage . This can be used for things like trip mines, where the first collision layer detects how far away the closest wall is, and the second collision layer checks whether any player or NPC has come between the mine and the opposing wall. TriggerMessage : The trigger identifier message to use when CollisionLayerTrigger detects an object.","title":"Component Properties"},{"location":"gameplay/raycast-placement-component/#see-also","text":"Beam Component Trigger Component","title":"See Also"},{"location":"gameplay/spawn-component/","text":"Spawn Component The SpawnComponent is frequently used to spawn instances of prefabs at runtime. The component references a prefab. It can then either spawn this prefab continuously in intervals, or at will by triggering the spawn command from (script) code. Properties Prefab: The prefab that will be spawned by this component. AttachAsChild: If true, the spawned object will be attached to the owner of the SpawnComponent. In most cases this should be disabled. SpawnAtStart: If true, the SpawnComponent will spawn the prefab immediately when it gets activated. SpawnContinuously: If true, the component will continue to spawn more and more prefab instances. This can only be stopped either through custom (script) code or by deleting the spawn component. MinDelay, DelayRange: The minimum time that has to pass before the component will spawn another instance, plus some random delay. This not only applies to the SpawnContinuously case, but also to cases where the spawn may be triggered from code. Meaning, this can be used to limit how often an action is allowed. For example, a gun can use a SpawnComponent to launch projectiles, and the gun code can simply trigger the SpawnComponent every time the user clicks. However, due to the MinDelay , the gun will only fire every once in a while, without having to write that logic in the gun code. Deviation: When a new prefab instance is created, it will be positioned at the location of the SpawnComponent. The Deviation allows you to add a random rotation away from the X axis. In Plasma most components use the +X axis as their main axis of operation. For instance, projectiles fly along +X, spot lights point into +X direction, etc. Therefore the SpawnComponent tilts the new instances away from the +X axis and all prefabs should be authored to work with this accordingly. Details See the API Docs for plSpawnComponent for the component's interface. See Also Prefabs","title":"Spawn Component"},{"location":"gameplay/spawn-component/#spawn-component","text":"The SpawnComponent is frequently used to spawn instances of prefabs at runtime. The component references a prefab. It can then either spawn this prefab continuously in intervals, or at will by triggering the spawn command from (script) code.","title":"Spawn Component"},{"location":"gameplay/spawn-component/#properties","text":"Prefab: The prefab that will be spawned by this component. AttachAsChild: If true, the spawned object will be attached to the owner of the SpawnComponent. In most cases this should be disabled. SpawnAtStart: If true, the SpawnComponent will spawn the prefab immediately when it gets activated. SpawnContinuously: If true, the component will continue to spawn more and more prefab instances. This can only be stopped either through custom (script) code or by deleting the spawn component. MinDelay, DelayRange: The minimum time that has to pass before the component will spawn another instance, plus some random delay. This not only applies to the SpawnContinuously case, but also to cases where the spawn may be triggered from code. Meaning, this can be used to limit how often an action is allowed. For example, a gun can use a SpawnComponent to launch projectiles, and the gun code can simply trigger the SpawnComponent every time the user clicks. However, due to the MinDelay , the gun will only fire every once in a while, without having to write that logic in the gun code. Deviation: When a new prefab instance is created, it will be positioned at the location of the SpawnComponent. The Deviation allows you to add a random rotation away from the X axis. In Plasma most components use the +X axis as their main axis of operation. For instance, projectiles fly along +X, spot lights point into +X direction, etc. Therefore the SpawnComponent tilts the new instances away from the +X axis and all prefabs should be authored to work with this accordingly.","title":"Properties"},{"location":"gameplay/spawn-component/#details","text":"See the API Docs for plSpawnComponent for the component's interface.","title":"Details"},{"location":"gameplay/spawn-component/#see-also","text":"Prefabs","title":"See Also"},{"location":"gameplay/timed-death-component/","text":"Timed Death Component The timed death component is used to automatically delete the object that it is attached to, after a timeout. Additionally, it may spawn a prefab when its timeout has been reached. Component Properties MinDelay : The minimum time to wait before deleting the parent object. DelayRange : An optional random range to wait. If this is set to zero, the component will execute exactly after a delay of MinDelay . TimeoutPrefab : If the component is triggered to delete the object, it may additionally spawn an instance of the selected prefab, at the location of the object. See Also Spawn Component","title":"Timed Death Component"},{"location":"gameplay/timed-death-component/#timed-death-component","text":"The timed death component is used to automatically delete the object that it is attached to, after a timeout. Additionally, it may spawn a prefab when its timeout has been reached.","title":"Timed Death Component"},{"location":"gameplay/timed-death-component/#component-properties","text":"MinDelay : The minimum time to wait before deleting the parent object. DelayRange : An optional random range to wait. If this is set to zero, the component will execute exactly after a delay of MinDelay . TimeoutPrefab : If the component is triggered to delete the object, it may additionally spawn an instance of the selected prefab, at the location of the object.","title":"Component Properties"},{"location":"gameplay/timed-death-component/#see-also","text":"Spawn Component","title":"See Also"},{"location":"graphics/always-visible-component/","text":"Always Visible Component The always visible component makes the renderer consider the object that this component is attached to as always in view. This effectively disables culling optimizations. This can be used to enforce rendering of an object at all times, even when it is outside the view. The need for this is extremely rare, though. See Also","title":"Always Visible Component"},{"location":"graphics/always-visible-component/#always-visible-component","text":"The always visible component makes the renderer consider the object that this component is attached to as always in view. This effectively disables culling optimizations. This can be used to enforce rendering of an object at all times, even when it is outside the view. The need for this is extremely rare, though.","title":"Always Visible Component"},{"location":"graphics/always-visible-component/#see-also","text":"","title":"See Also"},{"location":"graphics/camera-component/","text":"Camera Component The camera component is used to tell the renderer from which position and with which settings to render the scene. Apart from the component that acts as the main camera , there can be additional cameras in active use for render to texture (TODO) effects. Additionally, camera components can be used in the editor as 'bookmarks' to be able to quickly jump to specific positions in a level. When an object with a camera component is selected, the editor shows a preview of what the camera sees in the top left corner. Main Camera The camera settings that are used for rendering the scene are fully under control of the game state . Every frame it decides where to place the main camera and with which settings. At this point, no camera component is involved, your game may control the main camera without having any camera component in the scene. However, the default behavior of the game state (see plFallbackGameState ), is to check the scene for a camera component that has its UsageHint set to Main View . Unless you write a custom game state and override this behavior, the game state will simply copy all the camera settings from the first camera component that it can find with this usage hint. Consequently, you can control the main camera, by placing a component and setting its UsageHint to Main View . If you want a different camera component to take over from the current one, you need to change the usage hint on those camera components. Important: The plFallbackGameState is mostly for development and therefore has other convenience features for cameras. For example, you can switch through cameras in the scene using Page Up and Page Down . If you release a game, you should make sure to disable this behavior. Other Cameras A scene can contain any number of camera components. Unless they are referenced by other systems, they won't do anything and will have no performance impact. Camera Bookmarks Camera components can be placed as 'bookmarks', such that people working on a scene can quickly move the editor camera to areas of interest. This chapter describes how to do so. Include/Exclude Tags By default a camera renders all objects in the scene. Sometimes it can be desirable, though, for a camera to render only specific objects, or to ignore those. For example you may have descriptive labels attached to some objects, which the player can display on demand. Using the inclusion and exclusion tags on the camera, you can control which objects are going to be considered for rendering from this camera view. If any inclusion tag is set, only objects with any of these tags are rendered. If an exclusion tag is set, no object with any of these tags is rendered. Important: Don't forget that tags are not inherited . You can't hide an object by setting a tag on its parent node. So for the example with the object labels above, you would assign a 'label' tag to those objects and on your camera you would set 'label' as an exclusion tag. This way those objects are not rendered. When the player wants to see the labels, you would simply remove the exclusion tag from the camera, to make them appear. Important: Especially when using include tags , be aware that not only meshes, but also light sources (and everything else that's part of the rendering process) are affected by this. If you forget to set the necessary include tags on your light sources, the output will stay dark. Render to Texture Camera components can be used to render their view to a texture, which can then be referenced by a material and displayed on any mesh . To enable this mode, the UsageHint has to be set to Render to Texture . You also need to select a CameraRenderPipeline . The render pipeline defines how the scene is rendered and which rendering effects are applied. You need to configure which render pipelines (TODO) are available to the cameras in the asset profiles (TODO) . The RenderTargetOffset and RenderTargetSize allow you to render only to a part of the texture. Note that rendering to a texture involves additional steps. See the chapter about render-to-texture (TODO) for full instructions. Component Properties EditorShortcut : Used to configure level cameras . UsageHint : A hint what the camera is supposed to be used for. Systems like the game state may use this information to use or ignore this component. Mode , FOV , Dimensions : Configure whether this is a perspective or an orthographic view and how the other options are applied. Field-of-view (FOV) is used for perspective modes, dimensions are used for orthographic modes. NearPlane , FarPlane : The distances for the near and far plane. For best performance keep the far plane distance as low as possible. To prevent z-fighting make sure that the near plane is not too close and the far plane is not too far out. IncludeTags , ExcludeTags : See Include/Exclude Tags above. CameraRenderPipeline : Allows you to select a specific render pipeline (TODO) that shall be used to render the output from this camera. Available render pipelines are set up in the asset profiles (TODO) . RenderTarget , RenderTargetOffset , RenderTargetSize : Only available when UsageHint is set to Render to Texture . Aperture , ShutterTime , ISO , ExposureCompensation : These options are currently only used for tonemapping . They all affect the final exposure value, which means you can adjust any one of them to change the brightness of the output. In the future these values may be used for motion blur and depth-of-field. See Also Editor Camera Render to Texture (TODO) Render Pipeline (TODO) Asset Profiles (TODO) Tags","title":"Camera Component"},{"location":"graphics/camera-component/#camera-component","text":"The camera component is used to tell the renderer from which position and with which settings to render the scene. Apart from the component that acts as the main camera , there can be additional cameras in active use for render to texture (TODO) effects. Additionally, camera components can be used in the editor as 'bookmarks' to be able to quickly jump to specific positions in a level. When an object with a camera component is selected, the editor shows a preview of what the camera sees in the top left corner.","title":"Camera Component"},{"location":"graphics/camera-component/#main-camera","text":"The camera settings that are used for rendering the scene are fully under control of the game state . Every frame it decides where to place the main camera and with which settings. At this point, no camera component is involved, your game may control the main camera without having any camera component in the scene. However, the default behavior of the game state (see plFallbackGameState ), is to check the scene for a camera component that has its UsageHint set to Main View . Unless you write a custom game state and override this behavior, the game state will simply copy all the camera settings from the first camera component that it can find with this usage hint. Consequently, you can control the main camera, by placing a component and setting its UsageHint to Main View . If you want a different camera component to take over from the current one, you need to change the usage hint on those camera components. Important: The plFallbackGameState is mostly for development and therefore has other convenience features for cameras. For example, you can switch through cameras in the scene using Page Up and Page Down . If you release a game, you should make sure to disable this behavior.","title":"Main Camera"},{"location":"graphics/camera-component/#other-cameras","text":"A scene can contain any number of camera components. Unless they are referenced by other systems, they won't do anything and will have no performance impact.","title":"Other Cameras"},{"location":"graphics/camera-component/#camera-bookmarks","text":"Camera components can be placed as 'bookmarks', such that people working on a scene can quickly move the editor camera to areas of interest. This chapter describes how to do so.","title":"Camera Bookmarks"},{"location":"graphics/camera-component/#includeexclude-tags","text":"By default a camera renders all objects in the scene. Sometimes it can be desirable, though, for a camera to render only specific objects, or to ignore those. For example you may have descriptive labels attached to some objects, which the player can display on demand. Using the inclusion and exclusion tags on the camera, you can control which objects are going to be considered for rendering from this camera view. If any inclusion tag is set, only objects with any of these tags are rendered. If an exclusion tag is set, no object with any of these tags is rendered. Important: Don't forget that tags are not inherited . You can't hide an object by setting a tag on its parent node. So for the example with the object labels above, you would assign a 'label' tag to those objects and on your camera you would set 'label' as an exclusion tag. This way those objects are not rendered. When the player wants to see the labels, you would simply remove the exclusion tag from the camera, to make them appear. Important: Especially when using include tags , be aware that not only meshes, but also light sources (and everything else that's part of the rendering process) are affected by this. If you forget to set the necessary include tags on your light sources, the output will stay dark.","title":"Include/Exclude Tags"},{"location":"graphics/camera-component/#render-to-texture","text":"Camera components can be used to render their view to a texture, which can then be referenced by a material and displayed on any mesh . To enable this mode, the UsageHint has to be set to Render to Texture . You also need to select a CameraRenderPipeline . The render pipeline defines how the scene is rendered and which rendering effects are applied. You need to configure which render pipelines (TODO) are available to the cameras in the asset profiles (TODO) . The RenderTargetOffset and RenderTargetSize allow you to render only to a part of the texture. Note that rendering to a texture involves additional steps. See the chapter about render-to-texture (TODO) for full instructions.","title":"Render to Texture"},{"location":"graphics/camera-component/#component-properties","text":"EditorShortcut : Used to configure level cameras . UsageHint : A hint what the camera is supposed to be used for. Systems like the game state may use this information to use or ignore this component. Mode , FOV , Dimensions : Configure whether this is a perspective or an orthographic view and how the other options are applied. Field-of-view (FOV) is used for perspective modes, dimensions are used for orthographic modes. NearPlane , FarPlane : The distances for the near and far plane. For best performance keep the far plane distance as low as possible. To prevent z-fighting make sure that the near plane is not too close and the far plane is not too far out. IncludeTags , ExcludeTags : See Include/Exclude Tags above. CameraRenderPipeline : Allows you to select a specific render pipeline (TODO) that shall be used to render the output from this camera. Available render pipelines are set up in the asset profiles (TODO) . RenderTarget , RenderTargetOffset , RenderTargetSize : Only available when UsageHint is set to Render to Texture . Aperture , ShutterTime , ISO , ExposureCompensation : These options are currently only used for tonemapping . They all affect the final exposure value, which means you can adjust any one of them to change the brightness of the output. In the future these values may be used for motion blur and depth-of-field.","title":"Component Properties"},{"location":"graphics/camera-component/#see-also","text":"Editor Camera Render to Texture (TODO) Render Pipeline (TODO) Asset Profiles (TODO) Tags","title":"See Also"},{"location":"graphics/lod-component/","text":"LOD Component The Level-of-Detail (LOD) component switches child objects named LODn (with n from 0 to 4) on and off, depending on how close this object is to the main camera. The LOD component is used to reduce the performance impact of complex objects when they are far away. To do so, a LOD object consists of multiple states, from highly detailed to very coarse. By convention the highly detailed object is called LOD0 and the lesser detailed objects are called LOD1 , LOD2 , LOD3 and finally LOD4 . This component calculates how large it roughly appears on screen at it's current distance. This is called the coverage value. Using user defined coverage thresholds, it then selects which LOD child object to activate. All others get deactivated. The LODs have to be direct child objects called LOD0 to LOD4 . Other child objects are not affected. How many LODs are used depends on the number of elements in the LodThresholds array. The array describes up to which coverage value each LOD is used. Thus if it contains one value, two LODs will be used, LOD0 for coverage values above the specified threshold, and LOD1 at lower coverage values. To see the current coverage, enable the debug drawing. The coverage calculation uses spherical bounds. It should be configured to encompass the geometry of all LODs. To prevent LODs switching back and forth at one exact boundary, the LOD ranges may overlap by a fixed percentage. This way once one LOD gets activated, the coverage value has to change back quite a bit, before the previous LOD gets activated. Since this behavior can make it harder to set up the LOD thresholds, it can be deactivated, but in practice it should stay enabled. Component Properties BoundsOffset , BoundsRadius : Define the bounding sphere for the LOD object. This should be setup such that it surrounds all visible child geometry of all LODs, otherwise the component may not properly switch states when the object is partially visible. These bounds are also used to compute the on-screen coverage. ShowDebugInfo : If enabled, the LOD component draws information about the current screen coverage, selected LOD and coverage ranges. This should be used to determine at which coverage values a LOD switch should happen. OverlapRanges : Disable this to have the LOD switch take places exactly at the configured thresholds. This helps during setup. Keep this enabled during normal operation. It means that the LOD threshold ranges overlap, so that once the component switches from LOD n to n+1 , it won't immediately switch back to n when the camera takes a small step back. This is to prevent obvious back and forth. LodThresholds : An array of up to 4 threshold values. Each value is the coverage threshold up until which the LODn is used. So if element 0 is set to 0.2, that means LOD0 will be used up to a coverage value of 0.2 and at 0.199 it will switch to LOD1 . Up to five different LODs are allowed. If there is a LOD threshold, but no child node with the corresponding name, this disable all LODs, which can be used to hide the object entirely. See Also Always Visible Component Profiling","title":"LOD Component"},{"location":"graphics/lod-component/#lod-component","text":"The Level-of-Detail (LOD) component switches child objects named LODn (with n from 0 to 4) on and off, depending on how close this object is to the main camera. The LOD component is used to reduce the performance impact of complex objects when they are far away. To do so, a LOD object consists of multiple states, from highly detailed to very coarse. By convention the highly detailed object is called LOD0 and the lesser detailed objects are called LOD1 , LOD2 , LOD3 and finally LOD4 . This component calculates how large it roughly appears on screen at it's current distance. This is called the coverage value. Using user defined coverage thresholds, it then selects which LOD child object to activate. All others get deactivated. The LODs have to be direct child objects called LOD0 to LOD4 . Other child objects are not affected. How many LODs are used depends on the number of elements in the LodThresholds array. The array describes up to which coverage value each LOD is used. Thus if it contains one value, two LODs will be used, LOD0 for coverage values above the specified threshold, and LOD1 at lower coverage values. To see the current coverage, enable the debug drawing. The coverage calculation uses spherical bounds. It should be configured to encompass the geometry of all LODs. To prevent LODs switching back and forth at one exact boundary, the LOD ranges may overlap by a fixed percentage. This way once one LOD gets activated, the coverage value has to change back quite a bit, before the previous LOD gets activated. Since this behavior can make it harder to set up the LOD thresholds, it can be deactivated, but in practice it should stay enabled.","title":"LOD Component"},{"location":"graphics/lod-component/#component-properties","text":"BoundsOffset , BoundsRadius : Define the bounding sphere for the LOD object. This should be setup such that it surrounds all visible child geometry of all LODs, otherwise the component may not properly switch states when the object is partially visible. These bounds are also used to compute the on-screen coverage. ShowDebugInfo : If enabled, the LOD component draws information about the current screen coverage, selected LOD and coverage ranges. This should be used to determine at which coverage values a LOD switch should happen. OverlapRanges : Disable this to have the LOD switch take places exactly at the configured thresholds. This helps during setup. Keep this enabled during normal operation. It means that the LOD threshold ranges overlap, so that once the component switches from LOD n to n+1 , it won't immediately switch back to n when the camera takes a small step back. This is to prevent obvious back and forth. LodThresholds : An array of up to 4 threshold values. Each value is the coverage threshold up until which the LODn is used. So if element 0 is set to 0.2, that means LOD0 will be used up to a coverage value of 0.2 and at 0.199 it will switch to LOD1 . Up to five different LODs are allowed. If there is a LOD threshold, but no child node with the corresponding name, this disable all LODs, which can be used to hide the object entirely.","title":"Component Properties"},{"location":"graphics/lod-component/#see-also","text":"Always Visible Component Profiling","title":"See Also"},{"location":"graphics/occluder-component/","text":"Occluder Component The occluder component is used to add invisible geometry to a scene that is only used for occlusion culling . Currently the occluder component always uses a box shape. Contrary to greybox geometry , the occluder component itself is invisible. Enable occluder geometry visualization to see it in action. Occluders can be moved around dynamically, so you can attach it to a door and it will properly occlude objects when the door closes. You can also (de-)activate the entire component programmatically. For example a breakable wall can use an occluder as long as it is intact, and deactivate it when the wall breaks. Component Properties Extents : The size of the box. See Also Occlusion Culling Greyboxing","title":"Occluder Component"},{"location":"graphics/occluder-component/#occluder-component","text":"The occluder component is used to add invisible geometry to a scene that is only used for occlusion culling . Currently the occluder component always uses a box shape. Contrary to greybox geometry , the occluder component itself is invisible. Enable occluder geometry visualization to see it in action. Occluders can be moved around dynamically, so you can attach it to a door and it will properly occlude objects when the door closes. You can also (de-)activate the entire component programmatically. For example a breakable wall can use an occluder as long as it is intact, and deactivate it when the wall breaks.","title":"Occluder Component"},{"location":"graphics/occluder-component/#component-properties","text":"Extents : The size of the box.","title":"Component Properties"},{"location":"graphics/occluder-component/#see-also","text":"Occlusion Culling Greyboxing","title":"See Also"},{"location":"graphics/render-pipeline-overview/","text":"Render Pipeline Render pipelines are fully functional, but currently undocumented. See Also Shaders","title":"Render Pipeline"},{"location":"graphics/render-pipeline-overview/#render-pipeline","text":"Render pipelines are fully functional, but currently undocumented.","title":"Render Pipeline"},{"location":"graphics/render-pipeline-overview/#see-also","text":"Shaders","title":"See Also"},{"location":"graphics/sprite-component/","text":"Sprite Component The sprite component is used to render a textured quad that always faces the camera and whose on-screen size never exceeds a defined limit. Sprites are mainly used to place 3D icons or markers in the world. For example all the shape icons in the editor are sprites. However, they can also be used for simple effects, for instance to represent small projectiles. Although sprites have a world space size, their on-screen size is clamped to a maximum value. That means they won't fill up the screen when the camera comes very close. Component Properties Texture : The texture to use. BlendMode : How to blend the sprite with the background. Color : A tint color to multiply into the texture. Size : The actual size of the sprite in the world. Based on this, the screen space size is computed. This mostly affects how large the sprite appears from far away. MaxScreenSize : The maximum size of the sprite on screen. When the camera is close to the sprite, it will not fill up the entire screen. Instead, its on-screen size is clamped to this. Therefore, when getting close to a sprite, it appears to shrink. AspectRatio : The ratio of width to height of the sprite texture. See Also","title":"Sprite Component"},{"location":"graphics/sprite-component/#sprite-component","text":"The sprite component is used to render a textured quad that always faces the camera and whose on-screen size never exceeds a defined limit. Sprites are mainly used to place 3D icons or markers in the world. For example all the shape icons in the editor are sprites. However, they can also be used for simple effects, for instance to represent small projectiles. Although sprites have a world space size, their on-screen size is clamped to a maximum value. That means they won't fill up the screen when the camera comes very close.","title":"Sprite Component"},{"location":"graphics/sprite-component/#component-properties","text":"Texture : The texture to use. BlendMode : How to blend the sprite with the background. Color : A tint color to multiply into the texture. Size : The actual size of the sprite in the world. Based on this, the screen space size is computed. This mostly affects how large the sprite appears from far away. MaxScreenSize : The maximum size of the sprite on screen. When the camera is close to the sprite, it will not fill up the entire screen. Instead, its on-screen size is clamped to this. Therefore, when getting close to a sprite, it appears to shrink. AspectRatio : The ratio of width to height of the sprite texture.","title":"Component Properties"},{"location":"graphics/sprite-component/#see-also","text":"","title":"See Also"},{"location":"graphics/textures-overview/","text":"Textures Textures come in multiple forms. Most common are 2D textures loaded from file. 2D textures can also be created as render targets for render-to-texture (TODO) . For sky boxes or special effects you can also set up cubemap textures. When the source texture comes from a file, you can create a texture asset for that file by importing it. Otherwise you need to create the respective asset document manually, for example for render targets. Textures are most often referenced by materials . Texture assets only specify how source files are imported and combined, they don't define rendering behavior. Therefore most components don't use textures directly, but rather use materials, which configure the overall rendering, unless the component in question already sets up the rendering itself, such as the SkyBox component . All texture assets show a 3D preview . Here you can move the camera around to observe how the texture looks. Using the Mipmap slider in the toolbar, you can inspect individual mipmaps. With the channel selection dropdown next to the slider you can choose which channels to show. In the bottom left of the viewport, some stats are displayed, for instance what compression format is used. On the right-hand side you see the property grid where you choose the texture settings . Since transforming a texture can take quite some time, these assets are not automatically updated when you change their properties or even save the asset. Instead, transform the asset manually either by clicking the transform button in the toolbar or with Ctrl + E . 2D Texture Asset The following properties are available: Usage: Tells the texture converter what type of data the texture represents. This affects what color space (TODO) will be used, ie. whether the format uses sRGB or Linear encoding. If your color texture ends up using the incorrect color space (e.g. Linear when it should be sRGB ) you will typically notice that the texture appears too bright and washed out. Auto: In this mode the converter will guess the usage , utilizing file name heuristics, the source file format and sometimes even the content. If your file names use suffixes like _D , _Normal , etc. this can work pretty well. Color: Use this for textures that represent color. For example diffuse textures, skyboxes and emissive color textures. Linear: Use this for textures that represent 'data' that is not directly a color. For example roughness , metallic , ao and specular maps. HDR: For textures that contain high dynamic range data. Since HDR data requires special file formats, Auto mode can detect this reliably. Normal Map: For textures that represent normal maps. Normal Map (Inverted): For normal maps that use a different convention for the direction vectors. Bump Map: For greyscale textures that represent height values, which should be converted to a normal map. Mipmaps: This property allows you to choose whether the texture should have mipmaps and what algorithm to use for generating them. Unless you have a very special use case, mipmaps should always be enabled. The Kaiser filter usually gives slightly better results than the Linear filter. PreserveAlphaCoverage, AlphaThreshold: When you want to use a texture for vegetation, where the alpha channel is used to define transparent areas, then using this feature can improve how the vegetation looks when it is far away. The smaller a mipmap becomes, the more it represents the average color of the original texture. The same is true for the alpha channel. The problem then is, that if a vegetation texture is mostly transparent, the lower mipmaps will become more and more transparent. In practice that means the further you are away from a plant, the more transparent it becomes. For example all the leaves in a tree may disappear. If, on the other hand, the texture is mostly opaque, the lower mipmaps would become more and more opaque, so tree would appear much thicker when far away. PreserveAlphaCoverage can improve this, by making sure that the percentage of pixels with a value below and above AlphaThreshold stays the same throughout all mip levels. Thus, if the leaves of a tree are 70% transparent in the original texture, even when viewed from far away about 70% of its pixels will be transparent and not more. For this to work, the AlphaThreshold that will be used by the shader to discard pixels has to be known exactly and may not differ when the texture is used later. Computing the alpha coverage takes more time. Unless you are trying to solve the problem described above, keep this option disabled. Compression: Specifies how strong the texture should be compressed. Depending on Usage and whether an alpha channel is present, this determines which exact file format will be used (printed in the lower left corner of the viewport). Note that for some combinations there may be no difference between medium and strong compression. Don't choose Uncompressed unless you actually notice compression artifacts. Texture compression saves significant amount of memory (usually a 4:1 compression, sometimes even 8:1) and has no performance overhead during rendering. The only downside is, that it takes longer to transform compressed texture assets. DilateColor: For textures with an alpha mask. Will dilate (smear) the color channel outwards everywhere where the alpha channel is zero. Useful for example for vegetation textures, where the alpha channel defines a cutout mask. If the color channel does not contain some average color in the transparent areas, but something different, usually black, this unwanted color will bleed into the opaque areas in the lower mipmaps. As a result, masked objects appear to have a black edge. To fix this, either author your source textures to have a representative color in the transparent areas, or enable this option to automatically generate it. Note that this option makes texture transform slower. The image below shows the difference of a texture that is completely black in the transparent areas, versus the same texture, using color dilation. Notice how there is a black edge around the plant in the left image. Flip Horizontal: For textures that are stored upside down. HdrExposureBias: For textures that have the HDR Usage . Allows you to scale the brightness. Filtering: Specifies with which filter method the texture is sampled during rendering. If you select one of the Fixed modes, it will be filtered exactly with that mode. Mostly useful, if you want to use Nearest filtering, for artistic reasons. Using one of the other modes ( Low Quality / Default Quality / High Quality ) the texture will use a filtering mode that is decided at runtime. For example, if the default sampling mode is set to Anisotropic 2x , then High Quality will result in Anisotropic 4x (one mode higher) and Low Quality will be result in Trilinear sampling (one mode lower). Lowest and Highest will use two modes lower/higher, though Nearest filtering will never be used. AddressMode: Specifies whether a texture will be repeated when the texture coordinates used to sample the texture are outside the [0; 1] range. The 3D preview shows the effect of this property after you transform the asset. Channel Mapping: This option allows you to choose which channels the texture will have, and from which files to take the data for each channel. RGB(A) - Single Input is the most common choice, but you can also combine multiple textures, e.g. take the color from one input file and the alpha channel from another. Input N: Depending on the chosen channel mapping you need to specify one to four input textures. Cubemap Texture Asset Cubemap assets have a subset of the properties that 2D textures have. Their behavior is identical, except for Channel Mapping , which allows you to build a cubemap either from one or from six input files. Render Target Asset Render targets can be used like regular 2D textures. That means materials can reference them and display their content. However, they are filled by rendering to them at runtime, for example by using a camera to render the scene from a certain viewpoint. Have a look at the render to texture (TODO) article for details. Apart from some properties shared with 2D texture assets, render targets have these unique properties: Format: Determines whether the texture can store HDR data (10 bit or 16 bit) and whether the texture represents colors (sRGB) or linear data. Resolution: The dimensions of the texture. If set to CVar RT Resolution N the resolution will be read from the CVar r_RenderTargetResolutionN . This can be used to adjust the resolution dynamically. CVarResScale: If the resolution is read from a CVar, this allows to scale it. For instance to create a half resolution render target. See Also Materials Render to Texture (TODO) Sky Color Spaces (TODO)","title":"Textures"},{"location":"graphics/textures-overview/#textures","text":"Textures come in multiple forms. Most common are 2D textures loaded from file. 2D textures can also be created as render targets for render-to-texture (TODO) . For sky boxes or special effects you can also set up cubemap textures. When the source texture comes from a file, you can create a texture asset for that file by importing it. Otherwise you need to create the respective asset document manually, for example for render targets. Textures are most often referenced by materials . Texture assets only specify how source files are imported and combined, they don't define rendering behavior. Therefore most components don't use textures directly, but rather use materials, which configure the overall rendering, unless the component in question already sets up the rendering itself, such as the SkyBox component . All texture assets show a 3D preview . Here you can move the camera around to observe how the texture looks. Using the Mipmap slider in the toolbar, you can inspect individual mipmaps. With the channel selection dropdown next to the slider you can choose which channels to show. In the bottom left of the viewport, some stats are displayed, for instance what compression format is used. On the right-hand side you see the property grid where you choose the texture settings . Since transforming a texture can take quite some time, these assets are not automatically updated when you change their properties or even save the asset. Instead, transform the asset manually either by clicking the transform button in the toolbar or with Ctrl + E .","title":"Textures"},{"location":"graphics/textures-overview/#2d-texture-asset","text":"The following properties are available: Usage: Tells the texture converter what type of data the texture represents. This affects what color space (TODO) will be used, ie. whether the format uses sRGB or Linear encoding. If your color texture ends up using the incorrect color space (e.g. Linear when it should be sRGB ) you will typically notice that the texture appears too bright and washed out. Auto: In this mode the converter will guess the usage , utilizing file name heuristics, the source file format and sometimes even the content. If your file names use suffixes like _D , _Normal , etc. this can work pretty well. Color: Use this for textures that represent color. For example diffuse textures, skyboxes and emissive color textures. Linear: Use this for textures that represent 'data' that is not directly a color. For example roughness , metallic , ao and specular maps. HDR: For textures that contain high dynamic range data. Since HDR data requires special file formats, Auto mode can detect this reliably. Normal Map: For textures that represent normal maps. Normal Map (Inverted): For normal maps that use a different convention for the direction vectors. Bump Map: For greyscale textures that represent height values, which should be converted to a normal map. Mipmaps: This property allows you to choose whether the texture should have mipmaps and what algorithm to use for generating them. Unless you have a very special use case, mipmaps should always be enabled. The Kaiser filter usually gives slightly better results than the Linear filter. PreserveAlphaCoverage, AlphaThreshold: When you want to use a texture for vegetation, where the alpha channel is used to define transparent areas, then using this feature can improve how the vegetation looks when it is far away. The smaller a mipmap becomes, the more it represents the average color of the original texture. The same is true for the alpha channel. The problem then is, that if a vegetation texture is mostly transparent, the lower mipmaps will become more and more transparent. In practice that means the further you are away from a plant, the more transparent it becomes. For example all the leaves in a tree may disappear. If, on the other hand, the texture is mostly opaque, the lower mipmaps would become more and more opaque, so tree would appear much thicker when far away. PreserveAlphaCoverage can improve this, by making sure that the percentage of pixels with a value below and above AlphaThreshold stays the same throughout all mip levels. Thus, if the leaves of a tree are 70% transparent in the original texture, even when viewed from far away about 70% of its pixels will be transparent and not more. For this to work, the AlphaThreshold that will be used by the shader to discard pixels has to be known exactly and may not differ when the texture is used later. Computing the alpha coverage takes more time. Unless you are trying to solve the problem described above, keep this option disabled. Compression: Specifies how strong the texture should be compressed. Depending on Usage and whether an alpha channel is present, this determines which exact file format will be used (printed in the lower left corner of the viewport). Note that for some combinations there may be no difference between medium and strong compression. Don't choose Uncompressed unless you actually notice compression artifacts. Texture compression saves significant amount of memory (usually a 4:1 compression, sometimes even 8:1) and has no performance overhead during rendering. The only downside is, that it takes longer to transform compressed texture assets. DilateColor: For textures with an alpha mask. Will dilate (smear) the color channel outwards everywhere where the alpha channel is zero. Useful for example for vegetation textures, where the alpha channel defines a cutout mask. If the color channel does not contain some average color in the transparent areas, but something different, usually black, this unwanted color will bleed into the opaque areas in the lower mipmaps. As a result, masked objects appear to have a black edge. To fix this, either author your source textures to have a representative color in the transparent areas, or enable this option to automatically generate it. Note that this option makes texture transform slower. The image below shows the difference of a texture that is completely black in the transparent areas, versus the same texture, using color dilation. Notice how there is a black edge around the plant in the left image. Flip Horizontal: For textures that are stored upside down. HdrExposureBias: For textures that have the HDR Usage . Allows you to scale the brightness. Filtering: Specifies with which filter method the texture is sampled during rendering. If you select one of the Fixed modes, it will be filtered exactly with that mode. Mostly useful, if you want to use Nearest filtering, for artistic reasons. Using one of the other modes ( Low Quality / Default Quality / High Quality ) the texture will use a filtering mode that is decided at runtime. For example, if the default sampling mode is set to Anisotropic 2x , then High Quality will result in Anisotropic 4x (one mode higher) and Low Quality will be result in Trilinear sampling (one mode lower). Lowest and Highest will use two modes lower/higher, though Nearest filtering will never be used. AddressMode: Specifies whether a texture will be repeated when the texture coordinates used to sample the texture are outside the [0; 1] range. The 3D preview shows the effect of this property after you transform the asset. Channel Mapping: This option allows you to choose which channels the texture will have, and from which files to take the data for each channel. RGB(A) - Single Input is the most common choice, but you can also combine multiple textures, e.g. take the color from one input file and the alpha channel from another. Input N: Depending on the chosen channel mapping you need to specify one to four input textures.","title":"2D Texture Asset"},{"location":"graphics/textures-overview/#cubemap-texture-asset","text":"Cubemap assets have a subset of the properties that 2D textures have. Their behavior is identical, except for Channel Mapping , which allows you to build a cubemap either from one or from six input files.","title":"Cubemap Texture Asset"},{"location":"graphics/textures-overview/#render-target-asset","text":"Render targets can be used like regular 2D textures. That means materials can reference them and display their content. However, they are filled by rendering to them at runtime, for example by using a camera to render the scene from a certain viewpoint. Have a look at the render to texture (TODO) article for details. Apart from some properties shared with 2D texture assets, render targets have these unique properties: Format: Determines whether the texture can store HDR data (10 bit or 16 bit) and whether the texture represents colors (sRGB) or linear data. Resolution: The dimensions of the texture. If set to CVar RT Resolution N the resolution will be read from the CVar r_RenderTargetResolutionN . This can be used to adjust the resolution dynamically. CVarResScale: If the resolution is read from a CVar, this allows to scale it. For instance to create a half resolution render target.","title":"Render Target Asset"},{"location":"graphics/textures-overview/#see-also","text":"Materials Render to Texture (TODO) Sky Color Spaces (TODO)","title":"See Also"},{"location":"graphics/lighting/ambient-light-component/","text":"Ambient Light Component The Ambient Light Component lights up all objects equally. It is used to ensure that no area of a level is ever entirely dark. The component uses two colors, one for light coming from the sky (top down) and one for light coming from the ground (bottom up). Usually the top color should be slightly brighter and the bottom color should represent the top color as it would appear after being bounced off the ground. Both colors and the overall intensity should be kept low, otherwise the colors in the scene will appear washed out due to missing contrast. The image below shows a scene without any indirect light on the left, with a skylight in the middle and with ambient light on the right. Ambient light should be used sparingly. Prefer to use directional light components for the main sky and sun light contributions. You can even use multiple directional light components (without shadows and low intensity) to fake ambient light but with more directionality, ie. by having each directional light shine from roughly the same direction, to add some variation. Instead of ambient light, you could also use a sky light component . The effects of the Skylight will override the Ambient Light Component Properties TopColor : The ambient light coming from above. This is used to illuminate polygons that are facing the sky. BottomColor : The ambient light coming from below. This is used to illuminate polygons that are facing the ground. Intensity : The overall light intensity. See Also Lighting Sky Light Component","title":"Ambient Light Component"},{"location":"graphics/lighting/ambient-light-component/#ambient-light-component","text":"The Ambient Light Component lights up all objects equally. It is used to ensure that no area of a level is ever entirely dark. The component uses two colors, one for light coming from the sky (top down) and one for light coming from the ground (bottom up). Usually the top color should be slightly brighter and the bottom color should represent the top color as it would appear after being bounced off the ground. Both colors and the overall intensity should be kept low, otherwise the colors in the scene will appear washed out due to missing contrast. The image below shows a scene without any indirect light on the left, with a skylight in the middle and with ambient light on the right. Ambient light should be used sparingly. Prefer to use directional light components for the main sky and sun light contributions. You can even use multiple directional light components (without shadows and low intensity) to fake ambient light but with more directionality, ie. by having each directional light shine from roughly the same direction, to add some variation. Instead of ambient light, you could also use a sky light component . The effects of the Skylight will override the Ambient Light","title":"Ambient Light Component"},{"location":"graphics/lighting/ambient-light-component/#component-properties","text":"TopColor : The ambient light coming from above. This is used to illuminate polygons that are facing the sky. BottomColor : The ambient light coming from below. This is used to illuminate polygons that are facing the ground. Intensity : The overall light intensity.","title":"Component Properties"},{"location":"graphics/lighting/ambient-light-component/#see-also","text":"Lighting Sky Light Component","title":"See Also"},{"location":"graphics/lighting/directional-light-component/","text":"Directional Light Component The directional light component adds a light source that illuminates the entire scene from one direction. This is typically used for sun light. Since directional light affects everything, it isn't possible to cast shadows everywhere. Instead, shadows are restricted to a region around the camera and objects that are too far away from the camera, won't cast shadows. You can use multiple directional light sources, for example if you want directional ambient light , however, for performance reasons only one directional light should cast shadows. Component Properties See this page for shadow related component properties. LightColor, Intensity : The color and brightness of the light. NumCascades : How many shadow cascades to use. The more cascades are used, the crisper shadows close to the camera become. However, each cascade costs additional performance. MinShadowRange : How far from the camera the light should cast shadows. A low value means that only objects a short distance away will cast shadows, and objects farther away won't. FadeOutStart : At what fraction of the shadow range it should start to fade out. For instance, if the MinShadowRange is set to 10 meters, and FadeOutStart is set to 0.8, then the shadows will start to fade out at a distance of 8 meters. SplitModeWeight : TODO NearPlaneOffset : TODO See Also Lighting Dynamic Shadows","title":"Directional Light Component"},{"location":"graphics/lighting/directional-light-component/#directional-light-component","text":"The directional light component adds a light source that illuminates the entire scene from one direction. This is typically used for sun light. Since directional light affects everything, it isn't possible to cast shadows everywhere. Instead, shadows are restricted to a region around the camera and objects that are too far away from the camera, won't cast shadows. You can use multiple directional light sources, for example if you want directional ambient light , however, for performance reasons only one directional light should cast shadows.","title":"Directional Light Component"},{"location":"graphics/lighting/directional-light-component/#component-properties","text":"See this page for shadow related component properties. LightColor, Intensity : The color and brightness of the light. NumCascades : How many shadow cascades to use. The more cascades are used, the crisper shadows close to the camera become. However, each cascade costs additional performance. MinShadowRange : How far from the camera the light should cast shadows. A low value means that only objects a short distance away will cast shadows, and objects farther away won't. FadeOutStart : At what fraction of the shadow range it should start to fade out. For instance, if the MinShadowRange is set to 10 meters, and FadeOutStart is set to 0.8, then the shadows will start to fade out at a distance of 8 meters. SplitModeWeight : TODO NearPlaneOffset : TODO","title":"Component Properties"},{"location":"graphics/lighting/directional-light-component/#see-also","text":"Lighting Dynamic Shadows","title":"See Also"},{"location":"graphics/lighting/dynamic-shadows/","text":"Dynamic Shadows Light sources, such as point lights , spot lights and directional lights may cast dynamic shadows. ![[point-light.jpg]] Whether an object casts a shadow depends on whether it has the tag CastShadow set. Performance Shadows are implemented using shadow maps . That means every light source that shall cast a shadow, has to first render the current scene depth to a texture. This is a very costly operation, which is why you should keep the number of shadow casting light sources as low as possible. This also is more expensive, the more complex the shadow casting geometry is. Therefore consider switching shadows off for complex geometry and for small objects that don't contribute much anyway. For large complex geometry, you can also use low resolution proxy geometry for casting shadows, though you have to be careful with self-shadowing artifacts if the geometry is very different. Use your knowledge about the scene to switch shadow casting lights off when they are not needed. For example, if you need a light inside a room to cast dramatic shadows, but the room entrance is only visible from a corridor, use a trigger to only switch the light on when the player can actually see the light. Prefer to use spot lights over point lights, if that makes it possible to get away without shadows in the first place. All shadow casting light sources share a single shadow texture atlas. Every frame the engine determines the on-screen size of each light source and then allocates some area of the texture atlas to each light source. That means lights that are farther away will use a lower resolution shadow map than close up lights. Shadow Quality Shadow maps are prone to artifacts called shadow acne . Either light leaks through objects where there should be shadows, or shadows leak through objects where there should be light. This happens due to precision issues, especially when a shadow is cast nearly perpendicular to a surface. The SlopeBias and ConstantBias properties (see below) allow to tweak the shadows to reduce this issue in specific places, but there is no solution that will always work. Shadow Component Properties Dynamic light sources such as directional lights , point lights and spot lights can cast dynamic shadows. These components all have properties to tweak the shadows for quality. The following properties are common to these component types: CastShadows : If enabled, the light will cast dynamic shadows. Important: Casting shadows costs a lot of performance. Make sure to only have a small number of lights with shadows active at any one time, otherwise your game may perform poorly. PenumbraSize : This value specifies how soft the edge of shadows is supposed to be. The image below shows a penumbra size of 0 on the left and 0.5 on the right: SlopeBias, ConstantBias : TODO See Also","title":"Dynamic Shadows"},{"location":"graphics/lighting/dynamic-shadows/#dynamic-shadows","text":"Light sources, such as point lights , spot lights and directional lights may cast dynamic shadows. ![[point-light.jpg]] Whether an object casts a shadow depends on whether it has the tag CastShadow set.","title":"Dynamic Shadows"},{"location":"graphics/lighting/dynamic-shadows/#performance","text":"Shadows are implemented using shadow maps . That means every light source that shall cast a shadow, has to first render the current scene depth to a texture. This is a very costly operation, which is why you should keep the number of shadow casting light sources as low as possible. This also is more expensive, the more complex the shadow casting geometry is. Therefore consider switching shadows off for complex geometry and for small objects that don't contribute much anyway. For large complex geometry, you can also use low resolution proxy geometry for casting shadows, though you have to be careful with self-shadowing artifacts if the geometry is very different. Use your knowledge about the scene to switch shadow casting lights off when they are not needed. For example, if you need a light inside a room to cast dramatic shadows, but the room entrance is only visible from a corridor, use a trigger to only switch the light on when the player can actually see the light. Prefer to use spot lights over point lights, if that makes it possible to get away without shadows in the first place. All shadow casting light sources share a single shadow texture atlas. Every frame the engine determines the on-screen size of each light source and then allocates some area of the texture atlas to each light source. That means lights that are farther away will use a lower resolution shadow map than close up lights.","title":"Performance"},{"location":"graphics/lighting/dynamic-shadows/#shadow-quality","text":"Shadow maps are prone to artifacts called shadow acne . Either light leaks through objects where there should be shadows, or shadows leak through objects where there should be light. This happens due to precision issues, especially when a shadow is cast nearly perpendicular to a surface. The SlopeBias and ConstantBias properties (see below) allow to tweak the shadows to reduce this issue in specific places, but there is no solution that will always work.","title":"Shadow Quality"},{"location":"graphics/lighting/dynamic-shadows/#shadow-component-properties","text":"Dynamic light sources such as directional lights , point lights and spot lights can cast dynamic shadows. These components all have properties to tweak the shadows for quality. The following properties are common to these component types: CastShadows : If enabled, the light will cast dynamic shadows. Important: Casting shadows costs a lot of performance. Make sure to only have a small number of lights with shadows active at any one time, otherwise your game may perform poorly. PenumbraSize : This value specifies how soft the edge of shadows is supposed to be. The image below shows a penumbra size of 0 on the left and 0.5 on the right: SlopeBias, ConstantBias : TODO","title":"Shadow Component Properties"},{"location":"graphics/lighting/dynamic-shadows/#see-also","text":"","title":"See Also"},{"location":"graphics/lighting/lighting-overview/","text":"Lighting Lighting is the most important aspect of making a scene look good. Physically Based Rendering There are many formulas for computing lighting on surfaces. The defacto industry standard, which is also used in Plasma Engine, is P hysically B ased R endering (PBR) which describes a surface in terms of color , the surface normals, its roughness , whether it is a metal . Using this data, very convincing lighting can be computed. Therefore the standard type of material requires you to provide such textures. Optionally an occlusion texture can pronounce the lighting for small crevices. Static vs. Dynamic Lighting Many games differentiate between static or baked lighting, and dynamic lighting. Static lighting is precomputed and typically stored in lightmaps (dedicated textures) and other data structures. Dynamic lighting does not require any preprocessing or extra data. Baked lighting typically has the advantage that it can look much better because it can simulate light bounces and thus illuminate areas that are not directly lit. Currently Plasma Engine only supports dynamic lighting . That means every light source that you add to the scene can be moved around and change its color or brightness. It also means that every light source has a performance cost. The renderer uses a clustered forward rendering approach which can handle a relatively large amount of light sources efficiently. The most important rule is to reduce the number of overlapping light sources. The editor render modes allow you to look for hotspots. Shadows Dynamic lights have the disadvantage that they don't provide shadows by default. Instead, casting shadows is a separate process, which costs a lot of performance for every light source involved. Therefore, each light source requires you to decide whether it should cast shadows or not. You can use many small fill lights, as long as they don't cast shadows, but you should keep the number of shadow casting lights as low as possible, and each light should only cover an area as small as possible. For more details see the chapter about dynamic shadows . Light Component Types There are different component types to provide different types of lighting: Ambient Light Component : For lighting up a scene in general. Directional Light Component : For sun/moon light. Point Light Component : For light bulbs and overall fill lights. Spot Light Component : For flashlights and directed lighting. Sky Light Component : For dynamic light contribution from the sky. Reflection Probe Components : For localized reflection probes. See Also Materials Dynamic Shadows Render Modes","title":"Lighting"},{"location":"graphics/lighting/lighting-overview/#lighting","text":"Lighting is the most important aspect of making a scene look good.","title":"Lighting"},{"location":"graphics/lighting/lighting-overview/#physically-based-rendering","text":"There are many formulas for computing lighting on surfaces. The defacto industry standard, which is also used in Plasma Engine, is P hysically B ased R endering (PBR) which describes a surface in terms of color , the surface normals, its roughness , whether it is a metal . Using this data, very convincing lighting can be computed. Therefore the standard type of material requires you to provide such textures. Optionally an occlusion texture can pronounce the lighting for small crevices.","title":"Physically Based Rendering"},{"location":"graphics/lighting/lighting-overview/#static-vs-dynamic-lighting","text":"Many games differentiate between static or baked lighting, and dynamic lighting. Static lighting is precomputed and typically stored in lightmaps (dedicated textures) and other data structures. Dynamic lighting does not require any preprocessing or extra data. Baked lighting typically has the advantage that it can look much better because it can simulate light bounces and thus illuminate areas that are not directly lit. Currently Plasma Engine only supports dynamic lighting . That means every light source that you add to the scene can be moved around and change its color or brightness. It also means that every light source has a performance cost. The renderer uses a clustered forward rendering approach which can handle a relatively large amount of light sources efficiently. The most important rule is to reduce the number of overlapping light sources. The editor render modes allow you to look for hotspots.","title":"Static vs. Dynamic Lighting"},{"location":"graphics/lighting/lighting-overview/#shadows","text":"Dynamic lights have the disadvantage that they don't provide shadows by default. Instead, casting shadows is a separate process, which costs a lot of performance for every light source involved. Therefore, each light source requires you to decide whether it should cast shadows or not. You can use many small fill lights, as long as they don't cast shadows, but you should keep the number of shadow casting lights as low as possible, and each light should only cover an area as small as possible. For more details see the chapter about dynamic shadows .","title":"Shadows"},{"location":"graphics/lighting/lighting-overview/#light-component-types","text":"There are different component types to provide different types of lighting: Ambient Light Component : For lighting up a scene in general. Directional Light Component : For sun/moon light. Point Light Component : For light bulbs and overall fill lights. Spot Light Component : For flashlights and directed lighting. Sky Light Component : For dynamic light contribution from the sky. Reflection Probe Components : For localized reflection probes.","title":"Light Component Types"},{"location":"graphics/lighting/lighting-overview/#see-also","text":"Materials Dynamic Shadows Render Modes","title":"See Also"},{"location":"graphics/lighting/point-light-component/","text":"The point light component adds a dynamic light source that illuminates the scene equally in all directions. This is the most common type of light source. Each light source costs performance. Try to reduce the number of light sources that illuminate the same area. Use the light count render mode to find hotspots. Note that unless a light source casts dynamic shadows , it's light will shine through walls. Casting shadows is very costly, though. When it is possible to avoid casting shadows, for example by using multiple smaller lights instead, the performance should always be better. Component Properties See this page for shadow related component properties. LightColor, Intensity : The color and brightness of the light. Range : The distance over which the light source affects geometry. By default this is set to Auto , meaning the necessary range will be computed from the light's brightness. For full control, the range can be set manually. The light will always attenuate to zero within the given range, so by specifying a small range you can create a small, yet very bright light. See Also Lighting Spot Light Component","title":"Point light component"},{"location":"graphics/lighting/point-light-component/#component-properties","text":"See this page for shadow related component properties. LightColor, Intensity : The color and brightness of the light. Range : The distance over which the light source affects geometry. By default this is set to Auto , meaning the necessary range will be computed from the light's brightness. For full control, the range can be set manually. The light will always attenuate to zero within the given range, so by specifying a small range you can create a small, yet very bright light.","title":"Component Properties"},{"location":"graphics/lighting/point-light-component/#see-also","text":"Lighting Spot Light Component","title":"See Also"},{"location":"graphics/lighting/reflection-probe-components/","text":"Reflection Probe Components The two reflection probe components, box reflection probe component and sphere reflection probe component allow for localized reflections to be added to a scene. The probe makes a 360 degree screenshot of the scene to capture the overall lighting at this location into a cubemap which can be sampled at runtime to generate reflections. Scene Setup Sphere reflection probes project the captured cubemap to infinity, i.e. no parallax effect is observed when moving. Box reflection probes on the other hand project the cubemap to their extents, allowing for parallax correction of the reflection when moving in the scene. Probes take the parent game object's scale into account. Thus, sphere probes can actually be ellipsoids if scaled non-uniformly. The image below shows the difference between a box (to the left) and a sphere reflection probe (to the right). While the reflection in the metal ball both look convincing, the reflection on the floor is clearly off for the sphere reflection probe. By default, each probe captures everything in its radius. In many cases it makes sense to tag the ExcludeTags with SkyLight though so that the resulting cubemap is transparent where no geometry was rendered. This allows for the cubemaps to be dynamically composited at runtime. The benefits of this is that the skylight can change dynamically and won't be baked into the probe and it allows for parallax between e.g. a box probe and the sky light fallback reflection. The captured reflection is only visible in the probe's influence volume. For sphere probes this is limited by their radius and for box probes by their extents. At runtime, all probes are sorted by their volume and the smallest probe is sampled first. If the reflection is transparent in the cubemap, then next bigger cubemap that influences the geometry is sampled next. This continues until we hit an opaque pixel in a cubemap or if the fallback reflection of the sky light component is reached. The probe's ReflectionProbeMode by default is set to static , in which case the captured cubemap reflection is only updated once at the start or whenever the settings change. Alternatively it can be set to dynamic , in which case the probe is updated continuously. Component Properties Sphere reflection component: Radius : The influence range of the probe. The reflection is only visible on geometry intersecting this radius. Falloff : Percentage of the radius that is smoothly blended into other probes. SphereProjection : Enables paralax correction to project the reflection onto the shape of the sphere. Box reflection component: Extents : The extents of the box projection. The cubemap will be projected to this box. InfluenceScale , InfluenceShift : The influence volume can be smaller than the projected volume (extents) of the probe. This can be useful if you have e.g. a long corridor that you want to place multiple probes in. Each will have the same projection but a different part of the projection volume will be set as the influence volume centred around a probe capture offset. PositiveFalloff , NegativeFalloff : Percentage of the influence volume in each direction that is smoothly blended into other probes. The falloff is defined for each face of the box. BoxProjection : Enables paralax correction to project the reflection onto the shape of the box. Common properties for both sphere and box reflection probes that describe how the probe is captured: ReflectionProbeMode : Dynamic makes the skylight update continuously. Static will only update once at the start. IncludeTags , ExcludeTags : These tags define which objects in the scene are used to capture the scene. This is the same mechanism as used in the camera component . By default, the SkyLight tag is excluded to allow for dynamic composition with the sky light component . NearPlane , FarPlane : Camera settings used when the lighting is captured from the scene. If NearPlane is set to Auto , a value is computed automatically from the FarPlane . CaptureOffset : The capture offset allows for the capture position of the probe to be decoupled from the game object position. ShowDebugInfo : If enabled, a sphere with a preview of the probe cubemap is rendered at the position of the capture offset. Use this to check whether all desired objects contribute to the probe. Above the sphere will be a stack of other spheres that showcase the reflection with increased roughness. Best Practices Don't try to create mirrors with reflection probes Avoid mirrors or other perfectly reflective surfaces. Reflection probes are approximate and should not be used to emulate mirrors and other perfect reflectors. Even when using projection, the illusion of a perfect mirror immediately brakes down once objects are in the box-shaped room as the objects will be splatted to the boxs extents Ensure box projections have roughly the same length on each side If a box projection has very different dimensions on each axis you will run into stretching artifacts at the long ends of the box Instead of just one box probe, use multiple boxes to span long corridors to prevent this. To do this, take the existing box that spans the entire area and do the following: 1. Change it's Influence scale to e.g. 0.4 on the long axis, X in this case. 2. Duplicate the object at the same positions. 3. Set the duplicate's influence shift of the X-axis to 1. 4. Duplicate the object in-place again. 5. Set the new duplicate's influence shift of the X-axis to -1. Don't align box probes perfectly with walls When using BoxProjection , it is best to not try to align a box's extent perfectly with a room. Projections are intended to anchor reflections, not as a mirror substitute. In the following image you can see some of the pitfalls when trying to do so: While the floor reflection now fits perfectly, there is no reflection on the wall mirror. This is because there is a door to another room and to avoid hard interpolation discontinuties between reflection probes, the falloff in that direction needs to be set. As the falloff moves inwards the mirror on the wall basically gets no contribution of the room's probe anymore. On the left wall this problem can be avoided by setting the falloff to zero but that is generally not an option in most cases. Instead, increase the extents and set a falloff in each direction to fade into the neighboring reflection probes. Once the floor material is replaced with a more reasonable one that is not a perfect mirror the missalignment will no longer be aparent while still providing the desired parallax effect to the reflections. See Also Lighting Ambient Light Component Sky Light Component","title":"Reflection Probe Components"},{"location":"graphics/lighting/reflection-probe-components/#reflection-probe-components","text":"The two reflection probe components, box reflection probe component and sphere reflection probe component allow for localized reflections to be added to a scene. The probe makes a 360 degree screenshot of the scene to capture the overall lighting at this location into a cubemap which can be sampled at runtime to generate reflections.","title":"Reflection Probe Components"},{"location":"graphics/lighting/reflection-probe-components/#scene-setup","text":"Sphere reflection probes project the captured cubemap to infinity, i.e. no parallax effect is observed when moving. Box reflection probes on the other hand project the cubemap to their extents, allowing for parallax correction of the reflection when moving in the scene. Probes take the parent game object's scale into account. Thus, sphere probes can actually be ellipsoids if scaled non-uniformly. The image below shows the difference between a box (to the left) and a sphere reflection probe (to the right). While the reflection in the metal ball both look convincing, the reflection on the floor is clearly off for the sphere reflection probe. By default, each probe captures everything in its radius. In many cases it makes sense to tag the ExcludeTags with SkyLight though so that the resulting cubemap is transparent where no geometry was rendered. This allows for the cubemaps to be dynamically composited at runtime. The benefits of this is that the skylight can change dynamically and won't be baked into the probe and it allows for parallax between e.g. a box probe and the sky light fallback reflection. The captured reflection is only visible in the probe's influence volume. For sphere probes this is limited by their radius and for box probes by their extents. At runtime, all probes are sorted by their volume and the smallest probe is sampled first. If the reflection is transparent in the cubemap, then next bigger cubemap that influences the geometry is sampled next. This continues until we hit an opaque pixel in a cubemap or if the fallback reflection of the sky light component is reached. The probe's ReflectionProbeMode by default is set to static , in which case the captured cubemap reflection is only updated once at the start or whenever the settings change. Alternatively it can be set to dynamic , in which case the probe is updated continuously.","title":"Scene Setup"},{"location":"graphics/lighting/reflection-probe-components/#component-properties","text":"Sphere reflection component: Radius : The influence range of the probe. The reflection is only visible on geometry intersecting this radius. Falloff : Percentage of the radius that is smoothly blended into other probes. SphereProjection : Enables paralax correction to project the reflection onto the shape of the sphere. Box reflection component: Extents : The extents of the box projection. The cubemap will be projected to this box. InfluenceScale , InfluenceShift : The influence volume can be smaller than the projected volume (extents) of the probe. This can be useful if you have e.g. a long corridor that you want to place multiple probes in. Each will have the same projection but a different part of the projection volume will be set as the influence volume centred around a probe capture offset. PositiveFalloff , NegativeFalloff : Percentage of the influence volume in each direction that is smoothly blended into other probes. The falloff is defined for each face of the box. BoxProjection : Enables paralax correction to project the reflection onto the shape of the box. Common properties for both sphere and box reflection probes that describe how the probe is captured: ReflectionProbeMode : Dynamic makes the skylight update continuously. Static will only update once at the start. IncludeTags , ExcludeTags : These tags define which objects in the scene are used to capture the scene. This is the same mechanism as used in the camera component . By default, the SkyLight tag is excluded to allow for dynamic composition with the sky light component . NearPlane , FarPlane : Camera settings used when the lighting is captured from the scene. If NearPlane is set to Auto , a value is computed automatically from the FarPlane . CaptureOffset : The capture offset allows for the capture position of the probe to be decoupled from the game object position. ShowDebugInfo : If enabled, a sphere with a preview of the probe cubemap is rendered at the position of the capture offset. Use this to check whether all desired objects contribute to the probe. Above the sphere will be a stack of other spheres that showcase the reflection with increased roughness.","title":"Component Properties"},{"location":"graphics/lighting/reflection-probe-components/#best-practices","text":"","title":"Best Practices"},{"location":"graphics/lighting/reflection-probe-components/#dont-try-to-create-mirrors-with-reflection-probes","text":"Avoid mirrors or other perfectly reflective surfaces. Reflection probes are approximate and should not be used to emulate mirrors and other perfect reflectors. Even when using projection, the illusion of a perfect mirror immediately brakes down once objects are in the box-shaped room as the objects will be splatted to the boxs extents","title":"Don't try to create mirrors with reflection probes"},{"location":"graphics/lighting/reflection-probe-components/#ensure-box-projections-have-roughly-the-same-length-on-each-side","text":"If a box projection has very different dimensions on each axis you will run into stretching artifacts at the long ends of the box Instead of just one box probe, use multiple boxes to span long corridors to prevent this. To do this, take the existing box that spans the entire area and do the following: 1. Change it's Influence scale to e.g. 0.4 on the long axis, X in this case. 2. Duplicate the object at the same positions. 3. Set the duplicate's influence shift of the X-axis to 1. 4. Duplicate the object in-place again. 5. Set the new duplicate's influence shift of the X-axis to -1.","title":"Ensure box projections have roughly the same length on each side"},{"location":"graphics/lighting/reflection-probe-components/#dont-align-box-probes-perfectly-with-walls","text":"When using BoxProjection , it is best to not try to align a box's extent perfectly with a room. Projections are intended to anchor reflections, not as a mirror substitute. In the following image you can see some of the pitfalls when trying to do so: While the floor reflection now fits perfectly, there is no reflection on the wall mirror. This is because there is a door to another room and to avoid hard interpolation discontinuties between reflection probes, the falloff in that direction needs to be set. As the falloff moves inwards the mirror on the wall basically gets no contribution of the room's probe anymore. On the left wall this problem can be avoided by setting the falloff to zero but that is generally not an option in most cases. Instead, increase the extents and set a falloff in each direction to fade into the neighboring reflection probes. Once the floor material is replaced with a more reasonable one that is not a perfect mirror the missalignment will no longer be aparent while still providing the desired parallax effect to the reflections.","title":"Don't align box probes perfectly with walls"},{"location":"graphics/lighting/reflection-probe-components/#see-also","text":"Lighting Ambient Light Component Sky Light Component","title":"See Also"},{"location":"graphics/lighting/sky-light-component/","text":"Skylight Component The skylight component illuminates the entire scene, similar to the ambient light component . There are three main differences: Objects are illuminated using 6 different colors, one for each main direction (a so called 'ambient cube'). The ambient light component uses only 2 colors (top and bottom). The colors are dynamically computed from the scene. If the color of your sky changes, the ambient lighting of objects in the scene will reflect this. A global fallback reflection probe is generated. This is used for reflections if no local reflection probe components are present. The image above shows some objects lit only with the skylight component. Here the scene uses a skybox with blue sky, which is why the objects appear with a slightly blue tint. Skylight vs. Ambient Light Component The ambient light component uses a fixed color for lighting objects. Although you could animate those colors over time, for instance using a property animation (TODO) , it is not possible to be directional. Using the skylight component you can have a bit of directional lighting. In the image below the objects on the left are lit with an ambient light component. Note that the lighting is very flat. The objects on the right are lit with a skylight component. To demonstrate how it illuminates objects directionally, the skybox is set to have red, green and blue faces. The ambient light component also does not generate a fallback reflection probe. So you are completely reliant on reflection probe components for reflections. Scene Setup The skylight component continuously makes a 360 degree screenshot of the scene to capture the overall lighting. However, typically you don't want to capture the entire scene, but only very few elements. Most notably, you want to capture the background sky, e.g. the skybox. You may also want to capture the ground. Finally, if you have distant background geometry, like a city backdrop or mountains, which the player can never reach, you may want to include those in your skylight snapshot as well, especially when that geometry can affect the visibility and thus brightness of the sky. Therefore, the skylight component requires you to select those few objects and tag them , such that the update of the skylight only includes those objects. By default the IncludeTags property is already set to SkyLight , which means that only objects with this tag will be used for computing the overall lighting. Consequently, you have to select objects, like your skybox, and assign that tag to them, otherwise the skylight will stay black. The skylight ReflectionProbeMode can also be set to static. In this mode, it will either just do one scene capture at the start or, if CubeMap is set, just compute the ambient light and reflection from the given cubemap. The image below shows the ShowDebugInfo mode. Here the skylight component visualizes the geometry that is used to compute the skylight. In this case the skybox, the floor and the red object were all tagged with SkyLight , and therefore appear in the preview. The green box though, was not tagged and therefore does not affect the result. Important: When you insert the skylight into the scene, it will override existing ambient lighting, and your scene may turn black. That's because no object in the scene is yet contributing to the skylight. You need to add the proper tag on your sky to get the desired illumination. Component Properties ReflectionProbeMode : Dynamic makes the skylight update continuously. Static will only update once at the start or generate the lighting from a cubemap asset, if the CubeMap property is set. CubeMap : Select a static cubemap asset as the source of the lighting instead of capturing the scene. Only available if ReflectionProbeMode is set to Static . Intensity : This allows you to adjust the intensity of the applied ambient light. Saturation : With a saturation of 1, the color of the sky is applied exactly as it is to the scene. Often this would result in too colorful lighting, for example a strong blue hue. By reducing saturation, the light will become more monochrome. In the image at the top, saturation was set to 0.4 to reduce the blue tint from the sky. IncludeTags , ExcludeTags : These tags define which objects in the scene are used to compute the skylight. Make sure that the object that renders your sky has this include tag set. This is the same mechanism as used in the camera component . NearPlane , FarPlane : Camera settings used when the lighting is captured from the scene. If NearPlane is set to Auto , a value is computed automatically from the FarPlane . ShowDebugInfo : If enabled, a sphere with a preview of the sky image is rendered at the position of the skylight object. Use this to check whether all desired objects contribute to the skylight. Above the sphere will be a stack of other spheres that showcase the reflection with increased roughness. See Also Lighting Ambient Light Component Reflection Probe Components","title":"Skylight Component"},{"location":"graphics/lighting/sky-light-component/#skylight-component","text":"The skylight component illuminates the entire scene, similar to the ambient light component . There are three main differences: Objects are illuminated using 6 different colors, one for each main direction (a so called 'ambient cube'). The ambient light component uses only 2 colors (top and bottom). The colors are dynamically computed from the scene. If the color of your sky changes, the ambient lighting of objects in the scene will reflect this. A global fallback reflection probe is generated. This is used for reflections if no local reflection probe components are present. The image above shows some objects lit only with the skylight component. Here the scene uses a skybox with blue sky, which is why the objects appear with a slightly blue tint.","title":"Skylight Component"},{"location":"graphics/lighting/sky-light-component/#skylight-vs-ambient-light-component","text":"The ambient light component uses a fixed color for lighting objects. Although you could animate those colors over time, for instance using a property animation (TODO) , it is not possible to be directional. Using the skylight component you can have a bit of directional lighting. In the image below the objects on the left are lit with an ambient light component. Note that the lighting is very flat. The objects on the right are lit with a skylight component. To demonstrate how it illuminates objects directionally, the skybox is set to have red, green and blue faces. The ambient light component also does not generate a fallback reflection probe. So you are completely reliant on reflection probe components for reflections.","title":"Skylight vs. Ambient Light Component"},{"location":"graphics/lighting/sky-light-component/#scene-setup","text":"The skylight component continuously makes a 360 degree screenshot of the scene to capture the overall lighting. However, typically you don't want to capture the entire scene, but only very few elements. Most notably, you want to capture the background sky, e.g. the skybox. You may also want to capture the ground. Finally, if you have distant background geometry, like a city backdrop or mountains, which the player can never reach, you may want to include those in your skylight snapshot as well, especially when that geometry can affect the visibility and thus brightness of the sky. Therefore, the skylight component requires you to select those few objects and tag them , such that the update of the skylight only includes those objects. By default the IncludeTags property is already set to SkyLight , which means that only objects with this tag will be used for computing the overall lighting. Consequently, you have to select objects, like your skybox, and assign that tag to them, otherwise the skylight will stay black. The skylight ReflectionProbeMode can also be set to static. In this mode, it will either just do one scene capture at the start or, if CubeMap is set, just compute the ambient light and reflection from the given cubemap. The image below shows the ShowDebugInfo mode. Here the skylight component visualizes the geometry that is used to compute the skylight. In this case the skybox, the floor and the red object were all tagged with SkyLight , and therefore appear in the preview. The green box though, was not tagged and therefore does not affect the result. Important: When you insert the skylight into the scene, it will override existing ambient lighting, and your scene may turn black. That's because no object in the scene is yet contributing to the skylight. You need to add the proper tag on your sky to get the desired illumination.","title":"Scene Setup"},{"location":"graphics/lighting/sky-light-component/#component-properties","text":"ReflectionProbeMode : Dynamic makes the skylight update continuously. Static will only update once at the start or generate the lighting from a cubemap asset, if the CubeMap property is set. CubeMap : Select a static cubemap asset as the source of the lighting instead of capturing the scene. Only available if ReflectionProbeMode is set to Static . Intensity : This allows you to adjust the intensity of the applied ambient light. Saturation : With a saturation of 1, the color of the sky is applied exactly as it is to the scene. Often this would result in too colorful lighting, for example a strong blue hue. By reducing saturation, the light will become more monochrome. In the image at the top, saturation was set to 0.4 to reduce the blue tint from the sky. IncludeTags , ExcludeTags : These tags define which objects in the scene are used to compute the skylight. Make sure that the object that renders your sky has this include tag set. This is the same mechanism as used in the camera component . NearPlane , FarPlane : Camera settings used when the lighting is captured from the scene. If NearPlane is set to Auto , a value is computed automatically from the FarPlane . ShowDebugInfo : If enabled, a sphere with a preview of the sky image is rendered at the position of the skylight object. Use this to check whether all desired objects contribute to the skylight. Above the sphere will be a stack of other spheres that showcase the reflection with increased roughness.","title":"Component Properties"},{"location":"graphics/lighting/sky-light-component/#see-also","text":"Lighting Ambient Light Component Reflection Probe Components","title":"See Also"},{"location":"graphics/lighting/spot-light-component/","text":"Spot Light Component The spot light component adds a dynamic light source that illuminates the scene within a cone. The cone's inner angle determines the area that is illuminated equally bright. Between the inner angle and outer angle the light will fade to black. Spot lights should be preferred over point lights when this can prevent the need for dynamic shadows . For example, a light mounted to a ceiling should illuminate the area below it, but not shine through the ceiling and illuminate objects above. A spot light with a large cone can achieve this. Each light source costs performance. Try to reduce the number of light sources that illuminate the same area. Use the light count render mode to find hotspots. Note that unless a light source casts dynamic shadows , it's light will shine through walls. Casting shadows is very costly, though. When it is possible to avoid casting shadows, for example by using multiple smaller lights instead, the performance should always be better. Component Properties See this page for shadow related component properties. LightColor, Intensity : The color and brightness of the light. Range : The distance over which the light source affects geometry. By default this is set to Auto , meaning the necessary range will be computed from the light's brightness. For full control, the range can be set manually. The light will always attenuate to zero within the given range, so by specifying a small range you can create a small, yet very bright light. InnerSpotAngle : The inner angle of the spot light's cone. Within this angle the spot light will not attenuate (except by distance) and stay equally bright. OuterSpotAngle : The spot light will attenuate between the inner angle and the outer angle to zero. If the outer angler is very close to the inner angle, the spot light will have a very sharp cut off. If the outer angle is considerably larger than the inner angle, the spot light will smoothly fade to black at the edges. See Also Lighting Point Light Component","title":"Spot Light Component"},{"location":"graphics/lighting/spot-light-component/#spot-light-component","text":"The spot light component adds a dynamic light source that illuminates the scene within a cone. The cone's inner angle determines the area that is illuminated equally bright. Between the inner angle and outer angle the light will fade to black. Spot lights should be preferred over point lights when this can prevent the need for dynamic shadows . For example, a light mounted to a ceiling should illuminate the area below it, but not shine through the ceiling and illuminate objects above. A spot light with a large cone can achieve this. Each light source costs performance. Try to reduce the number of light sources that illuminate the same area. Use the light count render mode to find hotspots. Note that unless a light source casts dynamic shadows , it's light will shine through walls. Casting shadows is very costly, though. When it is possible to avoid casting shadows, for example by using multiple smaller lights instead, the performance should always be better.","title":"Spot Light Component"},{"location":"graphics/lighting/spot-light-component/#component-properties","text":"See this page for shadow related component properties. LightColor, Intensity : The color and brightness of the light. Range : The distance over which the light source affects geometry. By default this is set to Auto , meaning the necessary range will be computed from the light's brightness. For full control, the range can be set manually. The light will always attenuate to zero within the given range, so by specifying a small range you can create a small, yet very bright light. InnerSpotAngle : The inner angle of the spot light's cone. Within this angle the spot light will not attenuate (except by distance) and stay equally bright. OuterSpotAngle : The spot light will attenuate between the inner angle and the outer angle to zero. If the outer angler is very close to the inner angle, the spot light will have a very sharp cut off. If the outer angle is considerably larger than the inner angle, the spot light will smoothly fade to black at the edges.","title":"Component Properties"},{"location":"graphics/lighting/spot-light-component/#see-also","text":"Lighting Point Light Component","title":"See Also"},{"location":"graphics/meshes/custom-mesh-component/","text":"Custom Mesh Component The plCustomMeshComponent can be used in place of a mesh component , but rather than loading a mesh asset , the mesh geometry is provided through (C++) code dynamically at runtime. This component is useful when you need to build geometry dynamically, for example to create visualizers that show what a player can do at a specific point in your game world. These things may not be possible to build up from fixed pieces, and therefore need custom geometry. Mesh Buffer Resource Each custom mesh component references a plDynamicMeshBufferResource . Either its very own one, or a shared resource. At runtime, you would modify the geometry in that resource, and the plCustomMeshComponent takes care of rendering it. Instancing If you want to render multiple instances of the same geometry, create multiple plCustomMeshComponent s and set all of them to reference the same mesh buffer resource. Materials Each component only uses a single material for rendering, but if you want to use multiple materials to render different pieces of the geometry, you can use multiple custom mesh components, and have each one render a different part of the geometry by giving it a limited primitive range . Vertex Colors Custom mesh components use vertex colors , meaning every vertex stores its own color information. This is different to typical mesh data in Plasma, where per-vertex colors are usually not used. If you do not require vertex colors, this is fine and you can use any default material (and thus shader). The extra information will simply be ignored. However, if you do need the vertex color information, for instance to precisely control transparency, then you also need to set a material, which uses a shader (TODO) that actually reads the vertex color value and uses it in the way that you desire. You may need to write a custom shader for that. See Also Meshes Shaders","title":"Custom Mesh Component"},{"location":"graphics/meshes/custom-mesh-component/#custom-mesh-component","text":"The plCustomMeshComponent can be used in place of a mesh component , but rather than loading a mesh asset , the mesh geometry is provided through (C++) code dynamically at runtime. This component is useful when you need to build geometry dynamically, for example to create visualizers that show what a player can do at a specific point in your game world. These things may not be possible to build up from fixed pieces, and therefore need custom geometry.","title":"Custom Mesh Component"},{"location":"graphics/meshes/custom-mesh-component/#mesh-buffer-resource","text":"Each custom mesh component references a plDynamicMeshBufferResource . Either its very own one, or a shared resource. At runtime, you would modify the geometry in that resource, and the plCustomMeshComponent takes care of rendering it.","title":"Mesh Buffer Resource"},{"location":"graphics/meshes/custom-mesh-component/#instancing","text":"If you want to render multiple instances of the same geometry, create multiple plCustomMeshComponent s and set all of them to reference the same mesh buffer resource.","title":"Instancing"},{"location":"graphics/meshes/custom-mesh-component/#materials","text":"Each component only uses a single material for rendering, but if you want to use multiple materials to render different pieces of the geometry, you can use multiple custom mesh components, and have each one render a different part of the geometry by giving it a limited primitive range .","title":"Materials"},{"location":"graphics/meshes/custom-mesh-component/#vertex-colors","text":"Custom mesh components use vertex colors , meaning every vertex stores its own color information. This is different to typical mesh data in Plasma, where per-vertex colors are usually not used. If you do not require vertex colors, this is fine and you can use any default material (and thus shader). The extra information will simply be ignored. However, if you do need the vertex color information, for instance to precisely control transparency, then you also need to set a material, which uses a shader (TODO) that actually reads the vertex color value and uses it in the way that you desire. You may need to write a custom shader for that.","title":"Vertex Colors"},{"location":"graphics/meshes/custom-mesh-component/#see-also","text":"Meshes Shaders","title":"See Also"},{"location":"graphics/meshes/instanced-mesh-component/","text":"Instanced Mesh Component The instanced mesh component is used to place multiple instances of the same mesh in a scene, with only a single game object . This component is meant for rare special cases where you want to render many mesh instances, but remove the performance overhead of having a dedicated game object for each one. Important: Using the instanced mesh component does not mean, that only this enables the use of instancing on the GPU. The renderer already uses instancing, even for regular mesh components . So you don't need to fear that using non-instanced mesh components would be inefficient for rendering. Editing Instances The instanced mesh component has an array of items which hold the transform and color for each mesh instance: You add instances by extending the array. Click the blue properties of an entry, to make a gizmo appear with which you can transform that instance (see image at the top). This workflow is currently not particularly enjoyable. You can't select individual instances through the viewport and you can't use the standard gizmos to edit an instance. When and How to Use the Instanced Mesh Component This component is not meant to be used manually. For everything that you edit by hand, the regular mesh component is efficient enough and much more convenient to use. This component has been created to be used by runtime code that procedurally places meshes and may create hundreds or thousands of instances of the same mesh. In such a situation it does make a performance difference, whether the engine has to handle a single game object with a single instanced mesh component, or thousands of game objects with equally many regular mesh components. Consequently, if you write custom code that places meshes procedurally, you may utilize this component. Be aware though that the instanced mesh component acts as a single object in regards to frustum culling, meaning either all instances are visible or none , but nothing in between. So you should group instances by proximity and use multiple instanced mesh components, if the instances span a large area. Component Properties Mesh : The mesh asset to render. MainColor : This color will be combined with the color of each individual mesh instance. Materials : An array of materials to override the default material. Works the same way as for regular mesh components . InstanceData : An array of instances. Each instance has its own transform and color. Click the blue transform properties to activate a manipulator for that instance. See Also Meshes","title":"Instanced Mesh Component"},{"location":"graphics/meshes/instanced-mesh-component/#instanced-mesh-component","text":"The instanced mesh component is used to place multiple instances of the same mesh in a scene, with only a single game object . This component is meant for rare special cases where you want to render many mesh instances, but remove the performance overhead of having a dedicated game object for each one. Important: Using the instanced mesh component does not mean, that only this enables the use of instancing on the GPU. The renderer already uses instancing, even for regular mesh components . So you don't need to fear that using non-instanced mesh components would be inefficient for rendering.","title":"Instanced Mesh Component"},{"location":"graphics/meshes/instanced-mesh-component/#editing-instances","text":"The instanced mesh component has an array of items which hold the transform and color for each mesh instance: You add instances by extending the array. Click the blue properties of an entry, to make a gizmo appear with which you can transform that instance (see image at the top). This workflow is currently not particularly enjoyable. You can't select individual instances through the viewport and you can't use the standard gizmos to edit an instance.","title":"Editing Instances"},{"location":"graphics/meshes/instanced-mesh-component/#when-and-how-to-use-the-instanced-mesh-component","text":"This component is not meant to be used manually. For everything that you edit by hand, the regular mesh component is efficient enough and much more convenient to use. This component has been created to be used by runtime code that procedurally places meshes and may create hundreds or thousands of instances of the same mesh. In such a situation it does make a performance difference, whether the engine has to handle a single game object with a single instanced mesh component, or thousands of game objects with equally many regular mesh components. Consequently, if you write custom code that places meshes procedurally, you may utilize this component. Be aware though that the instanced mesh component acts as a single object in regards to frustum culling, meaning either all instances are visible or none , but nothing in between. So you should group instances by proximity and use multiple instanced mesh components, if the instances span a large area.","title":"When and How to Use the Instanced Mesh Component"},{"location":"graphics/meshes/instanced-mesh-component/#component-properties","text":"Mesh : The mesh asset to render. MainColor : This color will be combined with the color of each individual mesh instance. Materials : An array of materials to override the default material. Works the same way as for regular mesh components . InstanceData : An array of instances. Each instance has its own transform and color. Click the blue transform properties to activate a manipulator for that instance.","title":"Component Properties"},{"location":"graphics/meshes/instanced-mesh-component/#see-also","text":"Meshes","title":"See Also"},{"location":"graphics/meshes/mesh-asset/","text":"Mesh Asset A mesh asset represents a mesh that can be used for rendering. In the most common case the mesh asset imports the mesh data from an external file, such as an FBX file. However, it also supports generating the mesh data for common shapes (spheres, cylinders, ...) procedurally. Mesh assets are typically added to a scene with a mesh component . The left hand side of the asset document shows a 3D preview of the mesh. The viewport allows to switch the render mode to inspect the mesh normals, UV coordinates and so on. On the right hand side the asset properties specify how to import or generate the mesh data. Important: The mesh asset does not automatically update when you edit its properties. Instead you need to transform the asset ( Ctrl+E or with the rightmost button in the toolbar). Asset Properties PrimitiveType : This selects how the mesh data is generated. If From File is chosen, you need to also specify the MeshFile property. If you choose a procedural method, other configuration options appear. ForwardDir , RightDir , UpDir : With these you can change which axis is considered forward, right and up in the mesh data. For mesh data from FBX files, this information is typically embedded in the file. For other file types, you may need to adjust these to make the imported data appear correctly upright. UniformScaling : Adjusts the size of the mesh, for example to convert a mesh from centimeter to meter scale. RecalculateNormals , RecalculateTangents : If enabled, information about normals or tangents in the mesh file is ignored, and is instead computed from the vertex data. NormalPrecision , TexCoordPrecision : These options allow you to choose how precise normals and UV coordinates are represented. Leave these at the default, unless you notice precision issues. Higher precision means the mesh takes up more RAM on the GPU and is slightly slower to render. ImportMaterials : If enabled, the mesh import automatically generates material assets for the materials that the mesh file specifies. It also tries to populate those materials with sensible values and if possible also creates texture assets . Unfortunately this rarely works perfectly, and typically requires you to fix the generated assets afterwards. Note: Materials are only generated when the mesh has no materials set yet. After the initial creation of these other assets, you usually need to transform the mesh a second time to make them properly show up. Materials : The list of materials to use. The mesh may have multiple sub-meshes , and each sub-mesh uses a different material slot. Mesh components can override which material is used for which slot. Procedural Mesh Generation Through the PrimitiveType option you can choose to create a mesh procedurally. In this case object specific options appear. Note that by default objects use a detail level of 0 which means that the editor will pick a decent value, depending on the chosen primitive type. Be aware that some detail values seemingly have no effect. For instance, for cones, capsules and cylinders the detail represents the number of subdivisions along the circumference, and therefore can't be lower than 3. Therefore the values 1 , 2 and 3 all produce the same result. See Also Meshes Materials Assets Asset Import","title":"Mesh Asset"},{"location":"graphics/meshes/mesh-asset/#mesh-asset","text":"A mesh asset represents a mesh that can be used for rendering. In the most common case the mesh asset imports the mesh data from an external file, such as an FBX file. However, it also supports generating the mesh data for common shapes (spheres, cylinders, ...) procedurally. Mesh assets are typically added to a scene with a mesh component . The left hand side of the asset document shows a 3D preview of the mesh. The viewport allows to switch the render mode to inspect the mesh normals, UV coordinates and so on. On the right hand side the asset properties specify how to import or generate the mesh data. Important: The mesh asset does not automatically update when you edit its properties. Instead you need to transform the asset ( Ctrl+E or with the rightmost button in the toolbar).","title":"Mesh Asset"},{"location":"graphics/meshes/mesh-asset/#asset-properties","text":"PrimitiveType : This selects how the mesh data is generated. If From File is chosen, you need to also specify the MeshFile property. If you choose a procedural method, other configuration options appear. ForwardDir , RightDir , UpDir : With these you can change which axis is considered forward, right and up in the mesh data. For mesh data from FBX files, this information is typically embedded in the file. For other file types, you may need to adjust these to make the imported data appear correctly upright. UniformScaling : Adjusts the size of the mesh, for example to convert a mesh from centimeter to meter scale. RecalculateNormals , RecalculateTangents : If enabled, information about normals or tangents in the mesh file is ignored, and is instead computed from the vertex data. NormalPrecision , TexCoordPrecision : These options allow you to choose how precise normals and UV coordinates are represented. Leave these at the default, unless you notice precision issues. Higher precision means the mesh takes up more RAM on the GPU and is slightly slower to render. ImportMaterials : If enabled, the mesh import automatically generates material assets for the materials that the mesh file specifies. It also tries to populate those materials with sensible values and if possible also creates texture assets . Unfortunately this rarely works perfectly, and typically requires you to fix the generated assets afterwards. Note: Materials are only generated when the mesh has no materials set yet. After the initial creation of these other assets, you usually need to transform the mesh a second time to make them properly show up. Materials : The list of materials to use. The mesh may have multiple sub-meshes , and each sub-mesh uses a different material slot. Mesh components can override which material is used for which slot.","title":"Asset Properties"},{"location":"graphics/meshes/mesh-asset/#procedural-mesh-generation","text":"Through the PrimitiveType option you can choose to create a mesh procedurally. In this case object specific options appear. Note that by default objects use a detail level of 0 which means that the editor will pick a decent value, depending on the chosen primitive type. Be aware that some detail values seemingly have no effect. For instance, for cones, capsules and cylinders the detail represents the number of subdivisions along the circumference, and therefore can't be lower than 3. Therefore the values 1 , 2 and 3 all produce the same result.","title":"Procedural Mesh Generation"},{"location":"graphics/meshes/mesh-asset/#see-also","text":"Meshes Materials Assets Asset Import","title":"See Also"},{"location":"graphics/meshes/mesh-component/","text":"Mesh Component A mesh component is used to instantiate a mesh asset . Mesh components are purely visual, they have no physical interaction, so other physical objects cannot collide with them. To add physical interaction capabilities, an object has to have an additional collision shape and a static or dynamic physics actor . Mesh components will cast shadows when the CastShadow tag is set on the owner game object. The referenced mesh is rendered according to the used materials , which determine lighting and other visual effects. Component Properties Mesh : The mesh asset to render. Color : A tint color for the mesh instance. Typically this is just multiplied into the diffuse color of the mesh materials , though if the material uses a visual shader (TODO) , the mesh color can be used to represent arbitrary input data, for example to blend between material states. Materials : By default the referenced mesh is rendered with the materials that are set up inside the mesh asset. However, the mesh component can override the materials. Each mesh has one or many sub-meshes , meaning mesh parts that use different materials. This array allows to set an override for each of those sub-meshes. See Also Meshes Materials Lighting","title":"Mesh Component"},{"location":"graphics/meshes/mesh-component/#mesh-component","text":"A mesh component is used to instantiate a mesh asset . Mesh components are purely visual, they have no physical interaction, so other physical objects cannot collide with them. To add physical interaction capabilities, an object has to have an additional collision shape and a static or dynamic physics actor . Mesh components will cast shadows when the CastShadow tag is set on the owner game object. The referenced mesh is rendered according to the used materials , which determine lighting and other visual effects.","title":"Mesh Component"},{"location":"graphics/meshes/mesh-component/#component-properties","text":"Mesh : The mesh asset to render. Color : A tint color for the mesh instance. Typically this is just multiplied into the diffuse color of the mesh materials , though if the material uses a visual shader (TODO) , the mesh color can be used to represent arbitrary input data, for example to blend between material states. Materials : By default the referenced mesh is rendered with the materials that are set up inside the mesh asset. However, the mesh component can override the materials. Each mesh has one or many sub-meshes , meaning mesh parts that use different materials. This array allows to set an override for each of those sub-meshes.","title":"Component Properties"},{"location":"graphics/meshes/mesh-component/#see-also","text":"Meshes Materials Lighting","title":"See Also"},{"location":"graphics/meshes/meshes-overview/","text":"Meshes Meshes are the central feature of any 3D engine. Meshes can be separated into two kinds: the ones used for rendering, and the ones used for interactions ( physics ). Mesh data is either imported from external files, such as FBX files, or procedurally generated. Generating meshes procedurally is mostly useful for very basic shapes either for special cases or as placeholders during early development. Graphical meshes are handled by the mesh asset . Meshes that are used in physics simulations are called collision meshes . Once a mesh is imported as an asset , it can be placed in a scene as often as you like. For the most common use case you would use a mesh component to do so, but there are other components for special cases, such as the instanced mesh component . Meshes may also be used by other things, for example as a type of particle . To instantiate collision meshes, you need to use the proper shape component . Graphical meshes reference materials which define how the mesh gets rendered. Collision meshes may use surfaces to set their physical properties. See Also Materials Surfaces","title":"Meshes"},{"location":"graphics/meshes/meshes-overview/#meshes","text":"Meshes are the central feature of any 3D engine. Meshes can be separated into two kinds: the ones used for rendering, and the ones used for interactions ( physics ). Mesh data is either imported from external files, such as FBX files, or procedurally generated. Generating meshes procedurally is mostly useful for very basic shapes either for special cases or as placeholders during early development. Graphical meshes are handled by the mesh asset . Meshes that are used in physics simulations are called collision meshes . Once a mesh is imported as an asset , it can be placed in a scene as often as you like. For the most common use case you would use a mesh component to do so, but there are other components for special cases, such as the instanced mesh component . Meshes may also be used by other things, for example as a type of particle . To instantiate collision meshes, you need to use the proper shape component . Graphical meshes reference materials which define how the mesh gets rendered. Collision meshes may use surfaces to set their physical properties.","title":"Meshes"},{"location":"graphics/meshes/meshes-overview/#see-also","text":"Materials Surfaces","title":"See Also"},{"location":"graphics/render-to-texture/render-target-activator-component/","text":"Render Target Activator Component Render to texture functionality is working, but needs a rewrite, which is why this feature is currently undocumented. See Also Render to Texture (TODO)","title":"Render Target Activator Component"},{"location":"graphics/render-to-texture/render-target-activator-component/#render-target-activator-component","text":"Render to texture functionality is working, but needs a rewrite, which is why this feature is currently undocumented.","title":"Render Target Activator Component"},{"location":"graphics/render-to-texture/render-target-activator-component/#see-also","text":"Render to Texture (TODO)","title":"See Also"},{"location":"graphics/render-to-texture/render-to-texture/","text":"Render to Texture Render to texture functionality is working, but needs a rewrite, which is why this feature is currently undocumented. See Also Render Target Activator Component (TODO) Render Pipeline (TODO)","title":"Render to Texture"},{"location":"graphics/render-to-texture/render-to-texture/#render-to-texture","text":"Render to texture functionality is working, but needs a rewrite, which is why this feature is currently undocumented.","title":"Render to Texture"},{"location":"graphics/render-to-texture/render-to-texture/#see-also","text":"Render Target Activator Component (TODO) Render Pipeline (TODO)","title":"See Also"},{"location":"graphics/shaders/shader-debugging/","text":"Shader Debugging To debug a shader, one can configure it such that the shader compiler includes debugging information. To do so, include DEBUG as a platform in the [PLATFORMS] section of the shader: [PLATFORMS] ALL DEBUG [PERMUTATIONS] ALPHATEST WIREFRAME [RENDERSTATE] #if WIREFRAME == 1 WireFrame = true #endif [VERTEXSHADER] VS_OUT main(VS_IN Input) { ... } [PIXELSHADER] ... See Also Shaders Shader Templates ShaderCompiler","title":"Shader Debugging"},{"location":"graphics/shaders/shader-debugging/#shader-debugging","text":"To debug a shader, one can configure it such that the shader compiler includes debugging information. To do so, include DEBUG as a platform in the [PLATFORMS] section of the shader: [PLATFORMS] ALL DEBUG [PERMUTATIONS] ALPHATEST WIREFRAME [RENDERSTATE] #if WIREFRAME == 1 WireFrame = true #endif [VERTEXSHADER] VS_OUT main(VS_IN Input) { ... } [PIXELSHADER] ...","title":"Shader Debugging"},{"location":"graphics/shaders/shader-debugging/#see-also","text":"Shaders Shader Templates ShaderCompiler","title":"See Also"},{"location":"graphics/shaders/shader-permutation-variables/","text":"Shader Permutation Variables Permutation variables are global variables set either through C++ code or exposed through materials. The value of a permutation variable at the time of a drawcall affects which permutation of a shader is used for rendering. Permutation variables allow to create different variants of the same shader, without creating different shader files. Since their state is global, they decouple the decision which shader to use from the code that actually has this information at hand. For instance, materials support different rendering modes. By default they use proper lighting, but for debugging purposes we might want to override this and always output unlit diffuse color (or normals, UV coordinates, etc). The information which shader to use to render a certain object is stored either in a material or directly set through code. Without permutation variables we would either need to use an entirely different shader to get our debug output, which would mean that everything would need to support this functionality, or the shader would need to decide the final output mode dynamically, adding a large performance hit for a feature that is not used in the final game. Permutation variables solve this problem by creating different variants of the shader, and letting the engine pick the correct one depending on the current values. In shader code, permutation variables are exposed as #define 'd preprocessor variables and therefore can be evaluated like any other preprocessor directive. The Shader Permutations Section Each shader is made up of several sections : [PLATFORMS] ALL DEBUG [PERMUTATIONS] ALPHATEST WIREFRAME [MATERIALPARAMETER] Permutation ALPHATEST; [RENDERSTATE] #if WIREFRAME == 1 WireFrame = true #endif [VERTEXSHADER] VS_OUT main(VS_IN Input) { ... } [PIXELSHADER] ... In the [PERMUTATIONS] section the shader author has to list all permutation variables that are going to be evaluated inside the shader code. If a variable is used without being mentioned in this section, your shader might compile and work, but the result will always be the same. plPermVar Files Every permutation variable must be defined in a file that has the exact name of the permutation variable and the .plPermVar extension. All plPermVar files must reside in a specific subfolder in any data directory. By default the subfolder is \"Shaders/PermutationVars\" . bool Permutation variables The definition of a boolean permutation variable in its plPermVar file simply looks like this: bool TWO_SIDED; A boolean permutation variable is permuted over the values TRUE and FALSE . In a shader it would be evaluate like this: #if defined(PIXEL_SHADER) && TWO_SIDED == TRUE uint FrontFace : SV_IsFrontFace; #endif In C++ code the variable is set like this: plRenderContext::SetShaderPermutationVariable(\"TWO_SIDED\", \"TRUE\"); enum Permutation Variables Enum permutation variables allow you to use more than two permutation values and they can have more descriptive names. The definition of an enum variable in its plPermVar file looks like this: enum BLEND_MODE { OPAQUE, MASKED, TRANSPARENT, ADDITIVE, MODULATE }; Note: When evaluating an enum variable in a shader, the value must be prefixed with the name of the variable and an underscore: #if BLEND_MODE == BLEND_MODE_MASKED return opacity - MaskThreshold; #else return opacity; #endif As you can see, the name used for comparison is BLEND_MODE_MASKED although in the definition it was named MASKED . In C++ code we use the actual name though: plRenderContext::SetShaderPermutationVariable(\"BLEND_MODE\", \"MASKED\"); Exposing Permutations to materials By default permutation variables do not show up in materials and therefore cannot be manually specified by artists. If you want a variable to show up, simply list it in the [MATERIALPARAMETER] section: [MATERIALPARAMETER] Permutation ALPHATEST; The type ( bool or enum ) and the available values are automatically read from the plPermVar file that defines the variable and will show up in the material properties accordingly. See Also Shaders Shader Templates Shader Debugging The Shader Render State Section","title":"Shader Permutation Variables"},{"location":"graphics/shaders/shader-permutation-variables/#shader-permutation-variables","text":"Permutation variables are global variables set either through C++ code or exposed through materials. The value of a permutation variable at the time of a drawcall affects which permutation of a shader is used for rendering. Permutation variables allow to create different variants of the same shader, without creating different shader files. Since their state is global, they decouple the decision which shader to use from the code that actually has this information at hand. For instance, materials support different rendering modes. By default they use proper lighting, but for debugging purposes we might want to override this and always output unlit diffuse color (or normals, UV coordinates, etc). The information which shader to use to render a certain object is stored either in a material or directly set through code. Without permutation variables we would either need to use an entirely different shader to get our debug output, which would mean that everything would need to support this functionality, or the shader would need to decide the final output mode dynamically, adding a large performance hit for a feature that is not used in the final game. Permutation variables solve this problem by creating different variants of the shader, and letting the engine pick the correct one depending on the current values. In shader code, permutation variables are exposed as #define 'd preprocessor variables and therefore can be evaluated like any other preprocessor directive.","title":"Shader Permutation Variables"},{"location":"graphics/shaders/shader-permutation-variables/#the-shader-permutations-section","text":"Each shader is made up of several sections : [PLATFORMS] ALL DEBUG [PERMUTATIONS] ALPHATEST WIREFRAME [MATERIALPARAMETER] Permutation ALPHATEST; [RENDERSTATE] #if WIREFRAME == 1 WireFrame = true #endif [VERTEXSHADER] VS_OUT main(VS_IN Input) { ... } [PIXELSHADER] ... In the [PERMUTATIONS] section the shader author has to list all permutation variables that are going to be evaluated inside the shader code. If a variable is used without being mentioned in this section, your shader might compile and work, but the result will always be the same.","title":"The Shader Permutations Section"},{"location":"graphics/shaders/shader-permutation-variables/#plpermvar-files","text":"Every permutation variable must be defined in a file that has the exact name of the permutation variable and the .plPermVar extension. All plPermVar files must reside in a specific subfolder in any data directory. By default the subfolder is \"Shaders/PermutationVars\" .","title":"plPermVar Files"},{"location":"graphics/shaders/shader-permutation-variables/#bool-permutation-variables","text":"The definition of a boolean permutation variable in its plPermVar file simply looks like this: bool TWO_SIDED; A boolean permutation variable is permuted over the values TRUE and FALSE . In a shader it would be evaluate like this: #if defined(PIXEL_SHADER) && TWO_SIDED == TRUE uint FrontFace : SV_IsFrontFace; #endif In C++ code the variable is set like this: plRenderContext::SetShaderPermutationVariable(\"TWO_SIDED\", \"TRUE\");","title":"bool Permutation variables"},{"location":"graphics/shaders/shader-permutation-variables/#enum-permutation-variables","text":"Enum permutation variables allow you to use more than two permutation values and they can have more descriptive names. The definition of an enum variable in its plPermVar file looks like this: enum BLEND_MODE { OPAQUE, MASKED, TRANSPARENT, ADDITIVE, MODULATE }; Note: When evaluating an enum variable in a shader, the value must be prefixed with the name of the variable and an underscore: #if BLEND_MODE == BLEND_MODE_MASKED return opacity - MaskThreshold; #else return opacity; #endif As you can see, the name used for comparison is BLEND_MODE_MASKED although in the definition it was named MASKED . In C++ code we use the actual name though: plRenderContext::SetShaderPermutationVariable(\"BLEND_MODE\", \"MASKED\");","title":"enum Permutation Variables"},{"location":"graphics/shaders/shader-permutation-variables/#exposing-permutations-to-materials","text":"By default permutation variables do not show up in materials and therefore cannot be manually specified by artists. If you want a variable to show up, simply list it in the [MATERIALPARAMETER] section: [MATERIALPARAMETER] Permutation ALPHATEST; The type ( bool or enum ) and the available values are automatically read from the plPermVar file that defines the variable and will show up in the material properties accordingly.","title":"Exposing Permutations to materials"},{"location":"graphics/shaders/shader-permutation-variables/#see-also","text":"Shaders Shader Templates Shader Debugging The Shader Render State Section","title":"See Also"},{"location":"graphics/shaders/shader-render-state/","text":"Shader Render State The state of the rendering pipeline can only be set through shaders (TODO) . There is no way to change its state other than to select a shader which includes that specific state. Use shader permutations to create variants of a shader. Each variant may incorporate a different render state. By setting shader permutation variables at runtime, you select the specific shader variant (permutation) and thus also get its render state. This design follows what rendering APIs such as DirectX 12 and Vulkan require. The Shader Render State Section Each shader is made up of several sections : [PLATFORMS] ALL DEBUG [PERMUTATIONS] ALPHATEST WIREFRAME [RENDERSTATE] #if WIREFRAME == 1 WireFrame = true #endif [VERTEXSHADER] VS_OUT main(VS_IN Input) { ... } [PIXELSHADER] ... The render pipeline state associated with the shader is defined in the [RENDERSTATE] section. It may use permutation variables just like the shader code. To have different state for different permutations, use standard C preprocessor syntax. Render States The following variables are available in the [RENDERSTATE] section. Simply overwrite them with the desired value. Rasterizer States bool DepthClip = false bool FrontCounterClockwise = false bool LineAA = false bool MSAA = false bool ScissorTest = false bool WireFrame = false enum CullMode = CullMode_Back cpp CullMode = CullMode_None CullMode = CullMode_Back CullMode = CullMode_Front float DepthBiasClamp = 0.0 float SlopeScaledDepthBias = 0.0 int DepthBias = 0 Depth-Stencil State enum BackFaceDepthFailOp = StencilOp_Keep cpp BackFaceDepthFailOp = StencilOp_Keep BackFaceDepthFailOp = StencilOp_Zero BackFaceDepthFailOp = StencilOp_Replace BackFaceDepthFailOp = StencilOp_IncrementSaturated BackFaceDepthFailOp = StencilOp_DecrementSaturated BackFaceDepthFailOp = StencilOp_Invert BackFaceDepthFailOp = StencilOp_Increment BackFaceDepthFailOp = StencilOp_Decrement enum BackFaceFailOp = StencilOp_Keep cpp BackFaceFailOp = StencilOp_Keep BackFaceFailOp = StencilOp_Zero BackFaceFailOp = StencilOp_Replace BackFaceFailOp = StencilOp_IncrementSaturated BackFaceFailOp = StencilOp_DecrementSaturated BackFaceFailOp = StencilOp_Invert BackFaceFailOp = StencilOp_Increment BackFaceFailOp = StencilOp_Decrement enum BackFacePassOp = StencilOp_Keep cpp BackFacePassOp = StencilOp_Keep BackFacePassOp = StencilOp_Zero BackFacePassOp = StencilOp_Replace BackFacePassOp = StencilOp_IncrementSaturated BackFacePassOp = StencilOp_DecrementSaturated BackFacePassOp = StencilOp_Invert BackFacePassOp = StencilOp_Increment BackFacePassOp = StencilOp_Decrement enum BackFaceStencilFunc = CompareFunc_Always cpp BackFaceStencilFunc = CompareFunc_Never BackFaceStencilFunc = CompareFunc_Less BackFaceStencilFunc = CompareFunc_Equal BackFaceStencilFunc = CompareFunc_LessEqual BackFaceStencilFunc = CompareFunc_Greater BackFaceStencilFunc = CompareFunc_NotEqual BackFaceStencilFunc = CompareFunc_GreaterEqual BackFaceStencilFunc = CompareFunc_Always enum FrontFaceDepthFailOp = StencilOp_Keep cpp FrontFaceDepthFailOp = CompareFunc_Never FrontFaceDepthFailOp = CompareFunc_Less FrontFaceDepthFailOp = CompareFunc_Equal FrontFaceDepthFailOp = CompareFunc_LessEqual FrontFaceDepthFailOp = CompareFunc_Greater FrontFaceDepthFailOp = CompareFunc_NotEqual FrontFaceDepthFailOp = CompareFunc_GreaterEqual FrontFaceDepthFailOp = CompareFunc_Always enum FrontFaceFailOp = StencilOp_Keep cpp FrontFaceFailOp = CompareFunc_Never FrontFaceFailOp = CompareFunc_Less FrontFaceFailOp = CompareFunc_Equal FrontFaceFailOp = CompareFunc_LessEqual FrontFaceFailOp = CompareFunc_Greater FrontFaceFailOp = CompareFunc_NotEqual FrontFaceFailOp = CompareFunc_GreaterEqual FrontFaceFailOp = CompareFunc_Always enum FrontFacePassOp = StencilOp_Keep cpp FrontFacePassOp = CompareFunc_Never FrontFacePassOp = CompareFunc_Less FrontFacePassOp = CompareFunc_Equal FrontFacePassOp = CompareFunc_LessEqual FrontFacePassOp = CompareFunc_Greater FrontFacePassOp = CompareFunc_NotEqual FrontFacePassOp = CompareFunc_GreaterEqual FrontFacePassOp = CompareFunc_Always enum FrontFaceStencilFunc = CompareFunc_Always cpp FrontFaceStencilFunc = CompareFunc_Never FrontFaceStencilFunc = CompareFunc_Less FrontFaceStencilFunc = CompareFunc_Equal FrontFaceStencilFunc = CompareFunc_LessEqual FrontFaceStencilFunc = CompareFunc_Greater FrontFaceStencilFunc = CompareFunc_NotEqual FrontFaceStencilFunc = CompareFunc_GreaterEqual FrontFaceStencilFunc = CompareFunc_Always bool DepthTest = true bool DepthWrite = true bool SeparateFrontAndBack = false bool StencilTest = false enum DepthTestFunc = CompareFunc_Less cpp DepthTestFunc = CompareFunc_Never DepthTestFunc = CompareFunc_Less DepthTestFunc = CompareFunc_Equal DepthTestFunc = CompareFunc_LessEqual DepthTestFunc = CompareFunc_Greater DepthTestFunc = CompareFunc_NotEqual DepthTestFunc = CompareFunc_GreaterEqual DepthTestFunc = CompareFunc_Always int StencilReadMask = 255 int StencilWriteMask = 255 Blend State bool AlphaToCoverage = false bool IndependentBlend = false The following variables exist with suffix 0 to 7. If IndependentBlend is disabled, only the ones with suffix 0 are used. bool BlendingEnabled0 = false enum BlendOp0 = BlendOp_Add cpp BlendOp0 = BlendOp_Add BlendOp0 = BlendOp_Subtract BlendOp0 = BlendOp_RevSubtract BlendOp0 = BlendOp_Min BlendOp0 = BlendOp_Max enum BlendOpAlpha0 = BlendOp_Add cpp BlendOpAlpha0 = BlendOp_Add BlendOpAlpha0 = BlendOp_Subtract BlendOpAlpha0 = BlendOp_RevSubtract BlendOpAlpha0 = BlendOp_Min BlendOpAlpha0 = BlendOp_Max enum DestBlend0 = Blend_One cpp DestBlend0 = Blend_Zero DestBlend0 = Blend_One DestBlend0 = Blend_SrcColor DestBlend0 = Blend_InvSrcColor DestBlend0 = Blend_SrcAlpha DestBlend0 = Blend_InvSrcAlpha DestBlend0 = Blend_DestAlpha DestBlend0 = Blend_InvDestAlpha DestBlend0 = Blend_DestColor DestBlend0 = Blend_InvDestColor DestBlend0 = Blend_SrcAlphaSaturated DestBlend0 = Blend_BlendFactor DestBlend0 = Blend_InvBlendFactor enum DestBlendAlpha0 = Blend_One cpp DestBlendAlpha0 = Blend_Zero DestBlendAlpha0 = Blend_One DestBlendAlpha0 = Blend_SrcColor DestBlendAlpha0 = Blend_InvSrcColor DestBlendAlpha0 = Blend_SrcAlpha DestBlendAlpha0 = Blend_InvSrcAlpha DestBlendAlpha0 = Blend_DestAlpha DestBlendAlpha0 = Blend_InvDestAlpha DestBlendAlpha0 = Blend_DestColor DestBlendAlpha0 = Blend_InvDestColor DestBlendAlpha0 = Blend_SrcAlphaSaturated DestBlendAlpha0 = Blend_BlendFactor DestBlendAlpha0 = Blend_InvBlendFactor enum SourceBlend0 = Blend_One cpp SourceBlend0 = Blend_Zero SourceBlend0 = Blend_One SourceBlend0 = Blend_SrcColor SourceBlend0 = Blend_InvSrcColor SourceBlend0 = Blend_SrcAlpha SourceBlend0 = Blend_InvSrcAlpha SourceBlend0 = Blend_DestAlpha SourceBlend0 = Blend_InvDestAlpha SourceBlend0 = Blend_DestColor SourceBlend0 = Blend_InvDestColor SourceBlend0 = Blend_SrcAlphaSaturated SourceBlend0 = Blend_BlendFactor SourceBlend0 = Blend_InvBlendFactor enum SourceBlendAlpha0 = Blend_One cpp SourceBlendAlpha0 = Blend_Zero SourceBlendAlpha0 = Blend_One SourceBlendAlpha0 = Blend_SrcColor SourceBlendAlpha0 = Blend_InvSrcColor SourceBlendAlpha0 = Blend_SrcAlpha SourceBlendAlpha0 = Blend_InvSrcAlpha SourceBlendAlpha0 = Blend_DestAlpha SourceBlendAlpha0 = Blend_InvDestAlpha SourceBlendAlpha0 = Blend_DestColor SourceBlendAlpha0 = Blend_InvDestColor SourceBlendAlpha0 = Blend_SrcAlphaSaturated SourceBlendAlpha0 = Blend_BlendFactor SourceBlendAlpha0 = Blend_InvBlendFactor int WriteMask = 255 See Also Shaders Shader Permutation Variables Shader Templates Render Pipeline (TODO)","title":"Shader Render State"},{"location":"graphics/shaders/shader-render-state/#shader-render-state","text":"The state of the rendering pipeline can only be set through shaders (TODO) . There is no way to change its state other than to select a shader which includes that specific state. Use shader permutations to create variants of a shader. Each variant may incorporate a different render state. By setting shader permutation variables at runtime, you select the specific shader variant (permutation) and thus also get its render state. This design follows what rendering APIs such as DirectX 12 and Vulkan require.","title":"Shader Render State"},{"location":"graphics/shaders/shader-render-state/#the-shader-render-state-section","text":"Each shader is made up of several sections : [PLATFORMS] ALL DEBUG [PERMUTATIONS] ALPHATEST WIREFRAME [RENDERSTATE] #if WIREFRAME == 1 WireFrame = true #endif [VERTEXSHADER] VS_OUT main(VS_IN Input) { ... } [PIXELSHADER] ... The render pipeline state associated with the shader is defined in the [RENDERSTATE] section. It may use permutation variables just like the shader code. To have different state for different permutations, use standard C preprocessor syntax.","title":"The Shader Render State Section"},{"location":"graphics/shaders/shader-render-state/#render-states","text":"The following variables are available in the [RENDERSTATE] section. Simply overwrite them with the desired value.","title":"Render States"},{"location":"graphics/shaders/shader-render-state/#rasterizer-states","text":"bool DepthClip = false bool FrontCounterClockwise = false bool LineAA = false bool MSAA = false bool ScissorTest = false bool WireFrame = false enum CullMode = CullMode_Back cpp CullMode = CullMode_None CullMode = CullMode_Back CullMode = CullMode_Front float DepthBiasClamp = 0.0 float SlopeScaledDepthBias = 0.0 int DepthBias = 0","title":"Rasterizer States"},{"location":"graphics/shaders/shader-render-state/#depth-stencil-state","text":"enum BackFaceDepthFailOp = StencilOp_Keep cpp BackFaceDepthFailOp = StencilOp_Keep BackFaceDepthFailOp = StencilOp_Zero BackFaceDepthFailOp = StencilOp_Replace BackFaceDepthFailOp = StencilOp_IncrementSaturated BackFaceDepthFailOp = StencilOp_DecrementSaturated BackFaceDepthFailOp = StencilOp_Invert BackFaceDepthFailOp = StencilOp_Increment BackFaceDepthFailOp = StencilOp_Decrement enum BackFaceFailOp = StencilOp_Keep cpp BackFaceFailOp = StencilOp_Keep BackFaceFailOp = StencilOp_Zero BackFaceFailOp = StencilOp_Replace BackFaceFailOp = StencilOp_IncrementSaturated BackFaceFailOp = StencilOp_DecrementSaturated BackFaceFailOp = StencilOp_Invert BackFaceFailOp = StencilOp_Increment BackFaceFailOp = StencilOp_Decrement enum BackFacePassOp = StencilOp_Keep cpp BackFacePassOp = StencilOp_Keep BackFacePassOp = StencilOp_Zero BackFacePassOp = StencilOp_Replace BackFacePassOp = StencilOp_IncrementSaturated BackFacePassOp = StencilOp_DecrementSaturated BackFacePassOp = StencilOp_Invert BackFacePassOp = StencilOp_Increment BackFacePassOp = StencilOp_Decrement enum BackFaceStencilFunc = CompareFunc_Always cpp BackFaceStencilFunc = CompareFunc_Never BackFaceStencilFunc = CompareFunc_Less BackFaceStencilFunc = CompareFunc_Equal BackFaceStencilFunc = CompareFunc_LessEqual BackFaceStencilFunc = CompareFunc_Greater BackFaceStencilFunc = CompareFunc_NotEqual BackFaceStencilFunc = CompareFunc_GreaterEqual BackFaceStencilFunc = CompareFunc_Always enum FrontFaceDepthFailOp = StencilOp_Keep cpp FrontFaceDepthFailOp = CompareFunc_Never FrontFaceDepthFailOp = CompareFunc_Less FrontFaceDepthFailOp = CompareFunc_Equal FrontFaceDepthFailOp = CompareFunc_LessEqual FrontFaceDepthFailOp = CompareFunc_Greater FrontFaceDepthFailOp = CompareFunc_NotEqual FrontFaceDepthFailOp = CompareFunc_GreaterEqual FrontFaceDepthFailOp = CompareFunc_Always enum FrontFaceFailOp = StencilOp_Keep cpp FrontFaceFailOp = CompareFunc_Never FrontFaceFailOp = CompareFunc_Less FrontFaceFailOp = CompareFunc_Equal FrontFaceFailOp = CompareFunc_LessEqual FrontFaceFailOp = CompareFunc_Greater FrontFaceFailOp = CompareFunc_NotEqual FrontFaceFailOp = CompareFunc_GreaterEqual FrontFaceFailOp = CompareFunc_Always enum FrontFacePassOp = StencilOp_Keep cpp FrontFacePassOp = CompareFunc_Never FrontFacePassOp = CompareFunc_Less FrontFacePassOp = CompareFunc_Equal FrontFacePassOp = CompareFunc_LessEqual FrontFacePassOp = CompareFunc_Greater FrontFacePassOp = CompareFunc_NotEqual FrontFacePassOp = CompareFunc_GreaterEqual FrontFacePassOp = CompareFunc_Always enum FrontFaceStencilFunc = CompareFunc_Always cpp FrontFaceStencilFunc = CompareFunc_Never FrontFaceStencilFunc = CompareFunc_Less FrontFaceStencilFunc = CompareFunc_Equal FrontFaceStencilFunc = CompareFunc_LessEqual FrontFaceStencilFunc = CompareFunc_Greater FrontFaceStencilFunc = CompareFunc_NotEqual FrontFaceStencilFunc = CompareFunc_GreaterEqual FrontFaceStencilFunc = CompareFunc_Always bool DepthTest = true bool DepthWrite = true bool SeparateFrontAndBack = false bool StencilTest = false enum DepthTestFunc = CompareFunc_Less cpp DepthTestFunc = CompareFunc_Never DepthTestFunc = CompareFunc_Less DepthTestFunc = CompareFunc_Equal DepthTestFunc = CompareFunc_LessEqual DepthTestFunc = CompareFunc_Greater DepthTestFunc = CompareFunc_NotEqual DepthTestFunc = CompareFunc_GreaterEqual DepthTestFunc = CompareFunc_Always int StencilReadMask = 255 int StencilWriteMask = 255","title":"Depth-Stencil State"},{"location":"graphics/shaders/shader-render-state/#blend-state","text":"bool AlphaToCoverage = false bool IndependentBlend = false The following variables exist with suffix 0 to 7. If IndependentBlend is disabled, only the ones with suffix 0 are used. bool BlendingEnabled0 = false enum BlendOp0 = BlendOp_Add cpp BlendOp0 = BlendOp_Add BlendOp0 = BlendOp_Subtract BlendOp0 = BlendOp_RevSubtract BlendOp0 = BlendOp_Min BlendOp0 = BlendOp_Max enum BlendOpAlpha0 = BlendOp_Add cpp BlendOpAlpha0 = BlendOp_Add BlendOpAlpha0 = BlendOp_Subtract BlendOpAlpha0 = BlendOp_RevSubtract BlendOpAlpha0 = BlendOp_Min BlendOpAlpha0 = BlendOp_Max enum DestBlend0 = Blend_One cpp DestBlend0 = Blend_Zero DestBlend0 = Blend_One DestBlend0 = Blend_SrcColor DestBlend0 = Blend_InvSrcColor DestBlend0 = Blend_SrcAlpha DestBlend0 = Blend_InvSrcAlpha DestBlend0 = Blend_DestAlpha DestBlend0 = Blend_InvDestAlpha DestBlend0 = Blend_DestColor DestBlend0 = Blend_InvDestColor DestBlend0 = Blend_SrcAlphaSaturated DestBlend0 = Blend_BlendFactor DestBlend0 = Blend_InvBlendFactor enum DestBlendAlpha0 = Blend_One cpp DestBlendAlpha0 = Blend_Zero DestBlendAlpha0 = Blend_One DestBlendAlpha0 = Blend_SrcColor DestBlendAlpha0 = Blend_InvSrcColor DestBlendAlpha0 = Blend_SrcAlpha DestBlendAlpha0 = Blend_InvSrcAlpha DestBlendAlpha0 = Blend_DestAlpha DestBlendAlpha0 = Blend_InvDestAlpha DestBlendAlpha0 = Blend_DestColor DestBlendAlpha0 = Blend_InvDestColor DestBlendAlpha0 = Blend_SrcAlphaSaturated DestBlendAlpha0 = Blend_BlendFactor DestBlendAlpha0 = Blend_InvBlendFactor enum SourceBlend0 = Blend_One cpp SourceBlend0 = Blend_Zero SourceBlend0 = Blend_One SourceBlend0 = Blend_SrcColor SourceBlend0 = Blend_InvSrcColor SourceBlend0 = Blend_SrcAlpha SourceBlend0 = Blend_InvSrcAlpha SourceBlend0 = Blend_DestAlpha SourceBlend0 = Blend_InvDestAlpha SourceBlend0 = Blend_DestColor SourceBlend0 = Blend_InvDestColor SourceBlend0 = Blend_SrcAlphaSaturated SourceBlend0 = Blend_BlendFactor SourceBlend0 = Blend_InvBlendFactor enum SourceBlendAlpha0 = Blend_One cpp SourceBlendAlpha0 = Blend_Zero SourceBlendAlpha0 = Blend_One SourceBlendAlpha0 = Blend_SrcColor SourceBlendAlpha0 = Blend_InvSrcColor SourceBlendAlpha0 = Blend_SrcAlpha SourceBlendAlpha0 = Blend_InvSrcAlpha SourceBlendAlpha0 = Blend_DestAlpha SourceBlendAlpha0 = Blend_InvDestAlpha SourceBlendAlpha0 = Blend_DestColor SourceBlendAlpha0 = Blend_InvDestColor SourceBlendAlpha0 = Blend_SrcAlphaSaturated SourceBlendAlpha0 = Blend_BlendFactor SourceBlendAlpha0 = Blend_InvBlendFactor int WriteMask = 255","title":"Blend State"},{"location":"graphics/shaders/shader-render-state/#see-also","text":"Shaders Shader Permutation Variables Shader Templates Render Pipeline (TODO)","title":"See Also"},{"location":"graphics/shaders/shader-resources/","text":"Shaders Resources Shader resources are things like textures, samplers constant buffers etc. that need to be separately bound in the renderer for the shader to function. Each resource must be bound to a set and slot. Depending on the platform used, the requirements for this binding can be very different. E.g. in Vulkan slot assignments must be unique within a set across all stages while in DX11 most slots only need to be unique within a stage. Not following these rules will result in a runtime error. Manually assigning slots is an option but is very tedious. To make this easier, the shader system can automate this process provided some constraints are met how resourced are declared. Currently, Plasma does not support arrays of resources like Texture2D Diffuse[3] in its shaders. Resources must have unique names across all shader stages. The same resource name can be used in multiple stages as long as the resource it maps to is exactly the same. Resource Binding The shader system only supports the DX11 / DX12 register syntax for resource binding. Both the set and slot can be bound. If no set is given, it is implicitly set 0. Here is a list of a few examples of how to bind resources properly: Texture2D Diffuse : register(t3, space1); // DX12 syntax, slot 3, set 1 SamplerState MySampler : register(s4); // DX11 syntax slot 4, set 0 (default) ByteAddressBuffer MyBuffer BIND_RESOURCE(SLOT_AUTO, SET_RENDER_PASS); // Slot Auto, set 1 ByteAddressBuffer MyBuffer2 BIND_SET(SET_RENDER_PASS); // Slot Auto, set 1 CONSTANT_BUFFER(plTestPositions, 1) // Slot 1, set 0 (default) { ... }; CONSTANT_BUFFER2(plTestPositions, SLOT_AUTO, SET_MATERIAL) // Slot Auto, set 2 { ... }; The HLSL register syntax is a bit impractical, so the macros BIND_RESOURCE(Slot, Set) and BIND_SET(Set) were introduced. These will generate invalid HLSL code which the shader compiler will eventually parse, organize and patch to do the correct thing on each platform. In most cases, you should only be concerned about deciding in which set a resource should reside in. Either use the macro SLOT_AUTO when setting a slot or just use the BIND_SET macro which omits the slot entirely. While you can set any integer for the set, some platforms like Vulkan have a limit on how many sets can be managed at the same time with a minimum of four. Plasma defines macros for these four sets: SET_FRAME , SET_RENDER_PASS , SET_MATERIAL and SET_DRAW_CALL . Resources should ideally be bound to these sets according to their update frequency. Constant Buffers Constant buffers map to plGALShaderResourceType::ConstantBuffer in C++. To facilitate C++ interop, constant buffers should be placed into a separate header file that looks like this: #pragma once #include <Shaders/Common/ConstantBufferMacros.h> CONSTANT_BUFFER(plTestPositions, 3) { FLOAT4(Vertex0); FLOAT4(Vertex1); FLOAT4(Vertex2); }; By using the macros defined in ConstantBufferMacros.h like CONSTANT_BUFFER and the data types like FLOAT4 , the file can be included in both shader and C++ code. This makes it easy to create an instance of the constant buffer as a C++ struct in code to update it. Care must be taken to ensure that the constant buffer has the same layout in C++ and HLSL though: 1. Make sure that the size of your struct is a multiple of 16 bytes. Fill out any missing bytes with dummy FLOAT1 entries. 2. A FLOAT3 can't be followed by another FLOAT3 . It should be followed by a FLOAT1 first or some other types of the same byte counts to ensure the next FLOAT3 starts at a 16 byte boundary. This is necessary as the layout rules are different between HLSL and C++. Push Constants Push constants map to plGALShaderResourceType::PushConstants in C++. Push constants allow for fast updates of a small set of bytes. Usually at least 128 bytes. You can check plGALDeviceCapabilities::m_uiMaxPushConstantsSize for the max push constant buffer size. On platforms that don't support push constants like DX11, this is emulated via a constant buffer. Only one push constants block is supported across all shader stages of a shader. Like with constant buffers, special macros have to be used and the declaration should be put into a separate header so it can be included in both shader and C++ code: // Header: #pragma once #include <Shaders/Common/ConstantBufferMacros.h> BEGIN_PUSH_CONSTANTS(plTestData) { FLOAT4(VertexColor); FLOAT4(Vertex0); FLOAT4(Vertex1); FLOAT4(Vertex2); } END_PUSH_CONSTANTS(plTestData) // Shader: float4 main(VS_OUT a) : SV_Target { return GET_PUSH_CONSTANT(plTestData, VertexColor); } // C++: plTestData constants; constants.VertexColor = ...; pContext->SetPushConstants(\"plTestData\", constants); The BEGIN_PUSH_CONSTANTS and END_PUSH_CONSTANTS macros define the struct. Unlike with constant buffers, you can't simply access the values inside a shader by just the name of the variable, e.g. VertexColor . This is because depending on the platform, a different syntax needs to be used to access the content. To make the same shader compile on all platforms, you need to use the GET_PUSH_CONSTANT(Name, Constant) macro to access a member of the push constant buffer. Samplers Samplers map to plGALShaderResourceType::Sampler or plGALShaderResourceType::TextureAndSampler in C++. Two types of samplers are supported: SamplerState and SamplerComparisonState . The naming of the samplers is important, as it can be used to optimize your workflow. Plasma has a concept of immutable Samplers, these samplers are automatically bound so you can use them in the shader without needing to define them in C++. Immutable samplers are registered in code via plGALImmutableSamplers::RegisterImmutableSampler . Currently, these samplers are registered: LinearSampler , LinearClampSampler , PointSampler and PointClampSampler . Plasma does not allow for two different resources to have the same name, the only exception is textures and samplers which can have the same name by calling the sampler NAME_AutoSampler . The compiler will rename the sampler to NAME and on platforms that support combined image samplers both will be combined into a single resource of type plGALShaderResourceType::TextureAndSampler . The benefit of this approach is that when binding a texture resource to a material for example, the texture resource can define both the texture as well as the sampler state, binding both to the same name. SamplerState DiffuseSampler; SamplerComparisonState ShadowSampler; // Auto sampler combines with texture of the same name: Texture2D BaseTexture; SamplerState BaseTexture_AutoSampler; Textures Textures map to plGALShaderResourceType::Texture or plGALShaderResourceType::TextureAndSampler in C++ (see samplers above). Plasma supports all HLSL texture types except for 1D textures. You can work around this by creating 1xN 2DTextures. Texture1D texture1D; // 1D textures currently not supported. Texture1DArray texture1DArray; // 1D textures currently not supported. Texture2D texture2D; Texture2DArray texture2DArray; Texture2DMS<float4> texture2DMS; Texture2DMSArray<float4> texture2DMSArray; Texture3D texture3D; TextureCube textureCube; TextureCubeArray textureCubeArray; Read-write variants are also supported and map to plGALShaderResourceType::TextureRW in C++. RWTexture1D<float> rwTexture1D; // 1D textures currently not supported. RWTexture1DArray<float2> rwTexture1DArray; // 1D textures currently not supported. RWTexture2D<float3> rwTexture2D; RWTexture2DArray<float4> rwTexture2DArray; RWTexture3D<uint> rwTexture3D; Buffers There are three types of buffers supported by Plasma: 1. HLSL's Buffer<T> type is very similar to a 1D texture. A buffer of the same type T needs to be bound to the resource. Maps to plGALShaderResourceType::TexelBuffer in C++. 2. StructuredBuffer<T> should follow the same rules as for constant buffers: Put the declaration in a separate header file to allow access to it from C++ and ensure each struct is 16 bytes aligned. Maps to plGALShaderResourceType::StructuredBuffer in C++. 3. ByteAddressBuffer in just an array of bytes. A raw buffer needs to be bound to the resource. With HLSL 5.1, you can cast any offset of the buffer into a struct. Maps to plGALShaderResourceType::StructuredBuffer in C++. // Header: #pragma once #include <Shaders/Common/ConstantBufferMacros.h> struct PL_SHADER_STRUCT PerInstanceData { TRANSFORM(ObjectToWorld); }; // Shader: Buffer<uint> buffer; StructuredBuffer<PerInstanceData> structuredBuffer; ByteAddressBuffer byteAddressBuffer; Read-write variants of these buffers are also supported and map to plGALShaderResourceType::TexelBufferRW and plGALShaderResourceType::StructuredBufferRW respectively. RWBuffer<uint> rwBuffer; RWStructuredBuffer<plPerInstanceData> rwStructuredBuffer; RWByteAddressBuffer rwByteAddressBuffer; Append / Consume Buffers TODO: Future work: Append / consume buffers can be defined in shaders and are correctly reflected, but Plasma does not support binding resources to them right now. // Header: #pragma once #include <Shaders/Common/ConstantBufferMacros.h> struct PL_SHADER_STRUCT plAppendData { FLOAT2(Value); }; // Shader: AppendStructuredBuffer<plAppendData> appendStructuredBuffer; ConsumeStructuredBuffer<plAppendData> consumeStructuredBuffer; See Also Shaders ShaderCompiler","title":"Shaders Resources"},{"location":"graphics/shaders/shader-resources/#shaders-resources","text":"Shader resources are things like textures, samplers constant buffers etc. that need to be separately bound in the renderer for the shader to function. Each resource must be bound to a set and slot. Depending on the platform used, the requirements for this binding can be very different. E.g. in Vulkan slot assignments must be unique within a set across all stages while in DX11 most slots only need to be unique within a stage. Not following these rules will result in a runtime error. Manually assigning slots is an option but is very tedious. To make this easier, the shader system can automate this process provided some constraints are met how resourced are declared. Currently, Plasma does not support arrays of resources like Texture2D Diffuse[3] in its shaders. Resources must have unique names across all shader stages. The same resource name can be used in multiple stages as long as the resource it maps to is exactly the same.","title":"Shaders Resources"},{"location":"graphics/shaders/shader-resources/#resource-binding","text":"The shader system only supports the DX11 / DX12 register syntax for resource binding. Both the set and slot can be bound. If no set is given, it is implicitly set 0. Here is a list of a few examples of how to bind resources properly: Texture2D Diffuse : register(t3, space1); // DX12 syntax, slot 3, set 1 SamplerState MySampler : register(s4); // DX11 syntax slot 4, set 0 (default) ByteAddressBuffer MyBuffer BIND_RESOURCE(SLOT_AUTO, SET_RENDER_PASS); // Slot Auto, set 1 ByteAddressBuffer MyBuffer2 BIND_SET(SET_RENDER_PASS); // Slot Auto, set 1 CONSTANT_BUFFER(plTestPositions, 1) // Slot 1, set 0 (default) { ... }; CONSTANT_BUFFER2(plTestPositions, SLOT_AUTO, SET_MATERIAL) // Slot Auto, set 2 { ... }; The HLSL register syntax is a bit impractical, so the macros BIND_RESOURCE(Slot, Set) and BIND_SET(Set) were introduced. These will generate invalid HLSL code which the shader compiler will eventually parse, organize and patch to do the correct thing on each platform. In most cases, you should only be concerned about deciding in which set a resource should reside in. Either use the macro SLOT_AUTO when setting a slot or just use the BIND_SET macro which omits the slot entirely. While you can set any integer for the set, some platforms like Vulkan have a limit on how many sets can be managed at the same time with a minimum of four. Plasma defines macros for these four sets: SET_FRAME , SET_RENDER_PASS , SET_MATERIAL and SET_DRAW_CALL . Resources should ideally be bound to these sets according to their update frequency.","title":"Resource Binding"},{"location":"graphics/shaders/shader-resources/#constant-buffers","text":"Constant buffers map to plGALShaderResourceType::ConstantBuffer in C++. To facilitate C++ interop, constant buffers should be placed into a separate header file that looks like this: #pragma once #include <Shaders/Common/ConstantBufferMacros.h> CONSTANT_BUFFER(plTestPositions, 3) { FLOAT4(Vertex0); FLOAT4(Vertex1); FLOAT4(Vertex2); }; By using the macros defined in ConstantBufferMacros.h like CONSTANT_BUFFER and the data types like FLOAT4 , the file can be included in both shader and C++ code. This makes it easy to create an instance of the constant buffer as a C++ struct in code to update it. Care must be taken to ensure that the constant buffer has the same layout in C++ and HLSL though: 1. Make sure that the size of your struct is a multiple of 16 bytes. Fill out any missing bytes with dummy FLOAT1 entries. 2. A FLOAT3 can't be followed by another FLOAT3 . It should be followed by a FLOAT1 first or some other types of the same byte counts to ensure the next FLOAT3 starts at a 16 byte boundary. This is necessary as the layout rules are different between HLSL and C++.","title":"Constant Buffers"},{"location":"graphics/shaders/shader-resources/#push-constants","text":"Push constants map to plGALShaderResourceType::PushConstants in C++. Push constants allow for fast updates of a small set of bytes. Usually at least 128 bytes. You can check plGALDeviceCapabilities::m_uiMaxPushConstantsSize for the max push constant buffer size. On platforms that don't support push constants like DX11, this is emulated via a constant buffer. Only one push constants block is supported across all shader stages of a shader. Like with constant buffers, special macros have to be used and the declaration should be put into a separate header so it can be included in both shader and C++ code: // Header: #pragma once #include <Shaders/Common/ConstantBufferMacros.h> BEGIN_PUSH_CONSTANTS(plTestData) { FLOAT4(VertexColor); FLOAT4(Vertex0); FLOAT4(Vertex1); FLOAT4(Vertex2); } END_PUSH_CONSTANTS(plTestData) // Shader: float4 main(VS_OUT a) : SV_Target { return GET_PUSH_CONSTANT(plTestData, VertexColor); } // C++: plTestData constants; constants.VertexColor = ...; pContext->SetPushConstants(\"plTestData\", constants); The BEGIN_PUSH_CONSTANTS and END_PUSH_CONSTANTS macros define the struct. Unlike with constant buffers, you can't simply access the values inside a shader by just the name of the variable, e.g. VertexColor . This is because depending on the platform, a different syntax needs to be used to access the content. To make the same shader compile on all platforms, you need to use the GET_PUSH_CONSTANT(Name, Constant) macro to access a member of the push constant buffer.","title":"Push Constants"},{"location":"graphics/shaders/shader-resources/#samplers","text":"Samplers map to plGALShaderResourceType::Sampler or plGALShaderResourceType::TextureAndSampler in C++. Two types of samplers are supported: SamplerState and SamplerComparisonState . The naming of the samplers is important, as it can be used to optimize your workflow. Plasma has a concept of immutable Samplers, these samplers are automatically bound so you can use them in the shader without needing to define them in C++. Immutable samplers are registered in code via plGALImmutableSamplers::RegisterImmutableSampler . Currently, these samplers are registered: LinearSampler , LinearClampSampler , PointSampler and PointClampSampler . Plasma does not allow for two different resources to have the same name, the only exception is textures and samplers which can have the same name by calling the sampler NAME_AutoSampler . The compiler will rename the sampler to NAME and on platforms that support combined image samplers both will be combined into a single resource of type plGALShaderResourceType::TextureAndSampler . The benefit of this approach is that when binding a texture resource to a material for example, the texture resource can define both the texture as well as the sampler state, binding both to the same name. SamplerState DiffuseSampler; SamplerComparisonState ShadowSampler; // Auto sampler combines with texture of the same name: Texture2D BaseTexture; SamplerState BaseTexture_AutoSampler;","title":"Samplers"},{"location":"graphics/shaders/shader-resources/#textures","text":"Textures map to plGALShaderResourceType::Texture or plGALShaderResourceType::TextureAndSampler in C++ (see samplers above). Plasma supports all HLSL texture types except for 1D textures. You can work around this by creating 1xN 2DTextures. Texture1D texture1D; // 1D textures currently not supported. Texture1DArray texture1DArray; // 1D textures currently not supported. Texture2D texture2D; Texture2DArray texture2DArray; Texture2DMS<float4> texture2DMS; Texture2DMSArray<float4> texture2DMSArray; Texture3D texture3D; TextureCube textureCube; TextureCubeArray textureCubeArray; Read-write variants are also supported and map to plGALShaderResourceType::TextureRW in C++. RWTexture1D<float> rwTexture1D; // 1D textures currently not supported. RWTexture1DArray<float2> rwTexture1DArray; // 1D textures currently not supported. RWTexture2D<float3> rwTexture2D; RWTexture2DArray<float4> rwTexture2DArray; RWTexture3D<uint> rwTexture3D;","title":"Textures"},{"location":"graphics/shaders/shader-resources/#buffers","text":"There are three types of buffers supported by Plasma: 1. HLSL's Buffer<T> type is very similar to a 1D texture. A buffer of the same type T needs to be bound to the resource. Maps to plGALShaderResourceType::TexelBuffer in C++. 2. StructuredBuffer<T> should follow the same rules as for constant buffers: Put the declaration in a separate header file to allow access to it from C++ and ensure each struct is 16 bytes aligned. Maps to plGALShaderResourceType::StructuredBuffer in C++. 3. ByteAddressBuffer in just an array of bytes. A raw buffer needs to be bound to the resource. With HLSL 5.1, you can cast any offset of the buffer into a struct. Maps to plGALShaderResourceType::StructuredBuffer in C++. // Header: #pragma once #include <Shaders/Common/ConstantBufferMacros.h> struct PL_SHADER_STRUCT PerInstanceData { TRANSFORM(ObjectToWorld); }; // Shader: Buffer<uint> buffer; StructuredBuffer<PerInstanceData> structuredBuffer; ByteAddressBuffer byteAddressBuffer; Read-write variants of these buffers are also supported and map to plGALShaderResourceType::TexelBufferRW and plGALShaderResourceType::StructuredBufferRW respectively. RWBuffer<uint> rwBuffer; RWStructuredBuffer<plPerInstanceData> rwStructuredBuffer; RWByteAddressBuffer rwByteAddressBuffer;","title":"Buffers"},{"location":"graphics/shaders/shader-resources/#append-consume-buffers","text":"TODO: Future work: Append / consume buffers can be defined in shaders and are correctly reflected, but Plasma does not support binding resources to them right now. // Header: #pragma once #include <Shaders/Common/ConstantBufferMacros.h> struct PL_SHADER_STRUCT plAppendData { FLOAT2(Value); }; // Shader: AppendStructuredBuffer<plAppendData> appendStructuredBuffer; ConsumeStructuredBuffer<plAppendData> consumeStructuredBuffer;","title":"Append / Consume Buffers"},{"location":"graphics/shaders/shader-resources/#see-also","text":"Shaders ShaderCompiler","title":"See Also"},{"location":"graphics/shaders/shader-templates/","text":"Shader Templates Writing a custom shader from scratch is a rather difficult task, especially if you want a custom material shader, that has all the PBR lighting features. However, using shader templates this becomes quite straight forward. A shader template is simply a functioning shader file that you can edit. Shaders are generally written in HLSL , though Plasma shader files have additional sections for configuring various other aspects. There are different templates available for common use cases. Creating a New Shader from a Template Open a material asset that should use a custom shader. As it's ShaderMode select From File . Now click the browse ( ... ) button next to the Shader property, to open the context menu: From this menu select Create Shader From Template... . The following dialog is shown: Select the output file name and which template to create the shader from. When you select a template, the area below shows additional options. These simply select what code from the template should be included in your new file and is purely for convenience to get started more easily. You can add and remove any feature manually afterwards as needed. Once you click OK the shader file is created, applied to the material and opened in a text editor for you to start editing. When you make structural changes (exposed parameters, permutation variables used and such) you need to transform the material for the changes to properly show up. As long as you only modify the HLSL code, you can also just press F4 to reload resources at any time. If there are any errors, they are shown in the log . Adding a Shader Template If you want to add another shader template itself, have a look at the folder Data/Tools/plEditor/ShaderTemplates . All you need to do is add another .plShaderTemplate file here. Shader templates have an additional TEMPLATE_VARS section at the beginning that define what options to display in the dialog. The final shader code is built by running a C preprocessor over the file, but instead of replacing #if / #endif sections, it only looks at %if / %endif (using a percentage sign rather than a hash), to generate the output. See Also Shaders ShaderCompiler","title":"Shader Templates"},{"location":"graphics/shaders/shader-templates/#shader-templates","text":"Writing a custom shader from scratch is a rather difficult task, especially if you want a custom material shader, that has all the PBR lighting features. However, using shader templates this becomes quite straight forward. A shader template is simply a functioning shader file that you can edit. Shaders are generally written in HLSL , though Plasma shader files have additional sections for configuring various other aspects. There are different templates available for common use cases.","title":"Shader Templates"},{"location":"graphics/shaders/shader-templates/#creating-a-new-shader-from-a-template","text":"Open a material asset that should use a custom shader. As it's ShaderMode select From File . Now click the browse ( ... ) button next to the Shader property, to open the context menu: From this menu select Create Shader From Template... . The following dialog is shown: Select the output file name and which template to create the shader from. When you select a template, the area below shows additional options. These simply select what code from the template should be included in your new file and is purely for convenience to get started more easily. You can add and remove any feature manually afterwards as needed. Once you click OK the shader file is created, applied to the material and opened in a text editor for you to start editing. When you make structural changes (exposed parameters, permutation variables used and such) you need to transform the material for the changes to properly show up. As long as you only modify the HLSL code, you can also just press F4 to reload resources at any time. If there are any errors, they are shown in the log .","title":"Creating a New Shader from a Template"},{"location":"graphics/shaders/shader-templates/#adding-a-shader-template","text":"If you want to add another shader template itself, have a look at the folder Data/Tools/plEditor/ShaderTemplates . All you need to do is add another .plShaderTemplate file here. Shader templates have an additional TEMPLATE_VARS section at the beginning that define what options to display in the dialog. The final shader code is built by running a C preprocessor over the file, but instead of replacing #if / #endif sections, it only looks at %if / %endif (using a percentage sign rather than a hash), to generate the output.","title":"Adding a Shader Template"},{"location":"graphics/shaders/shader-templates/#see-also","text":"Shaders ShaderCompiler","title":"See Also"},{"location":"graphics/shaders/shaders-overview/","text":"Shaders Shaders are files with the .plShader extension. These files not only provide the HLSL code for each shader stage used, but also the complete render state used when drawing with this shader. Several permutations of the same shader can exist. Permutations can inpact the render state or affect the HLSL source code. Thus, one shader file can produce several outputs. Shader Sections Each shader is made up of several sections . Not all sections need to be defined as most have a default state. Here is a very simple shader: [PLATFORMS] ALL [PERMUTATIONS] [RENDERSTATE] [SHADER] cbuffer PerObject : register(b1) { float4x4 mvp : packoffset(c0); }; struct VS_IN { float3 pos : POSITION; float2 texcoord0 : TEXCOORD0; }; struct VS_OUT { float4 pos : SV_Position; float2 texcoord0 : TEXCOORD0; }; typedef VS_OUT PS_IN; [VERTEXSHADER] VS_OUT main(VS_IN Input) { VS_OUT RetVal; RetVal.pos = mul(mvp, float4(Input.pos, 1.0)); RetVal.texcoord0 = Input.texcoord0; return RetVal; } [PIXELSHADER] Texture2D DiffuseTexture; SamplerState PointClampSampler; float4 main(PS_IN Input) : SV_Target { return DiffuseTexture.Sample(PointClampSampler, Input.texcoord0); } The following sections are supported: PLATFORMS The PLATFORMS section lists the shader platforms that are supported by this shader and for which the shader should be compiled. Currently, these values are supported: * ALL : The shader is supported on all platforms. * DEBUG : If set, the shader is not optimized and contains debug information to allow stepping through it in tools like RenderDoc. * VULKAN : The shader will be compiled as SPIRV code for the Vulkan renderer. * DX11_SM40_93 : DX9 feature set. Used by the DX11 renderer when using downlevel hardware support. * DX11_SM40 : DX10 feature set. Used by the DX11 renderer when using downlevel hardware support. * DX11_SM41 : DX10.1 feature set. Used by the DX11 renderer when using downlevel hardware support. * DX11_SM50 : DX11 feature set. Default platform used by the DX11 renderer. [PLATFORMS] ALL DEBUG PERMUTATIONS The PERMUTATIONS section defines permutation variables which allow for modofication of the shader code. Each variable is exposed as a preprocessor variable, allowing for various sections of the shader to be modifed via preprocessor blocks. The values of these permutation variables are defined by the engine / material and the entire system is explained in detail in the dedicated shader permutation variables page. [PERMUTATIONS] ALPHATEST CAMERA_MODE = CAMERA_MODE_PERSPECTIVE MATERIALPARAMETER, MATERIALCONFIG If the shader is used as a material shader, two more sections are used: * MATERIALPARAMETER : Used to define which permutation variables should be allowed to be changed on a material in the editor. This is explained in detail in exposing permutation variables to materials . * MATERIALCONFIG : This section controls when a material is rendered during a frame. This is done by this line: RenderDataCategory = LitOpaque . You can use the preprocessor to change the value depending on some permutation variable, if neccessary. The valid values for RenderDataCategory are defined in code via plRenderData::RegisterCategory . Commonly used values are LitOpaque for opaque materials, LitMasked for alpha-tested materials and LitTransparent for alpha blended materials. [MATERIALCONFIG] #if (BLEND_MODE == BLEND_MODE_OPAQUE) RenderDataCategory = LitOpaque #elif (BLEND_MODE == BLEND_MODE_MASKED) RenderDataCategory = LitMasked #else RenderDataCategory = LitTransparent #endif RENDERSTATE Each shader defines the complete state of the renderer. This includes, but is not limited to blendind, rasterizer, depth stencil etc. You can use permutations variables and preprcessor macros to change the render state of shader permutations. This is explained in more detail on the shader render state page. [RENDERSTATE] if WIREFRAME == 1 WireFrame = true endif SHADER The SHADER section contains code that is shared among all shader stages. The content is simply prepended to all used stages before compiling. *SHADER stages Each shader stage has its own section. The following stages are supported: VERTEXSHADER , HULLSHADER , DOMAINSHADER , GEOMETRYSHADER , PIXELSHADER and COMPUTESHADER . Even if you define a shader section, you can use the preprocessor to remove its content via permutation variables, allowing you to remove stages from certain permutations. The entry point into each stage must be called main . The shader code supports preprocessor macros that are defined by permutation variables as well as include directives. Beyond that, any HLSL code is fine as long as it compiles on the platforms the shader defines. However, when defining resources, special care must be taken to ensure no conflicting resource mappings are created between the stages. Please refer to the shader resource page for further details and on how to facilitate interop with the C++ code. TEMPLATE_VARS This section is only used when creating a shader template . See Also Shader Render State Shader Permutation Variables Shader Resources Shader Templates ShaderCompiler Render Pipeline (TODO)","title":"Shaders"},{"location":"graphics/shaders/shaders-overview/#shaders","text":"Shaders are files with the .plShader extension. These files not only provide the HLSL code for each shader stage used, but also the complete render state used when drawing with this shader. Several permutations of the same shader can exist. Permutations can inpact the render state or affect the HLSL source code. Thus, one shader file can produce several outputs.","title":"Shaders"},{"location":"graphics/shaders/shaders-overview/#shader-sections","text":"Each shader is made up of several sections . Not all sections need to be defined as most have a default state. Here is a very simple shader: [PLATFORMS] ALL [PERMUTATIONS] [RENDERSTATE] [SHADER] cbuffer PerObject : register(b1) { float4x4 mvp : packoffset(c0); }; struct VS_IN { float3 pos : POSITION; float2 texcoord0 : TEXCOORD0; }; struct VS_OUT { float4 pos : SV_Position; float2 texcoord0 : TEXCOORD0; }; typedef VS_OUT PS_IN; [VERTEXSHADER] VS_OUT main(VS_IN Input) { VS_OUT RetVal; RetVal.pos = mul(mvp, float4(Input.pos, 1.0)); RetVal.texcoord0 = Input.texcoord0; return RetVal; } [PIXELSHADER] Texture2D DiffuseTexture; SamplerState PointClampSampler; float4 main(PS_IN Input) : SV_Target { return DiffuseTexture.Sample(PointClampSampler, Input.texcoord0); } The following sections are supported:","title":"Shader Sections"},{"location":"graphics/shaders/shaders-overview/#platforms","text":"The PLATFORMS section lists the shader platforms that are supported by this shader and for which the shader should be compiled. Currently, these values are supported: * ALL : The shader is supported on all platforms. * DEBUG : If set, the shader is not optimized and contains debug information to allow stepping through it in tools like RenderDoc. * VULKAN : The shader will be compiled as SPIRV code for the Vulkan renderer. * DX11_SM40_93 : DX9 feature set. Used by the DX11 renderer when using downlevel hardware support. * DX11_SM40 : DX10 feature set. Used by the DX11 renderer when using downlevel hardware support. * DX11_SM41 : DX10.1 feature set. Used by the DX11 renderer when using downlevel hardware support. * DX11_SM50 : DX11 feature set. Default platform used by the DX11 renderer. [PLATFORMS] ALL DEBUG","title":"PLATFORMS"},{"location":"graphics/shaders/shaders-overview/#permutations","text":"The PERMUTATIONS section defines permutation variables which allow for modofication of the shader code. Each variable is exposed as a preprocessor variable, allowing for various sections of the shader to be modifed via preprocessor blocks. The values of these permutation variables are defined by the engine / material and the entire system is explained in detail in the dedicated shader permutation variables page. [PERMUTATIONS] ALPHATEST CAMERA_MODE = CAMERA_MODE_PERSPECTIVE","title":"PERMUTATIONS"},{"location":"graphics/shaders/shaders-overview/#materialparameter-materialconfig","text":"If the shader is used as a material shader, two more sections are used: * MATERIALPARAMETER : Used to define which permutation variables should be allowed to be changed on a material in the editor. This is explained in detail in exposing permutation variables to materials . * MATERIALCONFIG : This section controls when a material is rendered during a frame. This is done by this line: RenderDataCategory = LitOpaque . You can use the preprocessor to change the value depending on some permutation variable, if neccessary. The valid values for RenderDataCategory are defined in code via plRenderData::RegisterCategory . Commonly used values are LitOpaque for opaque materials, LitMasked for alpha-tested materials and LitTransparent for alpha blended materials. [MATERIALCONFIG] #if (BLEND_MODE == BLEND_MODE_OPAQUE) RenderDataCategory = LitOpaque #elif (BLEND_MODE == BLEND_MODE_MASKED) RenderDataCategory = LitMasked #else RenderDataCategory = LitTransparent #endif","title":"MATERIALPARAMETER, MATERIALCONFIG"},{"location":"graphics/shaders/shaders-overview/#renderstate","text":"Each shader defines the complete state of the renderer. This includes, but is not limited to blendind, rasterizer, depth stencil etc. You can use permutations variables and preprcessor macros to change the render state of shader permutations. This is explained in more detail on the shader render state page. [RENDERSTATE]","title":"RENDERSTATE"},{"location":"graphics/shaders/shaders-overview/#if-wireframe-1","text":"WireFrame = true","title":"if WIREFRAME == 1"},{"location":"graphics/shaders/shaders-overview/#endif","text":"","title":"endif"},{"location":"graphics/shaders/shaders-overview/#shader","text":"The SHADER section contains code that is shared among all shader stages. The content is simply prepended to all used stages before compiling.","title":"SHADER"},{"location":"graphics/shaders/shaders-overview/#shader-stages","text":"Each shader stage has its own section. The following stages are supported: VERTEXSHADER , HULLSHADER , DOMAINSHADER , GEOMETRYSHADER , PIXELSHADER and COMPUTESHADER . Even if you define a shader section, you can use the preprocessor to remove its content via permutation variables, allowing you to remove stages from certain permutations. The entry point into each stage must be called main . The shader code supports preprocessor macros that are defined by permutation variables as well as include directives. Beyond that, any HLSL code is fine as long as it compiles on the platforms the shader defines. However, when defining resources, special care must be taken to ensure no conflicting resource mappings are created between the stages. Please refer to the shader resource page for further details and on how to facilitate interop with the C++ code.","title":"*SHADER stages"},{"location":"graphics/shaders/shaders-overview/#template_vars","text":"This section is only used when creating a shader template .","title":"TEMPLATE_VARS"},{"location":"graphics/shaders/shaders-overview/#see-also","text":"Shader Render State Shader Permutation Variables Shader Resources Shader Templates ShaderCompiler Render Pipeline (TODO)","title":"See Also"},{"location":"physics/jolt-debug-visualizations/","text":"Jolt Debug Visualizations To debug physics issues it can be very valuable to visualize certain aspects of the Jolt simulation. Debug visualizations are enabled through CVars . Visualize Jolt Geometry These CVars enable rendering of the Jolt collision geometry: Jolt.Visualize.Geometry : Enables visualization of physics collision geometry. Jolt.Visualize.Exclusive : Disables rendering of regular geometry. Jolt.Visualize.Distance : Configures the distance up to which Jolt geomtry gets extracted from objects. The collision geometry is rendered using these color codes: light blue = static geometry dark blue = kinematic yellow = dynamic bodies green = query shapes (or hitboxes ) pink = ragdolls and ropes Jolt Rope Component red = soft bodies transparent = triggers Debug Draw Constraints The debug draw CVars enable wireframe overlays which are mainly useful to debug issues with constraints. This visualization has a high performance impact and thus should only be used in very small test scenes. These debug draw CVars are available: Jolt.DebugDraw.Bodies : Enables visualization of physics bodies. Jolt.DebugDraw.Constraints : Enables basic visualization of constraints . Jolt.DebugDraw.ConstraintFrames : Enables more detailed visualization of constraints. Jolt.DebugDraw.ConstraintLimits : Enables visualization of constraint limits. See Also Jolt Integration Jolt Physics Settings Component","title":"Jolt Debug Visualizations"},{"location":"physics/jolt-debug-visualizations/#jolt-debug-visualizations","text":"To debug physics issues it can be very valuable to visualize certain aspects of the Jolt simulation. Debug visualizations are enabled through CVars .","title":"Jolt Debug Visualizations"},{"location":"physics/jolt-debug-visualizations/#visualize-jolt-geometry","text":"These CVars enable rendering of the Jolt collision geometry: Jolt.Visualize.Geometry : Enables visualization of physics collision geometry. Jolt.Visualize.Exclusive : Disables rendering of regular geometry. Jolt.Visualize.Distance : Configures the distance up to which Jolt geomtry gets extracted from objects. The collision geometry is rendered using these color codes: light blue = static geometry dark blue = kinematic yellow = dynamic bodies green = query shapes (or hitboxes ) pink = ragdolls and ropes Jolt Rope Component red = soft bodies transparent = triggers","title":"Visualize Jolt Geometry"},{"location":"physics/jolt-debug-visualizations/#debug-draw-constraints","text":"The debug draw CVars enable wireframe overlays which are mainly useful to debug issues with constraints. This visualization has a high performance impact and thus should only be used in very small test scenes. These debug draw CVars are available: Jolt.DebugDraw.Bodies : Enables visualization of physics bodies. Jolt.DebugDraw.Constraints : Enables basic visualization of constraints . Jolt.DebugDraw.ConstraintFrames : Enables more detailed visualization of constraints. Jolt.DebugDraw.ConstraintLimits : Enables visualization of constraint limits.","title":"Debug Draw Constraints"},{"location":"physics/jolt-debug-visualizations/#see-also","text":"Jolt Integration Jolt Physics Settings Component","title":"See Also"},{"location":"physics/jolt-overview/","text":"Jolt Physics Integration Jolt Physics is an open source physics engine. It computes the physical interactions between objects using rigid body dynamics . Physics engines are a vital part in most 3D games, to make objects collide and interact with each other convincingly. An important feature are also raycasts and shape queries which are used to detect objects and analyze the state of the world. Enable Jolt Support Support for Jolt is enabled by default on all platforms. It can be disabled in the CMake config . Working with Jolt The most important Jolt functionality is exposed through components, as well as through TypeScript . When you write custom C++ code, you can access the most important functionality, like raycasts and shape queries, through the abstract plPhysicsWorldModuleInterface , which is implementation independent. If you need to access Jolt features that are not exposed in Plasma, you can cast that interface to plJoltWorldModule and directly work with the JPH::PhysicsSystem . For Jolt details, refer to its documentation . Feature Overview You use components to tell Jolt which objects should be considered for its simulation, and how. In Jolt, objects participating in the simulation are called bodies but in Plasma they are usually referred to as actors . How to set up actors is described here . Reading up on actors is the best starting point. Actors are made up of shapes, such as spheres, boxes, capsules and meshes. Shapes are described here . Actors can be physically linked, to constrain their movement. This is how you would set up a door hinge for example. Linking two actors is accomplished using constraints . To make a player or NPC walk through a physically simulated scene, you need something that computes how the character collides with walls, climbs stairs, slides down slopes, and so on. This functionality is provided by a so called character controller . Often games have invisible areas that either need to be reached as a goal, or that activate something. Such areas are called triggers . Several non-Jolt components either use the available physics engine, or even expose new functionality. For example the raycast placement component does a raycast (using the abstract physics interface) and exposes the hit position to the user by moving a linked object there. The area damage component does a shape query and both damages and pushes the found physical objects. See Also Jolt Physics Jolt Architecture Jolt Actors","title":"Jolt Physics Integration"},{"location":"physics/jolt-overview/#jolt-physics-integration","text":"Jolt Physics is an open source physics engine. It computes the physical interactions between objects using rigid body dynamics . Physics engines are a vital part in most 3D games, to make objects collide and interact with each other convincingly. An important feature are also raycasts and shape queries which are used to detect objects and analyze the state of the world.","title":"Jolt Physics Integration"},{"location":"physics/jolt-overview/#enable-jolt-support","text":"Support for Jolt is enabled by default on all platforms. It can be disabled in the CMake config .","title":"Enable Jolt Support"},{"location":"physics/jolt-overview/#working-with-jolt","text":"The most important Jolt functionality is exposed through components, as well as through TypeScript . When you write custom C++ code, you can access the most important functionality, like raycasts and shape queries, through the abstract plPhysicsWorldModuleInterface , which is implementation independent. If you need to access Jolt features that are not exposed in Plasma, you can cast that interface to plJoltWorldModule and directly work with the JPH::PhysicsSystem . For Jolt details, refer to its documentation .","title":"Working with Jolt"},{"location":"physics/jolt-overview/#feature-overview","text":"You use components to tell Jolt which objects should be considered for its simulation, and how. In Jolt, objects participating in the simulation are called bodies but in Plasma they are usually referred to as actors . How to set up actors is described here . Reading up on actors is the best starting point. Actors are made up of shapes, such as spheres, boxes, capsules and meshes. Shapes are described here . Actors can be physically linked, to constrain their movement. This is how you would set up a door hinge for example. Linking two actors is accomplished using constraints . To make a player or NPC walk through a physically simulated scene, you need something that computes how the character collides with walls, climbs stairs, slides down slopes, and so on. This functionality is provided by a so called character controller . Often games have invisible areas that either need to be reached as a goal, or that activate something. Such areas are called triggers . Several non-Jolt components either use the available physics engine, or even expose new functionality. For example the raycast placement component does a raycast (using the abstract physics interface) and exposes the hit position to the user by moving a linked object there. The area damage component does a shape query and both damages and pushes the found physical objects.","title":"Feature Overview"},{"location":"physics/jolt-overview/#see-also","text":"Jolt Physics Jolt Architecture Jolt Actors","title":"See Also"},{"location":"physics/jolt-settings-component/","text":"Jolt Physics Settings Component The Jolt physics settings component is used to configure general Jolt simulation options. You can only have one such component in a scene, it is an error to have two or more. If no such component is present, all Jolt settings use default values. Component Properties ObjectGravity : The gravity that is applied to all dynamic actors . This property sets both the direction and strength of the gravity. CharacterGravity : A separate gravity value that is used for characters controllers . In many games the gravity for characters is higher than what's used for regular objects. SteppingMode : The stepping mode determines with what time steps Jolt is updated. This is most relevant when your game doesn't run at a fixed framerate: Variable : Jolt will be stepped every frame with the time delta from the previous frame. This mode will forward any frame rate variations to Jolt unfiltered, which means the time step can vary drastically. This mode has the least overhead, but can also result in an unstable simulation when the framerate varies too much. If your game doesn't use dynamic actors much and you mainly use it for raycasts, character movement and overlap queries, this can be entirely sufficient. Fixed : In this mode Jolt is always stepped with the time delta for the FixedFrameRate . If too little time has passed between frames, the Jolt update is skipped entirely, once the delta has been reached, Jolt is stepped. If the time between two frames is very long, up to MaxSubSteps are done to update Jolt. This mode is the most reliable, producing the most stable and deterministic results, since a variable framerate doesn't introduce any variation in how Jolt is updated. This mode is most suitable when your game runs at a locked framerate. This mode can be problematic, if you do have a variable framerate, especially when a frame can take a very long time. In that case the physics simulation will do up to MaxSubSteps simulation steps to catch up with the passed time. If that is not sufficient, the Jolt update will continue catching up during the next frame. As a result, the speed at which simulated objects move may appear erratic until the simulation has fully caught up with the passed time. SemiFixed : This mode is a compromise between Variable and Fixed . It prefers to use fixed time steps, to achieve good simulation stability. At high framerates it will do shorter update steps, but may also skip the Jolt update until enough time has passed. At low framerates, it will do up to MaxSubSteps per frame, but it will use those to always fully catch up with the time that passed between the frames. FixedFrameRate : The framerate to use for the 'fixed' timesteps. A higher framerate means the simulation will be more stable, but also cost more update steps and therefore performance. MaxSubSteps : The maximum number of simulation steps to do between two frames. This is to introduce an upper bound on the performance cost of the Jolt update during one frame. MaxBodies : For performance reasons, Jolt pre-allocates certain resources once at startup. Therefore you can't have more active bodies than this. The default value should be sufficient for the vast majority of use cases, but if necessary, you can increase the value here. See Also Jolt Integration","title":"Jolt Physics Settings Component"},{"location":"physics/jolt-settings-component/#jolt-physics-settings-component","text":"The Jolt physics settings component is used to configure general Jolt simulation options. You can only have one such component in a scene, it is an error to have two or more. If no such component is present, all Jolt settings use default values.","title":"Jolt Physics Settings Component"},{"location":"physics/jolt-settings-component/#component-properties","text":"ObjectGravity : The gravity that is applied to all dynamic actors . This property sets both the direction and strength of the gravity. CharacterGravity : A separate gravity value that is used for characters controllers . In many games the gravity for characters is higher than what's used for regular objects. SteppingMode : The stepping mode determines with what time steps Jolt is updated. This is most relevant when your game doesn't run at a fixed framerate: Variable : Jolt will be stepped every frame with the time delta from the previous frame. This mode will forward any frame rate variations to Jolt unfiltered, which means the time step can vary drastically. This mode has the least overhead, but can also result in an unstable simulation when the framerate varies too much. If your game doesn't use dynamic actors much and you mainly use it for raycasts, character movement and overlap queries, this can be entirely sufficient. Fixed : In this mode Jolt is always stepped with the time delta for the FixedFrameRate . If too little time has passed between frames, the Jolt update is skipped entirely, once the delta has been reached, Jolt is stepped. If the time between two frames is very long, up to MaxSubSteps are done to update Jolt. This mode is the most reliable, producing the most stable and deterministic results, since a variable framerate doesn't introduce any variation in how Jolt is updated. This mode is most suitable when your game runs at a locked framerate. This mode can be problematic, if you do have a variable framerate, especially when a frame can take a very long time. In that case the physics simulation will do up to MaxSubSteps simulation steps to catch up with the passed time. If that is not sufficient, the Jolt update will continue catching up during the next frame. As a result, the speed at which simulated objects move may appear erratic until the simulation has fully caught up with the passed time. SemiFixed : This mode is a compromise between Variable and Fixed . It prefers to use fixed time steps, to achieve good simulation stability. At high framerates it will do shorter update steps, but may also skip the Jolt update until enough time has passed. At low framerates, it will do up to MaxSubSteps per frame, but it will use those to always fully catch up with the time that passed between the frames. FixedFrameRate : The framerate to use for the 'fixed' timesteps. A higher framerate means the simulation will be more stable, but also cost more update steps and therefore performance. MaxSubSteps : The maximum number of simulation steps to do between two frames. This is to introduce an upper bound on the performance cost of the Jolt update during one frame. MaxBodies : For performance reasons, Jolt pre-allocates certain resources once at startup. Therefore you can't have more active bodies than this. The default value should be sufficient for the vast majority of use cases, but if necessary, you can increase the value here.","title":"Component Properties"},{"location":"physics/jolt-settings-component/#see-also","text":"Jolt Integration","title":"See Also"},{"location":"physics/actors/jolt-actors/","text":"Jolt Actors In Plasma objects that are part of the physical simulation are referred to as actors . Note that inside Jolt Physics, they are called bodies . Every actor has its own simulation state, such as position, velocity, torque, contact points with other actors, and so on. Everything that should participate in the simulation, be it static background geometry, or fully simulated bodies, has to be an actor. In some special cases, for instance for greyboxing geometry , the engine takes care to create collision meshes and actors automatically for you. However, for the most part, you have to set up physics actors yourself. We distinguish between three types of actors: static actors, dynamic actors and triggers . Additionally, dynamic actors can be kinematic . Actors themselves don't have a physical shape. Instead they have to be made up of pieces which hold shape components . Upon creation, every actor traverses the node hierarchy below its owner game object to search for shape components. All shapes that are found are added to the actor as a compound shape . If another actor is found in the process, shapes below that node are ignored, though. This way a single actor can have a complex shape, even if every single piece is only a sphere, box, capsule or other simple shape. Static Actors Static actors represent physical objects that never move. This should be the case for the vast majority of the scene geometry. Static actors are much more efficient to deal with. Also, they are the only actors that can use concave collision geometry , meaning arbitrary triangle meshes. Obviously, those meshes cannot be animated. Static actors are set up by attaching a static actor component to a game object . Dynamic Actors Dynamic actors represent all physical objects that move. The physics simulation furthermore distinguishes between kinematic actors and fully simulated (non-kinematic) actors. Kinematic actors are objects whose transform is determined by the game logic. That means you can freely move them around your scene and they will always end up exactly where you moved them to. Regular actors (non-kinematic ones) are simulated using rigid body simulation. These objects collide with other objects, react to forces such as gravity, bounce off of objects that they collide with and slide or roll across surfaces realistically. Regular actors are used to represent all the physical objects in a world that should react realistically to external stimuli. Kinematic actors are used for everything that needs to move, and should affect the simulated objects, but should itself be under full control of the game logic. Kinematic actors will push other actors out of their way relentlessly. If a kinematic actor moves into another kinematic or static actor, the two will simply pass through each other. Whether a dynamic actor is treated as a kinematic actor or not, is an on/off switch. It is possible to switch this property back and forth at will. Dynamic actors are set up by attaching a dynamic actor component to a game object . Triggers Triggers are a special type of actor. Triggers don't interfere with the simulation, meaning nothing ever collides with them. Instead, triggers monitor whether any other actor overlaps with their volume. If so, they raise an event message to inform other code. Triggers are an efficient solution to detect overlaps, when it is imperative that no overlap is ever missed. If on the other hand you only want to check for overlapping objects at some time or only every couple of seconds, it can be more efficient to just do an overlap check through the physics API. Triggers are set up by attaching a trigger component to a game object . Other Actors Plasma comes with a couple of additional components that end up as physics actors in the simulation, but have additional functionality for specific use cases. For example the query shape actor and the hitbox component can be used to define hit-boxes and the ragdoll component is used to physically simulate creatures. Character Controller A character controller is a special kind of kinematic actor that has convenience functions to move around a scene, slide along obstacles and slopes, and so on. Character controllers are used as very abstract representations of creatures and players and implement the important aspect of moving and colliding properly throughout a physical scene. See Also Jolt Shapes Jolt Static Actor Component Jolt Dynamic Actor Component Jolt Trigger Component","title":"Jolt Actors"},{"location":"physics/actors/jolt-actors/#jolt-actors","text":"In Plasma objects that are part of the physical simulation are referred to as actors . Note that inside Jolt Physics, they are called bodies . Every actor has its own simulation state, such as position, velocity, torque, contact points with other actors, and so on. Everything that should participate in the simulation, be it static background geometry, or fully simulated bodies, has to be an actor. In some special cases, for instance for greyboxing geometry , the engine takes care to create collision meshes and actors automatically for you. However, for the most part, you have to set up physics actors yourself. We distinguish between three types of actors: static actors, dynamic actors and triggers . Additionally, dynamic actors can be kinematic . Actors themselves don't have a physical shape. Instead they have to be made up of pieces which hold shape components . Upon creation, every actor traverses the node hierarchy below its owner game object to search for shape components. All shapes that are found are added to the actor as a compound shape . If another actor is found in the process, shapes below that node are ignored, though. This way a single actor can have a complex shape, even if every single piece is only a sphere, box, capsule or other simple shape.","title":"Jolt Actors"},{"location":"physics/actors/jolt-actors/#static-actors","text":"Static actors represent physical objects that never move. This should be the case for the vast majority of the scene geometry. Static actors are much more efficient to deal with. Also, they are the only actors that can use concave collision geometry , meaning arbitrary triangle meshes. Obviously, those meshes cannot be animated. Static actors are set up by attaching a static actor component to a game object .","title":"Static Actors"},{"location":"physics/actors/jolt-actors/#dynamic-actors","text":"Dynamic actors represent all physical objects that move. The physics simulation furthermore distinguishes between kinematic actors and fully simulated (non-kinematic) actors. Kinematic actors are objects whose transform is determined by the game logic. That means you can freely move them around your scene and they will always end up exactly where you moved them to. Regular actors (non-kinematic ones) are simulated using rigid body simulation. These objects collide with other objects, react to forces such as gravity, bounce off of objects that they collide with and slide or roll across surfaces realistically. Regular actors are used to represent all the physical objects in a world that should react realistically to external stimuli. Kinematic actors are used for everything that needs to move, and should affect the simulated objects, but should itself be under full control of the game logic. Kinematic actors will push other actors out of their way relentlessly. If a kinematic actor moves into another kinematic or static actor, the two will simply pass through each other. Whether a dynamic actor is treated as a kinematic actor or not, is an on/off switch. It is possible to switch this property back and forth at will. Dynamic actors are set up by attaching a dynamic actor component to a game object .","title":"Dynamic Actors"},{"location":"physics/actors/jolt-actors/#triggers","text":"Triggers are a special type of actor. Triggers don't interfere with the simulation, meaning nothing ever collides with them. Instead, triggers monitor whether any other actor overlaps with their volume. If so, they raise an event message to inform other code. Triggers are an efficient solution to detect overlaps, when it is imperative that no overlap is ever missed. If on the other hand you only want to check for overlapping objects at some time or only every couple of seconds, it can be more efficient to just do an overlap check through the physics API. Triggers are set up by attaching a trigger component to a game object .","title":"Triggers"},{"location":"physics/actors/jolt-actors/#other-actors","text":"Plasma comes with a couple of additional components that end up as physics actors in the simulation, but have additional functionality for specific use cases. For example the query shape actor and the hitbox component can be used to define hit-boxes and the ragdoll component is used to physically simulate creatures.","title":"Other Actors"},{"location":"physics/actors/jolt-actors/#character-controller","text":"A character controller is a special kind of kinematic actor that has convenience functions to move around a scene, slide along obstacles and slopes, and so on. Character controllers are used as very abstract representations of creatures and players and implement the important aspect of moving and colliding properly throughout a physical scene.","title":"Character Controller"},{"location":"physics/actors/jolt-actors/#see-also","text":"Jolt Shapes Jolt Static Actor Component Jolt Dynamic Actor Component Jolt Trigger Component","title":"See Also"},{"location":"physics/actors/jolt-dynamic-actor-component/","text":"Jolt Dynamic Actor Component The Jolt dynamic actor component is used to add physical behavior to an object. Dynamic actors are also referred to as rigid bodies . They are simulated by the physics engine. Kinematic vs. Simulated Dynamic actors can be in one of two modes: fully simulated or kinematic . For a kinematic body, the game code dictates its position and rotation, and the physics engine uses this information to push simulated objects out of their way. Kinematic actors are typically used for elevators, doors and other large pieces that are supposed to push other objects away and strictly follow an animation without any physical simulation of their movement. Non-kinematic, or fully simulated objects on the other hand, are fully controlled by the physics engine. Their position and rotation is determined by forces, such as gravity, acting on them, as well as what other static and dynamic objects they collide with. Setting the position of such an actor has no effect, the physics engine will override the value with its own result. To affect a simulated object, you can apply external forces and impulses . For example the area damage component applies an outward impulse to all rigid bodies in its vicinity to push them away. Whether a dynamic actor is kinematic or not is simply a flag and it is possible to toggle that state back and forth at runtime. This for example allows to animate an object along a predetermined path by making it kinematic at first, and then switch it to simulated at the end of its animation, to make it fall and collide realistically from there on. In the video below a property animation (TODO) was used to do exactly that: Mass vs. Density Dynamic actors have a weight. The weight determines how much force it takes to push them and how much they push other rigid bodies. There are two ways to adjust an actor's weight. If you set the Mass property, this is the bodies absolute weight no matter its size and shape. Thus a small stone with mass 10 (kilogram) will appear heavy whereas a huge boulder also with mass 10 will appear light. The other way is to set its Density property instead. In this case the volume of all the attached shapes is computed and scaled by the density. That means the object's final mass will depend on its scale, so a small stone would get a weight of 0.5 (kilogram) whereas a huge boulder would get a weight of 1000 kg. Using densities is more convenient to get started. The default density often already produces believable results. If you create a prefab that is supposed to be instantiated at various sizes, it is best to use density. Important: Physics engines are notoriously bad at dealing with large mass differences. Objects should never be too light or too heavy in general. Objects with a mass below 1 tend to be flung away at ridiculous speeds when they are pushed by heavy objects. Objects with a mass above 100 should be avoided as well. Due to these limitations, it is not advisable to use realistic weights for objects, as many objects would become too light and their simulation would suffer from erratic behavior. Instead, choose a weight somewhere in the 0.5 to 100 range that looks good enough. Consequently, it can often be easier to specify their value as an absolute Mass , instead of trying to achieve the same through the indirect Density . Center Of Mass The center of mass (COM) is the point in space around which an actor spins when a force is applied to it. The COM is computed automatically from the shapes and their masses. It sometimes ends up too high and makes objects tip over too easily. To adjust the center of mass, enable the property CustomCenterOfMass and edit the CenterOfMass property value. OnContact Reactions TODO Simulation Stability Simulated rigid bodies may not act as desired. Some bodies jitter and don't come to rest, others fly off at high speeds after collisions. Some objects may even tunnel through walls, meaning that instead of colliding properly with a wall, they manage to get to the other side. These are all known issues with real-time physics engines. With the limited available computational power they have to do many approximations to achieve the desired real-time performance. Consequently, you have to be careful how you set up your rigid-bodies, to improve simulation stability: Avoid small and thin objects: Thin objects are always problematic. For small objects, consider making their collision shape as large as possible, potentially larger than the graphical representation. Avoid very heavy and very light objects: See Mass vs. Density above for details. Use Continuous Collision Detection (CCD) for important small objects: Continuous collision detection is mainly used to prevent objects from tunneling through other objects. For example a physically simulated grenade may be thrown at a high speed, which means it is prone to get through walls. This is less likely to happen for larger objects. CCD costs extra performance for every object on which it is used, but significantly reduces the likelihood for tunneling to happen. Increase angular damping: Some objects tend to spin too fast after collisions. By increasing angular damping, you can make them come to rest more quickly. Reduce the complexity of the shape: Especially convex meshes are prone to jittering when the mesh has long thin triangles. Build convex meshes by hand to control their complexity, if an automatically created convex mesh results in unstable behavior. Component Properties CollisionLayer : The collision layer to use. Kinematic : See Kinematic vs. Simulated above. StartAsleep : If enabled, the actor starts in the 'sleeping' state and will not be physically simulated until it gets into contact with another active actor. This is a performance optimization to prevent performance spikes after loading a level. If used badly, an object can float in air and not fall down until something else touches it. Make sure to only use this on objects that are convincingly placed to begin with. Mass , Density : See Mass vs. Density above. Surface : The surface to use for this actor's shapes. The surface determines the friction and restitution during simulation, but also determines what effects are spawned when you interact with the object. Note that collision meshes already specify the surface to use. If a surface is selected on the actor, it overrides the mesh's surface. GravityFactor : Adjusts the influence of gravity on this object. If set to zero, it will float in space. LinearDamping , AngularDamping : The damping properties affect how quickly an actor loses momentum and comes to rest. This can be adjusted separately for positional (linear) movement and rotational (angular) movement. ContinuousCollisionDetection : See Simulation Stability above. OnContact : See OnContact Reactions above. CustomCenterOfMass , CenterOfMass : See Center Of Mass above. See Also Jolt Static Actor Component Jolt Shapes Jolt Constraints","title":"Jolt Dynamic Actor Component"},{"location":"physics/actors/jolt-dynamic-actor-component/#jolt-dynamic-actor-component","text":"The Jolt dynamic actor component is used to add physical behavior to an object. Dynamic actors are also referred to as rigid bodies . They are simulated by the physics engine.","title":"Jolt Dynamic Actor Component"},{"location":"physics/actors/jolt-dynamic-actor-component/#kinematic-vs-simulated","text":"Dynamic actors can be in one of two modes: fully simulated or kinematic . For a kinematic body, the game code dictates its position and rotation, and the physics engine uses this information to push simulated objects out of their way. Kinematic actors are typically used for elevators, doors and other large pieces that are supposed to push other objects away and strictly follow an animation without any physical simulation of their movement. Non-kinematic, or fully simulated objects on the other hand, are fully controlled by the physics engine. Their position and rotation is determined by forces, such as gravity, acting on them, as well as what other static and dynamic objects they collide with. Setting the position of such an actor has no effect, the physics engine will override the value with its own result. To affect a simulated object, you can apply external forces and impulses . For example the area damage component applies an outward impulse to all rigid bodies in its vicinity to push them away. Whether a dynamic actor is kinematic or not is simply a flag and it is possible to toggle that state back and forth at runtime. This for example allows to animate an object along a predetermined path by making it kinematic at first, and then switch it to simulated at the end of its animation, to make it fall and collide realistically from there on. In the video below a property animation (TODO) was used to do exactly that:","title":"Kinematic vs. Simulated"},{"location":"physics/actors/jolt-dynamic-actor-component/#mass-vs-density","text":"Dynamic actors have a weight. The weight determines how much force it takes to push them and how much they push other rigid bodies. There are two ways to adjust an actor's weight. If you set the Mass property, this is the bodies absolute weight no matter its size and shape. Thus a small stone with mass 10 (kilogram) will appear heavy whereas a huge boulder also with mass 10 will appear light. The other way is to set its Density property instead. In this case the volume of all the attached shapes is computed and scaled by the density. That means the object's final mass will depend on its scale, so a small stone would get a weight of 0.5 (kilogram) whereas a huge boulder would get a weight of 1000 kg. Using densities is more convenient to get started. The default density often already produces believable results. If you create a prefab that is supposed to be instantiated at various sizes, it is best to use density. Important: Physics engines are notoriously bad at dealing with large mass differences. Objects should never be too light or too heavy in general. Objects with a mass below 1 tend to be flung away at ridiculous speeds when they are pushed by heavy objects. Objects with a mass above 100 should be avoided as well. Due to these limitations, it is not advisable to use realistic weights for objects, as many objects would become too light and their simulation would suffer from erratic behavior. Instead, choose a weight somewhere in the 0.5 to 100 range that looks good enough. Consequently, it can often be easier to specify their value as an absolute Mass , instead of trying to achieve the same through the indirect Density .","title":"Mass vs. Density"},{"location":"physics/actors/jolt-dynamic-actor-component/#center-of-mass","text":"The center of mass (COM) is the point in space around which an actor spins when a force is applied to it. The COM is computed automatically from the shapes and their masses. It sometimes ends up too high and makes objects tip over too easily. To adjust the center of mass, enable the property CustomCenterOfMass and edit the CenterOfMass property value.","title":"Center Of Mass"},{"location":"physics/actors/jolt-dynamic-actor-component/#oncontact-reactions","text":"TODO","title":"OnContact Reactions"},{"location":"physics/actors/jolt-dynamic-actor-component/#simulation-stability","text":"Simulated rigid bodies may not act as desired. Some bodies jitter and don't come to rest, others fly off at high speeds after collisions. Some objects may even tunnel through walls, meaning that instead of colliding properly with a wall, they manage to get to the other side. These are all known issues with real-time physics engines. With the limited available computational power they have to do many approximations to achieve the desired real-time performance. Consequently, you have to be careful how you set up your rigid-bodies, to improve simulation stability: Avoid small and thin objects: Thin objects are always problematic. For small objects, consider making their collision shape as large as possible, potentially larger than the graphical representation. Avoid very heavy and very light objects: See Mass vs. Density above for details. Use Continuous Collision Detection (CCD) for important small objects: Continuous collision detection is mainly used to prevent objects from tunneling through other objects. For example a physically simulated grenade may be thrown at a high speed, which means it is prone to get through walls. This is less likely to happen for larger objects. CCD costs extra performance for every object on which it is used, but significantly reduces the likelihood for tunneling to happen. Increase angular damping: Some objects tend to spin too fast after collisions. By increasing angular damping, you can make them come to rest more quickly. Reduce the complexity of the shape: Especially convex meshes are prone to jittering when the mesh has long thin triangles. Build convex meshes by hand to control their complexity, if an automatically created convex mesh results in unstable behavior.","title":"Simulation Stability"},{"location":"physics/actors/jolt-dynamic-actor-component/#component-properties","text":"CollisionLayer : The collision layer to use. Kinematic : See Kinematic vs. Simulated above. StartAsleep : If enabled, the actor starts in the 'sleeping' state and will not be physically simulated until it gets into contact with another active actor. This is a performance optimization to prevent performance spikes after loading a level. If used badly, an object can float in air and not fall down until something else touches it. Make sure to only use this on objects that are convincingly placed to begin with. Mass , Density : See Mass vs. Density above. Surface : The surface to use for this actor's shapes. The surface determines the friction and restitution during simulation, but also determines what effects are spawned when you interact with the object. Note that collision meshes already specify the surface to use. If a surface is selected on the actor, it overrides the mesh's surface. GravityFactor : Adjusts the influence of gravity on this object. If set to zero, it will float in space. LinearDamping , AngularDamping : The damping properties affect how quickly an actor loses momentum and comes to rest. This can be adjusted separately for positional (linear) movement and rotational (angular) movement. ContinuousCollisionDetection : See Simulation Stability above. OnContact : See OnContact Reactions above. CustomCenterOfMass , CenterOfMass : See Center Of Mass above.","title":"Component Properties"},{"location":"physics/actors/jolt-dynamic-actor-component/#see-also","text":"Jolt Static Actor Component Jolt Shapes Jolt Constraints","title":"See Also"},{"location":"physics/actors/jolt-queryshape-actor-component/","text":"Jolt Query Shape Actor Component The Jolt query shape actor component is a kinematic actor that doesn't interact with the physics simulation. Its intended use is to define areas that you want to be able to detect via raycasts and overlap queries but otherwise shouldn't interfere with the physical simulation. Example The image above shows a mock-up of a lever that a player should be able to activate. The handle of the lever itself is very thin. If you use a raycast to detect what interactable objects a player is looking at, it can be difficult to hit. If the lever handle itself moves after activation, it would also move away from the point the player is looking at, so quickly activating it again becomes even harder. To make this easier, instead of making the lever into a kinematic object and raycast against that, we can use a sphere shape that has a much larger size and therefore is much easier to pick. However, we do not want this exaggerated shape to interfere with other objects. For example when a character controller moves towards it, we do not want it to block the path. Instead it should just pass through. We could achieve this with collision layers , however the more collision layers you have, the harder it becomes to maintain them properly. By using a query shape actor, it is very easy to define that this object only participates in queries , e.g. raycasts and overlap tests. And even then only, if those queries are configured to include query shapes. Component Properties CollisionLayer : The collision layer to use. Note that query shapes do not collide with anything, but this is used for additional filtering when raycasting against these objects. Surface : The surface to use for this actor's shapes. This is only used to report the surface type back in raycasts and such. Otherwise the surface does not affect the actor's behavior in any way. See Also Jolt Actors Jolt Hitbox Component","title":"Jolt Query Shape Actor Component"},{"location":"physics/actors/jolt-queryshape-actor-component/#jolt-query-shape-actor-component","text":"The Jolt query shape actor component is a kinematic actor that doesn't interact with the physics simulation. Its intended use is to define areas that you want to be able to detect via raycasts and overlap queries but otherwise shouldn't interfere with the physical simulation.","title":"Jolt Query Shape Actor Component"},{"location":"physics/actors/jolt-queryshape-actor-component/#example","text":"The image above shows a mock-up of a lever that a player should be able to activate. The handle of the lever itself is very thin. If you use a raycast to detect what interactable objects a player is looking at, it can be difficult to hit. If the lever handle itself moves after activation, it would also move away from the point the player is looking at, so quickly activating it again becomes even harder. To make this easier, instead of making the lever into a kinematic object and raycast against that, we can use a sphere shape that has a much larger size and therefore is much easier to pick. However, we do not want this exaggerated shape to interfere with other objects. For example when a character controller moves towards it, we do not want it to block the path. Instead it should just pass through. We could achieve this with collision layers , however the more collision layers you have, the harder it becomes to maintain them properly. By using a query shape actor, it is very easy to define that this object only participates in queries , e.g. raycasts and overlap tests. And even then only, if those queries are configured to include query shapes.","title":"Example"},{"location":"physics/actors/jolt-queryshape-actor-component/#component-properties","text":"CollisionLayer : The collision layer to use. Note that query shapes do not collide with anything, but this is used for additional filtering when raycasting against these objects. Surface : The surface to use for this actor's shapes. This is only used to report the surface type back in raycasts and such. Otherwise the surface does not affect the actor's behavior in any way.","title":"Component Properties"},{"location":"physics/actors/jolt-queryshape-actor-component/#see-also","text":"Jolt Actors Jolt Hitbox Component","title":"See Also"},{"location":"physics/actors/jolt-static-actor-component/","text":"Jolt Static Actor Component The Jolt static actor component is used to represent static collision geomtry. Most geometry in a scene should be static , meaning that it never moves, rotates, scales or is animated in any way. Static geometry is generally faster to process, and in the case of physics simulations, only static actors may use concave collision geometry. All Jolt shapes that can be found in the hierarchy below the static actor are combined to form the compound shape of the actor. However, if any other actor (static or dynamic) is part of the hierarchy below the static actor, the shapes below that object are ignored for this actor. Additionally, if the static actor itself references a collision mesh , it will also become part of the actor compound shape. Only static actors are able to reference concave triangle collision meshes. If you need your geometry to be able to move, use a dynamic actor instead. Component Properties CollisionLayer : The collision layer defines which objects will collide with this actor. CollisionMesh : An optional convex or concave collision mesh representing the static actor geometry. This will be combined with all shapes found in the hierarchy below the owner object. IncludeInNavmesh : If set, this object will be considered an obstacle for AI and navmeshes are generated around it. PullSurfacesFromGraphicsMesh : If this is enabled, at startup the actor will check whether there is a graphics mesh component attached to the same owner, which has the same amount of materials, as the collision mesh. If so, it will query those materials for their surfaces and use them to override the surfaces that are stored in the collision mesh. This can be very convenient, especially for complex meshes, because you only need to set up the materials for the graphics mesh, and don't need to mirror the same setup on the collision mesh. Also modifications to the graphics mesh (or its materials) will then apply to the collision mesh as well. Enabling this option forces the graphics mesh to be loaded at startup and therefore reduces potential for streaming data in the background. Surface : The surface to use for this actor's shapes. The surface determines the friction and restitution during simulation, but also determines what effects are spawned when you interact with the object. Note that collision meshes already specify the surface to use. If a surface is selected on the actor, it overrides the mesh's surface. See Also Jolt Dynamic Actor Component Jolt Shapes Jolt Collision Meshes","title":"Jolt Static Actor Component"},{"location":"physics/actors/jolt-static-actor-component/#jolt-static-actor-component","text":"The Jolt static actor component is used to represent static collision geomtry. Most geometry in a scene should be static , meaning that it never moves, rotates, scales or is animated in any way. Static geometry is generally faster to process, and in the case of physics simulations, only static actors may use concave collision geometry. All Jolt shapes that can be found in the hierarchy below the static actor are combined to form the compound shape of the actor. However, if any other actor (static or dynamic) is part of the hierarchy below the static actor, the shapes below that object are ignored for this actor. Additionally, if the static actor itself references a collision mesh , it will also become part of the actor compound shape. Only static actors are able to reference concave triangle collision meshes. If you need your geometry to be able to move, use a dynamic actor instead.","title":"Jolt Static Actor Component"},{"location":"physics/actors/jolt-static-actor-component/#component-properties","text":"CollisionLayer : The collision layer defines which objects will collide with this actor. CollisionMesh : An optional convex or concave collision mesh representing the static actor geometry. This will be combined with all shapes found in the hierarchy below the owner object. IncludeInNavmesh : If set, this object will be considered an obstacle for AI and navmeshes are generated around it. PullSurfacesFromGraphicsMesh : If this is enabled, at startup the actor will check whether there is a graphics mesh component attached to the same owner, which has the same amount of materials, as the collision mesh. If so, it will query those materials for their surfaces and use them to override the surfaces that are stored in the collision mesh. This can be very convenient, especially for complex meshes, because you only need to set up the materials for the graphics mesh, and don't need to mirror the same setup on the collision mesh. Also modifications to the graphics mesh (or its materials) will then apply to the collision mesh as well. Enabling this option forces the graphics mesh to be loaded at startup and therefore reduces potential for streaming data in the background. Surface : The surface to use for this actor's shapes. The surface determines the friction and restitution during simulation, but also determines what effects are spawned when you interact with the object. Note that collision meshes already specify the surface to use. If a surface is selected on the actor, it overrides the mesh's surface.","title":"Component Properties"},{"location":"physics/actors/jolt-static-actor-component/#see-also","text":"Jolt Dynamic Actor Component Jolt Shapes Jolt Collision Meshes","title":"See Also"},{"location":"physics/actors/jolt-trigger-component/","text":"Jolt Trigger Component The Jolt trigger component is a special kind of actor that determines whether other actors overlap with its volume. If so, it sends a trigger event message . Other components or script code can react to this message to implement their game logic. Triggers are often used to open and close doors, to check whether a character walked over a pickup item and to detect when the player reached some location. A trigger is set up the same way as a static actor or a dynamic actor , by attaching collision shapes to it. Which other physics objects activate the trigger is determined through the collision layers on the attached shapes. Since triggers are not simulated like rigid bodies, they don't require much configuration. Triggers can be moved around at runtime and they will fire, when an object enters a trigger because the trigger moved into the object. When a trigger fires, it sends the event message plMsgTriggerTriggered . The message states which other object was involved, and whether it entered or left the trigger volume. It will also pass along the TriggerMessage string. This can be used to identify which (kind of) trigger was just triggered. Note: Physics triggers only detect overlaps with other physics objects. For such scenarios they are an efficient solution. If, however, you need to query overlaps with other kinds of objects, you should take a look at the spatial system . To achieve more complex trigger behavior, for instance to only activate something after a delay, you can utilize the trigger delay modifier component . Component Properties CollisionLayer : The collision layer to use. TriggerMessage : The string that should be sent along with the plMsgTriggerTriggered . See Also Trigger Delay Modifier Component Jolt Actors Jolt Shapes Spatial System Marker Component","title":"Jolt Trigger Component"},{"location":"physics/actors/jolt-trigger-component/#jolt-trigger-component","text":"The Jolt trigger component is a special kind of actor that determines whether other actors overlap with its volume. If so, it sends a trigger event message . Other components or script code can react to this message to implement their game logic. Triggers are often used to open and close doors, to check whether a character walked over a pickup item and to detect when the player reached some location. A trigger is set up the same way as a static actor or a dynamic actor , by attaching collision shapes to it. Which other physics objects activate the trigger is determined through the collision layers on the attached shapes. Since triggers are not simulated like rigid bodies, they don't require much configuration. Triggers can be moved around at runtime and they will fire, when an object enters a trigger because the trigger moved into the object. When a trigger fires, it sends the event message plMsgTriggerTriggered . The message states which other object was involved, and whether it entered or left the trigger volume. It will also pass along the TriggerMessage string. This can be used to identify which (kind of) trigger was just triggered. Note: Physics triggers only detect overlaps with other physics objects. For such scenarios they are an efficient solution. If, however, you need to query overlaps with other kinds of objects, you should take a look at the spatial system . To achieve more complex trigger behavior, for instance to only activate something after a delay, you can utilize the trigger delay modifier component .","title":"Jolt Trigger Component"},{"location":"physics/actors/jolt-trigger-component/#component-properties","text":"CollisionLayer : The collision layer to use. TriggerMessage : The string that should be sent along with the plMsgTriggerTriggered .","title":"Component Properties"},{"location":"physics/actors/jolt-trigger-component/#see-also","text":"Trigger Delay Modifier Component Jolt Actors Jolt Shapes Spatial System Marker Component","title":"See Also"},{"location":"physics/collision-shapes/jolt-box-shape-component/","text":"Jolt Box Shape Component The Jolt box shape component adds a box as a shape to the Jolt actor that is attached to the closest parent node. You can attach this component to the same node where the actor component is attached, or you can create a child object to attach it to, which allows you to position the shape relative to the actor. Boxes are relatively efficient for the physics engine to handle. Prefer them over the convex shape component when possible. Component Properties HalfExtents : The width, height and depth of the box shape, from its center position. See Also Jolt Shapes Jolt Actors","title":"Jolt Box Shape Component"},{"location":"physics/collision-shapes/jolt-box-shape-component/#jolt-box-shape-component","text":"The Jolt box shape component adds a box as a shape to the Jolt actor that is attached to the closest parent node. You can attach this component to the same node where the actor component is attached, or you can create a child object to attach it to, which allows you to position the shape relative to the actor. Boxes are relatively efficient for the physics engine to handle. Prefer them over the convex shape component when possible.","title":"Jolt Box Shape Component"},{"location":"physics/collision-shapes/jolt-box-shape-component/#component-properties","text":"HalfExtents : The width, height and depth of the box shape, from its center position.","title":"Component Properties"},{"location":"physics/collision-shapes/jolt-box-shape-component/#see-also","text":"Jolt Shapes Jolt Actors","title":"See Also"},{"location":"physics/collision-shapes/jolt-capsule-shape-component/","text":"Jolt Capsule Shape Component The Jolt capsule shape component adds a capsule as a shape to the Jolt actor that is attached to the closest parent node. You can attach this component to the same node where the actor component is attached, or you can create a child object to attach it to, which allows you to position the shape relative to the actor. Capsules are relatively efficient for the physics engine to handle. Prefer them over the convex shape component when possible. For long thin objects, especially static collision geometry, capsules may also be more efficient and yield better results, than box shapes . Component Properties Radius : The radius of the capsule, ie its thickness. Height : The height or length of the capsule. See Also Jolt Shapes Jolt Actors","title":"Jolt Capsule Shape Component"},{"location":"physics/collision-shapes/jolt-capsule-shape-component/#jolt-capsule-shape-component","text":"The Jolt capsule shape component adds a capsule as a shape to the Jolt actor that is attached to the closest parent node. You can attach this component to the same node where the actor component is attached, or you can create a child object to attach it to, which allows you to position the shape relative to the actor. Capsules are relatively efficient for the physics engine to handle. Prefer them over the convex shape component when possible. For long thin objects, especially static collision geometry, capsules may also be more efficient and yield better results, than box shapes .","title":"Jolt Capsule Shape Component"},{"location":"physics/collision-shapes/jolt-capsule-shape-component/#component-properties","text":"Radius : The radius of the capsule, ie its thickness. Height : The height or length of the capsule.","title":"Component Properties"},{"location":"physics/collision-shapes/jolt-capsule-shape-component/#see-also","text":"Jolt Shapes Jolt Actors","title":"See Also"},{"location":"physics/collision-shapes/jolt-collision-layers/","text":"Jolt Collision Layers Collision layers are a concept used by physics engines to decide which objects should collide with which other objects. For instance, a box thrown into a scene should obviously collide with the scene geometry. There may, however, be things that it should not collide with. For example, you may have triggers that should only be triggered by the player or NPCs, but not by other objects. Collision layers can be used to solve this problem. Collision Layer Matrix Collision layers are evaluated by the physics engine and are therefore the fastest method to reject collisions. Filtering out objects that entered a trigger could be done on a higher level, but having the physics engine filter them out early is best for performance. Due to this, there is a limit of 32 available collision layers. If you require more, you need to rethink your setup and maybe use other methods to handle certain cases. Each layer can be set to collide or not collide with each other layer. Consequently, the available combinations form a matrix of 32x32 entries. Each entry is either enabled or disabled. The matrix is mirrored along the diagonal. If layer A is set to collide with layer B , then of course layer B has to be set to collide with layer A . For convenience, in the editor you can give a name to each layer. The name is irrelevant at runtime, though. Under Project > Plugin Settings > Jolt Project Settings... you can configure the Jolt collision layers, as seen above. Physics Actors and Shapes The collision layer is specified on each actor. All the shapes of a physics actor are in the same collision layer. When two shapes overlap, the physics engine will check their collision layers. If the pair of layers is configured to collide, the two shapes will interact. Otherwise they will pass right through each other. Raycasts and Shape Queries When doing a raycast or other shape query, you also need to provide a collision layer to determine with which physics shapes the ray should collide. Therefore you often need additional collision layers to implement common operations. For example, you may need a layer to handle 'visibility checks'. Those rays should pass through transparent objects, as glass windows should not block the AI from seeing the player. On the other hand, if you want to 'interact' with the scene, you need a different collision layer, one that would not pass through transparent objects but maybe ignore other types of objects. Combinations can quickly add up and therefore you must consider very carefully what filtering must be done by the physics engine already, and what additional filtering could be done in your own code. However, in raycasts and shape queries you can additionally provide other options, for instance to ignore all dynamic or character objects. Therefore, not all filtering functionality has to be exclusively achieved through collision layers. See Also Jolt Integration Jolt Collision Meshes","title":"Jolt Collision Layers"},{"location":"physics/collision-shapes/jolt-collision-layers/#jolt-collision-layers","text":"Collision layers are a concept used by physics engines to decide which objects should collide with which other objects. For instance, a box thrown into a scene should obviously collide with the scene geometry. There may, however, be things that it should not collide with. For example, you may have triggers that should only be triggered by the player or NPCs, but not by other objects. Collision layers can be used to solve this problem.","title":"Jolt Collision Layers"},{"location":"physics/collision-shapes/jolt-collision-layers/#collision-layer-matrix","text":"Collision layers are evaluated by the physics engine and are therefore the fastest method to reject collisions. Filtering out objects that entered a trigger could be done on a higher level, but having the physics engine filter them out early is best for performance. Due to this, there is a limit of 32 available collision layers. If you require more, you need to rethink your setup and maybe use other methods to handle certain cases. Each layer can be set to collide or not collide with each other layer. Consequently, the available combinations form a matrix of 32x32 entries. Each entry is either enabled or disabled. The matrix is mirrored along the diagonal. If layer A is set to collide with layer B , then of course layer B has to be set to collide with layer A . For convenience, in the editor you can give a name to each layer. The name is irrelevant at runtime, though. Under Project > Plugin Settings > Jolt Project Settings... you can configure the Jolt collision layers, as seen above.","title":"Collision Layer Matrix"},{"location":"physics/collision-shapes/jolt-collision-layers/#physics-actors-and-shapes","text":"The collision layer is specified on each actor. All the shapes of a physics actor are in the same collision layer. When two shapes overlap, the physics engine will check their collision layers. If the pair of layers is configured to collide, the two shapes will interact. Otherwise they will pass right through each other.","title":"Physics Actors and Shapes"},{"location":"physics/collision-shapes/jolt-collision-layers/#raycasts-and-shape-queries","text":"When doing a raycast or other shape query, you also need to provide a collision layer to determine with which physics shapes the ray should collide. Therefore you often need additional collision layers to implement common operations. For example, you may need a layer to handle 'visibility checks'. Those rays should pass through transparent objects, as glass windows should not block the AI from seeing the player. On the other hand, if you want to 'interact' with the scene, you need a different collision layer, one that would not pass through transparent objects but maybe ignore other types of objects. Combinations can quickly add up and therefore you must consider very carefully what filtering must be done by the physics engine already, and what additional filtering could be done in your own code. However, in raycasts and shape queries you can additionally provide other options, for instance to ignore all dynamic or character objects. Therefore, not all filtering functionality has to be exclusively achieved through collision layers.","title":"Raycasts and Shape Queries"},{"location":"physics/collision-shapes/jolt-collision-layers/#see-also","text":"Jolt Integration Jolt Collision Meshes","title":"See Also"},{"location":"physics/collision-shapes/jolt-collision-meshes/","text":"Jolt Collision Meshes Collision meshes are special meshes that are used by the physics engine to compute physical interactions. Their internal representation is optimized to speed up this task. Additionally, physics engines generally distinguish between two types of meshes: convex meshes and concave meshes. While concave meshes can represent any arbitrary geometric shape, they can only be used for static physics actors , which limits them to be used for the static level geometry. Convex meshes are often an oversimplification of the original mesh. However, they can be used for all physical interactions. Concave Collision Meshes To create a concave collision mesh, use the asset type Jolt Triangle Collision Mesh when importing an asset . The image above shows a mesh imported as a concave collision mesh. As you can see it represents every detail faithfully. Due to this complexity, the model can only be used for static physics actors , meaning you can place it in a level, scale and rotate it, but you may not move it dynamically during the game and it cannot be used to simulate a rigid body. The complexity of a mesh has direct impact on the performance of the game. Especially small details may result in large computational costs when dynamic objects collide with those detailed areas. If you want to optimize performance, you should author dedicated collision meshes with reduced complexity, instead of using the render mesh directly. Concave collision meshes are set directly on the static physics actor component and have no dedicated physics shape component. Triangle meshes can use a different surface for each submesh. Convex Collision Meshes The simulation of dynamic actors is only possible with convex shapes. To create a convex collision mesh, use the asset type Jolt Convex Collision Mesh when importing an asset . To attach a convex mesh to an actor, use the convex mesh shape component . Convex meshes may only use a single surface , even if the mesh is made up of multiple convex pieces. There are multiple modes how to create the convex collision mesh: Convex Hull In the image above the mesh import computed the convex hull . The number of vertices and triangles was also reduced to less than 250 (a requirement by Jolt). Obviously, the mesh lost all of its details and the object will not collide with its surroundings according to its actual geometry, but in many use cases that won't be obvious. This is the most efficient way to use an arbitrary mesh as a collision mesh, as it will always use exactly one, very low poly convex mesh for the physics calculations. Convex Decomposition In the image above the mesh import decomposed the mesh into multiple pieces (seven pieces in this case). Each piece is a convex mesh with less than 250 vertices and triangles. This mode allows you to dictate into how many pieces to split the mesh. The more pieces, the closer the result resembles the original shape. These collision meshes can still be used for dynamic simulation, the Jolt actors simply use multiple convex shapes as their representation. Of course the more pieces such a mesh contains, the less efficient the simulation becomes. Visualizing Collision Meshes Sometimes you want to visualize the collision mesh of an object within a scene. One way is to use the Jolt debug visualizations . However, for some use cases you can also just attach a Collision Mesh Visualizer component. This renders the collision mesh into your scene the same way as in the images above. See Also Jolt Physics Integration Jolt Physics Shapes Jolt Physics Actors Jolt Collision Layers Jolt Debug Visualizations Surfaces","title":"Jolt Collision Meshes"},{"location":"physics/collision-shapes/jolt-collision-meshes/#jolt-collision-meshes","text":"Collision meshes are special meshes that are used by the physics engine to compute physical interactions. Their internal representation is optimized to speed up this task. Additionally, physics engines generally distinguish between two types of meshes: convex meshes and concave meshes. While concave meshes can represent any arbitrary geometric shape, they can only be used for static physics actors , which limits them to be used for the static level geometry. Convex meshes are often an oversimplification of the original mesh. However, they can be used for all physical interactions.","title":"Jolt Collision Meshes"},{"location":"physics/collision-shapes/jolt-collision-meshes/#concave-collision-meshes","text":"To create a concave collision mesh, use the asset type Jolt Triangle Collision Mesh when importing an asset . The image above shows a mesh imported as a concave collision mesh. As you can see it represents every detail faithfully. Due to this complexity, the model can only be used for static physics actors , meaning you can place it in a level, scale and rotate it, but you may not move it dynamically during the game and it cannot be used to simulate a rigid body. The complexity of a mesh has direct impact on the performance of the game. Especially small details may result in large computational costs when dynamic objects collide with those detailed areas. If you want to optimize performance, you should author dedicated collision meshes with reduced complexity, instead of using the render mesh directly. Concave collision meshes are set directly on the static physics actor component and have no dedicated physics shape component. Triangle meshes can use a different surface for each submesh.","title":"Concave Collision Meshes"},{"location":"physics/collision-shapes/jolt-collision-meshes/#convex-collision-meshes","text":"The simulation of dynamic actors is only possible with convex shapes. To create a convex collision mesh, use the asset type Jolt Convex Collision Mesh when importing an asset . To attach a convex mesh to an actor, use the convex mesh shape component . Convex meshes may only use a single surface , even if the mesh is made up of multiple convex pieces. There are multiple modes how to create the convex collision mesh:","title":"Convex Collision Meshes"},{"location":"physics/collision-shapes/jolt-collision-meshes/#convex-hull","text":"In the image above the mesh import computed the convex hull . The number of vertices and triangles was also reduced to less than 250 (a requirement by Jolt). Obviously, the mesh lost all of its details and the object will not collide with its surroundings according to its actual geometry, but in many use cases that won't be obvious. This is the most efficient way to use an arbitrary mesh as a collision mesh, as it will always use exactly one, very low poly convex mesh for the physics calculations.","title":"Convex Hull"},{"location":"physics/collision-shapes/jolt-collision-meshes/#convex-decomposition","text":"In the image above the mesh import decomposed the mesh into multiple pieces (seven pieces in this case). Each piece is a convex mesh with less than 250 vertices and triangles. This mode allows you to dictate into how many pieces to split the mesh. The more pieces, the closer the result resembles the original shape. These collision meshes can still be used for dynamic simulation, the Jolt actors simply use multiple convex shapes as their representation. Of course the more pieces such a mesh contains, the less efficient the simulation becomes.","title":"Convex Decomposition"},{"location":"physics/collision-shapes/jolt-collision-meshes/#visualizing-collision-meshes","text":"Sometimes you want to visualize the collision mesh of an object within a scene. One way is to use the Jolt debug visualizations . However, for some use cases you can also just attach a Collision Mesh Visualizer component. This renders the collision mesh into your scene the same way as in the images above.","title":"Visualizing Collision Meshes"},{"location":"physics/collision-shapes/jolt-collision-meshes/#see-also","text":"Jolt Physics Integration Jolt Physics Shapes Jolt Physics Actors Jolt Collision Layers Jolt Debug Visualizations Surfaces","title":"See Also"},{"location":"physics/collision-shapes/jolt-convex-shape-component/","text":"Jolt Convex Shape Component The Jolt convex shape component adds a convex mesh as a shape to the Jolt actor that is attached to the closest parent node. You can attach this component to the same node where the actor component is attached, or you can create a child object to attach it to, which allows you to position the shape relative to the actor. Convex mesh shapes are the least efficient shape to handle for the physics engine, but it is also the only shape that allows you to define your own (convex) collision geometry. For many kind of objects this is necessary. The convex shape component references a convex collision mesh , which you need to create first. Note that the editor doesn't visualize convex shape components in any way. The image above was taken by using a collision mesh visualizer component . Component Properties CollisionMesh : The convex collision mesh to use. See Also Jolt Shapes Jolt Actors","title":"Jolt Convex Shape Component"},{"location":"physics/collision-shapes/jolt-convex-shape-component/#jolt-convex-shape-component","text":"The Jolt convex shape component adds a convex mesh as a shape to the Jolt actor that is attached to the closest parent node. You can attach this component to the same node where the actor component is attached, or you can create a child object to attach it to, which allows you to position the shape relative to the actor. Convex mesh shapes are the least efficient shape to handle for the physics engine, but it is also the only shape that allows you to define your own (convex) collision geometry. For many kind of objects this is necessary. The convex shape component references a convex collision mesh , which you need to create first. Note that the editor doesn't visualize convex shape components in any way. The image above was taken by using a collision mesh visualizer component .","title":"Jolt Convex Shape Component"},{"location":"physics/collision-shapes/jolt-convex-shape-component/#component-properties","text":"CollisionMesh : The convex collision mesh to use.","title":"Component Properties"},{"location":"physics/collision-shapes/jolt-convex-shape-component/#see-also","text":"Jolt Shapes Jolt Actors","title":"See Also"},{"location":"physics/collision-shapes/jolt-cylinder-shape-component/","text":"Jolt Cylinder Shape Component The Jolt cylinder shape component adds a cylinder as a shape to the Jolt actor that is attached to the closest parent node. You can attach this component to the same node where the actor component is attached, or you can create a child object to attach it to, which allows you to position the shape relative to the actor. Cylinders are relatively efficient for the physics engine to handle. However, cylinders are known to not always yield the desired behavior. Generally prefer them over the convex shapes , though if the behavior is too erratic, you can try those instead, and see if the results are better. Whenever possible, prefer to use capsule shapes instead. Component Properties Radius : The radius of the cylinder, ie its thickness. Height : The height or length of the cylinder. See Also Jolt Shapes Jolt Actors","title":"Jolt Cylinder Shape Component"},{"location":"physics/collision-shapes/jolt-cylinder-shape-component/#jolt-cylinder-shape-component","text":"The Jolt cylinder shape component adds a cylinder as a shape to the Jolt actor that is attached to the closest parent node. You can attach this component to the same node where the actor component is attached, or you can create a child object to attach it to, which allows you to position the shape relative to the actor. Cylinders are relatively efficient for the physics engine to handle. However, cylinders are known to not always yield the desired behavior. Generally prefer them over the convex shapes , though if the behavior is too erratic, you can try those instead, and see if the results are better. Whenever possible, prefer to use capsule shapes instead.","title":"Jolt Cylinder Shape Component"},{"location":"physics/collision-shapes/jolt-cylinder-shape-component/#component-properties","text":"Radius : The radius of the cylinder, ie its thickness. Height : The height or length of the cylinder.","title":"Component Properties"},{"location":"physics/collision-shapes/jolt-cylinder-shape-component/#see-also","text":"Jolt Shapes Jolt Actors","title":"See Also"},{"location":"physics/collision-shapes/jolt-shapes/","text":"Jolt Shapes A Jolt actor configures how an object behaves in the physics simulation. However, every physical presence also requires to have a 3D shape. The shape of actors is set up using Jolt shape components. Dynamic actors can only be simulated with convex shapes. Therefore concave collision meshes are exclusive to static actors . All shape components represent convex geometry and work with all physics actor types. Shape Components The following shape components are available: Jolt Sphere Shape Component Jolt Box Shape Component Jolt Capsule Shape Component Jolt Cylinder Shape Component Jolt Convex Shape Component Actor Shape Setup The easiest kind of actor shape setup is to simply attach a shape component to the same game object that the actor component is attached to. This way the position of the game object is also the center of the shape, which is often sufficient. For more complex shapes, you can add child nodes below the actor node, attach the shapes to those nodes, and position the nodes as needed. When an actor is initialized for the simulation, it traverses the hierarchy below its owner game object and gathers all shape components. When it encounters another actor component, all shapes below that node are ignored. All shapes that are found this way are added to the actor as one compound shape . This way you can build a single actor that has a complex shape, made up of many parts. You can't add or remove individual shapes during simulation. If you need pieces to be destructible, you need to turn them into separate actors. To still have those actors move in unison, you need to join them using a fixed joint . Friction and Restitution Friction and restitution are the two physical properties that affect a shape's physical behavior the most. See this section for details. See Also Jolt Actors Jolt Collision Layers Surfaces","title":"Jolt Shapes"},{"location":"physics/collision-shapes/jolt-shapes/#jolt-shapes","text":"A Jolt actor configures how an object behaves in the physics simulation. However, every physical presence also requires to have a 3D shape. The shape of actors is set up using Jolt shape components. Dynamic actors can only be simulated with convex shapes. Therefore concave collision meshes are exclusive to static actors . All shape components represent convex geometry and work with all physics actor types.","title":"Jolt Shapes"},{"location":"physics/collision-shapes/jolt-shapes/#shape-components","text":"The following shape components are available: Jolt Sphere Shape Component Jolt Box Shape Component Jolt Capsule Shape Component Jolt Cylinder Shape Component Jolt Convex Shape Component","title":"Shape Components"},{"location":"physics/collision-shapes/jolt-shapes/#actor-shape-setup","text":"The easiest kind of actor shape setup is to simply attach a shape component to the same game object that the actor component is attached to. This way the position of the game object is also the center of the shape, which is often sufficient. For more complex shapes, you can add child nodes below the actor node, attach the shapes to those nodes, and position the nodes as needed. When an actor is initialized for the simulation, it traverses the hierarchy below its owner game object and gathers all shape components. When it encounters another actor component, all shapes below that node are ignored. All shapes that are found this way are added to the actor as one compound shape . This way you can build a single actor that has a complex shape, made up of many parts. You can't add or remove individual shapes during simulation. If you need pieces to be destructible, you need to turn them into separate actors. To still have those actors move in unison, you need to join them using a fixed joint .","title":"Actor Shape Setup"},{"location":"physics/collision-shapes/jolt-shapes/#friction-and-restitution","text":"Friction and restitution are the two physical properties that affect a shape's physical behavior the most. See this section for details.","title":"Friction and Restitution"},{"location":"physics/collision-shapes/jolt-shapes/#see-also","text":"Jolt Actors Jolt Collision Layers Surfaces","title":"See Also"},{"location":"physics/collision-shapes/jolt-sphere-shape-component/","text":"Jolt Sphere Shape Component The Jolt sphere shape component adds a sphere as a shape to the Jolt actor that is attached to the closest parent node. You can attach this component to the same node where the actor component is attached, or you can create a child object to attach it to, which allows you to position the shape relative to the actor. Spheres are very efficient for the physics engine to handle. Therefore you should prefer them over all other shapes, especially convex shapes , when you can approximate the geometry of an object with one or a couple sphere shapes. Component Properties Radius : The radius of the sphere shape. See Also Jolt Shapes Jolt Actors","title":"Jolt Sphere Shape Component"},{"location":"physics/collision-shapes/jolt-sphere-shape-component/#jolt-sphere-shape-component","text":"The Jolt sphere shape component adds a sphere as a shape to the Jolt actor that is attached to the closest parent node. You can attach this component to the same node where the actor component is attached, or you can create a child object to attach it to, which allows you to position the shape relative to the actor. Spheres are very efficient for the physics engine to handle. Therefore you should prefer them over all other shapes, especially convex shapes , when you can approximate the geometry of an object with one or a couple sphere shapes.","title":"Jolt Sphere Shape Component"},{"location":"physics/collision-shapes/jolt-sphere-shape-component/#component-properties","text":"Radius : The radius of the sphere shape.","title":"Component Properties"},{"location":"physics/collision-shapes/jolt-sphere-shape-component/#see-also","text":"Jolt Shapes Jolt Actors","title":"See Also"},{"location":"physics/constraints/jolt-cone-constraint-component/","text":"Jolt Cone Constraint Component The Jolt cone constraint component is a constraint that links two actors in a similar way to a point constraint but additionally constrains the maximum angle that the actors can swing. If additionally the maximum twist around the rotational axis shall be constrained, use a swing-twist constraint . Component Properties Shared Constraint Component Properties ConeAngle : The maximum angle how for the child actor may tilt or swing relative to the parent actor. This sets up a cone shape within which the child actor may be. This can be used to prevent unrealistic rotations for stiff objects. See Also Jolt Constraints Jolt Actors Jolt Point Constraint Component Jolt Swing-Twist Constraint Component","title":"Jolt Cone Constraint Component"},{"location":"physics/constraints/jolt-cone-constraint-component/#jolt-cone-constraint-component","text":"The Jolt cone constraint component is a constraint that links two actors in a similar way to a point constraint but additionally constrains the maximum angle that the actors can swing. If additionally the maximum twist around the rotational axis shall be constrained, use a swing-twist constraint .","title":"Jolt Cone Constraint Component"},{"location":"physics/constraints/jolt-cone-constraint-component/#component-properties","text":"Shared Constraint Component Properties ConeAngle : The maximum angle how for the child actor may tilt or swing relative to the parent actor. This sets up a cone shape within which the child actor may be. This can be used to prevent unrealistic rotations for stiff objects.","title":"Component Properties"},{"location":"physics/constraints/jolt-cone-constraint-component/#see-also","text":"Jolt Constraints Jolt Actors Jolt Point Constraint Component Jolt Swing-Twist Constraint Component","title":"See Also"},{"location":"physics/constraints/jolt-constraints/","text":"Jolt Constraints A Jolt constraint is a component that links Jolt actors to constrain their movement. Constraints are often also called joints . There are multiple types of constraints that each constrain the actors in different ways. Constraints can be used for simple things like door hinges, up to complex configurations like rag dolls. Joining Actors Constraints can be used to link: Two dynamic actors . This will create a more complex dynamic object that can still freely move throughout the scene, but has multiple parts that can wiggle around. One static actor with one dynamic actor . In this case the dynamic actor is now constrained in its movement relative to the static actor. Since the static actor never moves, the dynamic actor's freedom is severely limited. One dynamic actor with no second actor. This just affixes the dynamic actor with 'the world'. This is effectively the same as joining it with another static actor. One dynamic actor with one kinematic actor . If the kinematic actor moves, the dynamic actor is forced to follow according to the constraint between them. The dynamic actor does not affect the kinematic actor in any way. This setup is common to have full control over the movement of one part (the kinematic actor) and still get physically plausible behavior from another part. There are two ways that a constraint component can link actors. Using Object References You can use the object references on the joint component to specify both the parent actor and the child actor : With this method, the game objects for the actors and the joint don't need to have a specific hierarchy, they can be placed just next to each other. This also allows to create loops of linked actors. On the other hand, you always need to know exactly which actors shall be linked. Note: The Child Actor of a joint always has to be specified this way. Using the Object Hierarchy To determine which Parent Actor to link the joint to, you can utilize the object hierarchy. If no parent actor is specified as an object reference, the joint will automatically traverse its object hierarchy upwards and search for the closest object with a static or dynamic actor. If it finds one, the joint will link to that object as the parent actor. If it does not find any such actor on a parent node, the joint will attach to 'the world', meaning it will be fixed to its current world position. This method can be useful especially when you want to put the joint into a prefab . For example, if you have a prefab for a chandelier that can swing around, you can set up the joint to only reference the actor that represents the chandelier, but the prefab does not contain an actor for the anchor point. If this prefab is placed into a scene, the rule that a joint without a parent actor just gets linked to the world, means that you can easily place these prefabs in a scene, and they will swing around the location where they have been instantiated. However, if your scene contains for example a moveable room, you can also place it there and make sure to attach the chandelier prefab as a child node of the moveable room actor. That means, the joint of the chandelier will now link the chandelier actor with the moveable room actor, and thus whenever the room moves, the chandelier will be physically dragged along as well. Important: This method of linking actors only works properly, if the parent actor is either static or kinematic . It doesn't work for simulated dynamic actors, as the transform update introduces jitter. Constraint Position The position and rotation of a constraint specifies the pivot point. For hinge constraints , point constraints , cone constraints and swing-twist constraints the position of the joint determines around which point the linked actors rotate. For fixed constraints the position still affects how strong forces may become due to lever effects. For distance constraints the joint position determines from where the distance is computed. Only for slider constraints is the position of the joint less relevant, though for computational stability, it should still be somewhere in between the linked actors. Using the ChildActorAnchor Property Using the ChildActorAnchor property is entirely optional and often not needed. Basically this option is used to reference any object, and tell the constraint that the position and orientation of that object should be the anchor point where the constraint acts upon the child actor. For most constraint types this is rarely needed. If you link two actors with a hinge constraint, the position of the constraint already defines where between the actors the hinge will be. However, this setup always assumes that the two actors should start out in the 'default configuration' of the constraint. For example, for a hinge constraint with a stiff spring, the default configuration is when it is fully relaxed, meaning you could not put them into a start configuration where the constraint is already under tension. Similarly, for a distance constraint, the child actor would always need to start out such that the constraint position is exactly at the position of the actor, where it should link to the actor. This can be problematic especially when the distance constraint uses a minimum distance , as the constraint would immediately push away the actor, potentially with high speed. Using a dedicated object to specify the anchor point, both situations can be fixed. The anchor point specifies the position at which the constraint affects the child actor. For most constraint types the rotation of the anchor point is important as well. If the anchor's transform is identical to the constraint's transform, it is redundant and could be left out. However, if it is different from the constraint's transform, this difference tells the constraint how much its start state deviates from the 'default configuration'. For example for a hinge constraint the position of the anchor should always be identical to the constraint's position, otherwise the child actor will jerk towards the hinge constraint at startup. However, the rotation may deviate from the constraint's rotation, which specifies how much the constraint is already rotated at startup. Shared Constraint Component Properties These properties are shared among all joint types: PairCollision : If disabled, joined actors will not collide with each other. This can be preferable, because then the joined actors may overlap. ParentActor , ChildActor : References to objects with actor components to link with this constraint. See joining actors above. ChildActorAnchor : An optional reference to an object that tells the constraint where it should attach to the child actor. See using the ChildActorAnchor property above. BreakForce , BreakTorque : If either of these values is larger than zero, the constraint is breakable . That means if during the physics simulation the force or torque acting upon the constraint exceeds this threshold, the constraint will be deleted and the bodies won't be joined any longer. When this happens, plMsgPhysicsJointBroke will be sent as an event message . Constraint Types These types of constraints are currently available: Jolt Hinge Constraint Component Jolt Point Constraint Component Jolt Fixed Constraint Component Jolt Slider Constraint Component Jolt Distance Constraint Component Jolt Cone Constraint Component Jolt Swing-Twist Constraint Component Constraint Stability Joining multiple actors in a chain can quickly result in really bad simulation results. Have a look at dynamic actor simulation stability for ways to improve this. When working with constraints the following aspects help a lot: Make sure that the involved actors are not too light. For smaller objects the automatically computed mass is often too low for the constraint to be stable. Drastically increase linear and especially angular damping on the actors ( 0.1 to 0.8 ). Don't create very long chains, try to achieve the desired result with as few constraints as possible. Don't use constraint limits (hinge constraint, swing-twist constraint), they can add significant instability. See Also Jolt Actors","title":"Jolt Constraints"},{"location":"physics/constraints/jolt-constraints/#jolt-constraints","text":"A Jolt constraint is a component that links Jolt actors to constrain their movement. Constraints are often also called joints . There are multiple types of constraints that each constrain the actors in different ways. Constraints can be used for simple things like door hinges, up to complex configurations like rag dolls.","title":"Jolt Constraints"},{"location":"physics/constraints/jolt-constraints/#joining-actors","text":"Constraints can be used to link: Two dynamic actors . This will create a more complex dynamic object that can still freely move throughout the scene, but has multiple parts that can wiggle around. One static actor with one dynamic actor . In this case the dynamic actor is now constrained in its movement relative to the static actor. Since the static actor never moves, the dynamic actor's freedom is severely limited. One dynamic actor with no second actor. This just affixes the dynamic actor with 'the world'. This is effectively the same as joining it with another static actor. One dynamic actor with one kinematic actor . If the kinematic actor moves, the dynamic actor is forced to follow according to the constraint between them. The dynamic actor does not affect the kinematic actor in any way. This setup is common to have full control over the movement of one part (the kinematic actor) and still get physically plausible behavior from another part. There are two ways that a constraint component can link actors.","title":"Joining Actors"},{"location":"physics/constraints/jolt-constraints/#using-object-references","text":"You can use the object references on the joint component to specify both the parent actor and the child actor : With this method, the game objects for the actors and the joint don't need to have a specific hierarchy, they can be placed just next to each other. This also allows to create loops of linked actors. On the other hand, you always need to know exactly which actors shall be linked. Note: The Child Actor of a joint always has to be specified this way.","title":"Using Object References"},{"location":"physics/constraints/jolt-constraints/#using-the-object-hierarchy","text":"To determine which Parent Actor to link the joint to, you can utilize the object hierarchy. If no parent actor is specified as an object reference, the joint will automatically traverse its object hierarchy upwards and search for the closest object with a static or dynamic actor. If it finds one, the joint will link to that object as the parent actor. If it does not find any such actor on a parent node, the joint will attach to 'the world', meaning it will be fixed to its current world position. This method can be useful especially when you want to put the joint into a prefab . For example, if you have a prefab for a chandelier that can swing around, you can set up the joint to only reference the actor that represents the chandelier, but the prefab does not contain an actor for the anchor point. If this prefab is placed into a scene, the rule that a joint without a parent actor just gets linked to the world, means that you can easily place these prefabs in a scene, and they will swing around the location where they have been instantiated. However, if your scene contains for example a moveable room, you can also place it there and make sure to attach the chandelier prefab as a child node of the moveable room actor. That means, the joint of the chandelier will now link the chandelier actor with the moveable room actor, and thus whenever the room moves, the chandelier will be physically dragged along as well. Important: This method of linking actors only works properly, if the parent actor is either static or kinematic . It doesn't work for simulated dynamic actors, as the transform update introduces jitter.","title":"Using the Object Hierarchy"},{"location":"physics/constraints/jolt-constraints/#constraint-position","text":"The position and rotation of a constraint specifies the pivot point. For hinge constraints , point constraints , cone constraints and swing-twist constraints the position of the joint determines around which point the linked actors rotate. For fixed constraints the position still affects how strong forces may become due to lever effects. For distance constraints the joint position determines from where the distance is computed. Only for slider constraints is the position of the joint less relevant, though for computational stability, it should still be somewhere in between the linked actors.","title":"Constraint Position"},{"location":"physics/constraints/jolt-constraints/#using-the-childactoranchor-property","text":"Using the ChildActorAnchor property is entirely optional and often not needed. Basically this option is used to reference any object, and tell the constraint that the position and orientation of that object should be the anchor point where the constraint acts upon the child actor. For most constraint types this is rarely needed. If you link two actors with a hinge constraint, the position of the constraint already defines where between the actors the hinge will be. However, this setup always assumes that the two actors should start out in the 'default configuration' of the constraint. For example, for a hinge constraint with a stiff spring, the default configuration is when it is fully relaxed, meaning you could not put them into a start configuration where the constraint is already under tension. Similarly, for a distance constraint, the child actor would always need to start out such that the constraint position is exactly at the position of the actor, where it should link to the actor. This can be problematic especially when the distance constraint uses a minimum distance , as the constraint would immediately push away the actor, potentially with high speed. Using a dedicated object to specify the anchor point, both situations can be fixed. The anchor point specifies the position at which the constraint affects the child actor. For most constraint types the rotation of the anchor point is important as well. If the anchor's transform is identical to the constraint's transform, it is redundant and could be left out. However, if it is different from the constraint's transform, this difference tells the constraint how much its start state deviates from the 'default configuration'. For example for a hinge constraint the position of the anchor should always be identical to the constraint's position, otherwise the child actor will jerk towards the hinge constraint at startup. However, the rotation may deviate from the constraint's rotation, which specifies how much the constraint is already rotated at startup.","title":"Using the ChildActorAnchor Property"},{"location":"physics/constraints/jolt-constraints/#shared-constraint-component-properties","text":"These properties are shared among all joint types: PairCollision : If disabled, joined actors will not collide with each other. This can be preferable, because then the joined actors may overlap. ParentActor , ChildActor : References to objects with actor components to link with this constraint. See joining actors above. ChildActorAnchor : An optional reference to an object that tells the constraint where it should attach to the child actor. See using the ChildActorAnchor property above. BreakForce , BreakTorque : If either of these values is larger than zero, the constraint is breakable . That means if during the physics simulation the force or torque acting upon the constraint exceeds this threshold, the constraint will be deleted and the bodies won't be joined any longer. When this happens, plMsgPhysicsJointBroke will be sent as an event message .","title":"Shared Constraint Component Properties"},{"location":"physics/constraints/jolt-constraints/#constraint-types","text":"These types of constraints are currently available: Jolt Hinge Constraint Component Jolt Point Constraint Component Jolt Fixed Constraint Component Jolt Slider Constraint Component Jolt Distance Constraint Component Jolt Cone Constraint Component Jolt Swing-Twist Constraint Component","title":"Constraint Types"},{"location":"physics/constraints/jolt-constraints/#constraint-stability","text":"Joining multiple actors in a chain can quickly result in really bad simulation results. Have a look at dynamic actor simulation stability for ways to improve this. When working with constraints the following aspects help a lot: Make sure that the involved actors are not too light. For smaller objects the automatically computed mass is often too low for the constraint to be stable. Drastically increase linear and especially angular damping on the actors ( 0.1 to 0.8 ). Don't create very long chains, try to achieve the desired result with as few constraints as possible. Don't use constraint limits (hinge constraint, swing-twist constraint), they can add significant instability.","title":"Constraint Stability"},{"location":"physics/constraints/jolt-constraints/#see-also","text":"Jolt Actors","title":"See Also"},{"location":"physics/constraints/jolt-distance-constraint-component/","text":"Jolt Distance Constraint Component The Jolt distance constraint component is a constraint that links two actors such that they will keep a minimum and maximum distance. If the actors come closer than the minimum distance, they will repel each other, if they get farther apart than the maximum distance, they will attract each other. The distance joint can be used to fake the behavior of chains and ropes, if no proper simulation and visualization of individual chain links is needed. Component Properties Shared Constraint Component Properties MinDistance : The minimum distance that the two joined actors should keep from each other. If the constraint has no parent actor, the child actor will keep this distance from the constraint position. Note that if the distance constraint does not use the ChildActorAnchor option, a non-zero minimum distance will make the child actor be pushed away right after startup. MaxDistance : The maximum distance between the two joined actors. Without a spring, the two joined actors are unable to get farther apart than this. With a spring, the two actors will be pulled back together with the spring force, when they become farther apart than this. Frequency : Determines how often (per second) the constraint is enforced. Higher values make the constraint stiffer, but can also lead to oscillation. Good values are in range 0.1 to 20. Damping : How much to dampen actors when they overshoot the target position. Lower values make the objects bounce back harder, higher values make them just stop. See Also Jolt Constraints Jolt Actors Jolt Shapes","title":"Jolt Distance Constraint Component"},{"location":"physics/constraints/jolt-distance-constraint-component/#jolt-distance-constraint-component","text":"The Jolt distance constraint component is a constraint that links two actors such that they will keep a minimum and maximum distance. If the actors come closer than the minimum distance, they will repel each other, if they get farther apart than the maximum distance, they will attract each other. The distance joint can be used to fake the behavior of chains and ropes, if no proper simulation and visualization of individual chain links is needed.","title":"Jolt Distance Constraint Component"},{"location":"physics/constraints/jolt-distance-constraint-component/#component-properties","text":"Shared Constraint Component Properties MinDistance : The minimum distance that the two joined actors should keep from each other. If the constraint has no parent actor, the child actor will keep this distance from the constraint position. Note that if the distance constraint does not use the ChildActorAnchor option, a non-zero minimum distance will make the child actor be pushed away right after startup. MaxDistance : The maximum distance between the two joined actors. Without a spring, the two joined actors are unable to get farther apart than this. With a spring, the two actors will be pulled back together with the spring force, when they become farther apart than this. Frequency : Determines how often (per second) the constraint is enforced. Higher values make the constraint stiffer, but can also lead to oscillation. Good values are in range 0.1 to 20. Damping : How much to dampen actors when they overshoot the target position. Lower values make the objects bounce back harder, higher values make them just stop.","title":"Component Properties"},{"location":"physics/constraints/jolt-distance-constraint-component/#see-also","text":"Jolt Constraints Jolt Actors Jolt Shapes","title":"See Also"},{"location":"physics/constraints/jolt-fixed-constraint-component/","text":"Jolt Fixed Constraint Component The Jolt fixed joint component is the most simple joint type. It links two actors together, such that they move in unison. If all you want is to merge two shapes , you should just attach them to the same actor. The main use case for fixed joints is to make them breakable . NOTE: Breakable constraints are currently not implemented in Jolt. That means joints can't automatically break due to physical stress. At the moment you can only make joints break by deactivating a constraint programmatically. Component Properties Shared Constraint Component Properties See Also Jolt Constraints Jolt Actors Jolt Shapes","title":"Jolt Fixed Constraint Component"},{"location":"physics/constraints/jolt-fixed-constraint-component/#jolt-fixed-constraint-component","text":"The Jolt fixed joint component is the most simple joint type. It links two actors together, such that they move in unison. If all you want is to merge two shapes , you should just attach them to the same actor. The main use case for fixed joints is to make them breakable . NOTE: Breakable constraints are currently not implemented in Jolt. That means joints can't automatically break due to physical stress. At the moment you can only make joints break by deactivating a constraint programmatically.","title":"Jolt Fixed Constraint Component"},{"location":"physics/constraints/jolt-fixed-constraint-component/#component-properties","text":"Shared Constraint Component Properties","title":"Component Properties"},{"location":"physics/constraints/jolt-fixed-constraint-component/#see-also","text":"Jolt Constraints Jolt Actors Jolt Shapes","title":"See Also"},{"location":"physics/constraints/jolt-hinge-constraint-component/","text":"Jolt Hinge Constraint Component The Jolt hinge constraint component is a constraint that links two actors such that they can only rotate around one axis relative to each other. How far the joined objects can rotate can be limited. The hinge can also be powered with a drive , meaning it will rotate on its own with a maximum force. The drive can also be configured to effectively act like a spring, pulling the hinge towards a desired rotation. Component Properties Shared Constraint Component Properties LimitMode : Defines whether the constraint can spin freely, or is restricted by LowerLimit and UpperLimit . NoLimit : The constraint can spin without restriction. HardLimit : The constraint cannot rotate farther than LowerLimit and UpperLimit . If it hits the boundary, it may bounce back. LowerLimit , UpperLimit : The lower and upper allowed rotation angles, if LimitMode is enabled. Friction : How easy it is to rotate the hinge. Higher values make the constraint stiffer. DriveMode : Specifies whether the constraint will apply a force to rotate the actors. NoDrive : The constraint will not rotate on its own. ReachVelocity : The constraint will try to rotate at a speed of DriveTargetValue . ReachPosition : The constraint will try to rotate towards the angle DriveTargetValue . DriveStrength : The maximum force the constraint can apply to try to reach its target. See Also Jolt Constraints Jolt Actors Jolt Shapes","title":"Jolt Hinge Constraint Component"},{"location":"physics/constraints/jolt-hinge-constraint-component/#jolt-hinge-constraint-component","text":"The Jolt hinge constraint component is a constraint that links two actors such that they can only rotate around one axis relative to each other. How far the joined objects can rotate can be limited. The hinge can also be powered with a drive , meaning it will rotate on its own with a maximum force. The drive can also be configured to effectively act like a spring, pulling the hinge towards a desired rotation.","title":"Jolt Hinge Constraint Component"},{"location":"physics/constraints/jolt-hinge-constraint-component/#component-properties","text":"Shared Constraint Component Properties LimitMode : Defines whether the constraint can spin freely, or is restricted by LowerLimit and UpperLimit . NoLimit : The constraint can spin without restriction. HardLimit : The constraint cannot rotate farther than LowerLimit and UpperLimit . If it hits the boundary, it may bounce back. LowerLimit , UpperLimit : The lower and upper allowed rotation angles, if LimitMode is enabled. Friction : How easy it is to rotate the hinge. Higher values make the constraint stiffer. DriveMode : Specifies whether the constraint will apply a force to rotate the actors. NoDrive : The constraint will not rotate on its own. ReachVelocity : The constraint will try to rotate at a speed of DriveTargetValue . ReachPosition : The constraint will try to rotate towards the angle DriveTargetValue . DriveStrength : The maximum force the constraint can apply to try to reach its target.","title":"Component Properties"},{"location":"physics/constraints/jolt-hinge-constraint-component/#see-also","text":"Jolt Constraints Jolt Actors Jolt Shapes","title":"See Also"},{"location":"physics/constraints/jolt-point-constraint-component/","text":"Jolt Point Constraint Component The Jolt point constraint component is a constraint that links two actors with a ball and socket constraint , meaning the actors can rotate around the constraint's pivot point, but cannot move apart. The point constraint is very simple and therefore also quite stable. If you can get away with its limited functionality, prefer to use it over more complex constraints. If the maximum swing or twist angle shall be constrained, use a cone constraint or a swing-twist constraint . Component Properties Shared Joint Component Properties See Also Jolt Constraints Jolt Actors Jolt Cone Constraint Component Jolt Swing-Twist Constraint Component","title":"Jolt Point Constraint Component"},{"location":"physics/constraints/jolt-point-constraint-component/#jolt-point-constraint-component","text":"The Jolt point constraint component is a constraint that links two actors with a ball and socket constraint , meaning the actors can rotate around the constraint's pivot point, but cannot move apart. The point constraint is very simple and therefore also quite stable. If you can get away with its limited functionality, prefer to use it over more complex constraints. If the maximum swing or twist angle shall be constrained, use a cone constraint or a swing-twist constraint .","title":"Jolt Point Constraint Component"},{"location":"physics/constraints/jolt-point-constraint-component/#component-properties","text":"Shared Joint Component Properties","title":"Component Properties"},{"location":"physics/constraints/jolt-point-constraint-component/#see-also","text":"Jolt Constraints Jolt Actors Jolt Cone Constraint Component Jolt Swing-Twist Constraint Component","title":"See Also"},{"location":"physics/constraints/jolt-slider-constraint-component/","text":"Jolt Slider Constraint Component The Jolt slider constraint component is a constraint that links two actors such that they can only slide along one axis relative to each other. Optionally, how for the joined actors can slide can be limited. Component Properties Shared Constraint Component Properties LimitMode : Specifies whether the distance of sliding is limited. NoLimit : The actors can slide unlimited far. Since they will still collide with other objects, there may be no need to limit the slide distance through the joint. HardLimit : When the actors reach the end of the joint range, they will be stopped. LowerLimit , UpperLimit : How far the actor can deviate from the start position in either direction. Friction : How easy it is to slide along the constraint. Higher values make the joint stiffer. DriveMode : Specifies whether the constraint will apply a force to push the actors. NoDrive : The slider will not push the actors. ReachVelocity : The constraint will try to push at a speed of DriveTargetValue . ReachPosition : The constraint will try to push towards the relative position DriveTargetValue . DriveStrength : The maximum force the constraint can apply to try to reach its target. See Also Jolt Constraints Jolt Actors Jolt Shapes","title":"Jolt Slider Constraint Component"},{"location":"physics/constraints/jolt-slider-constraint-component/#jolt-slider-constraint-component","text":"The Jolt slider constraint component is a constraint that links two actors such that they can only slide along one axis relative to each other. Optionally, how for the joined actors can slide can be limited.","title":"Jolt Slider Constraint Component"},{"location":"physics/constraints/jolt-slider-constraint-component/#component-properties","text":"Shared Constraint Component Properties LimitMode : Specifies whether the distance of sliding is limited. NoLimit : The actors can slide unlimited far. Since they will still collide with other objects, there may be no need to limit the slide distance through the joint. HardLimit : When the actors reach the end of the joint range, they will be stopped. LowerLimit , UpperLimit : How far the actor can deviate from the start position in either direction. Friction : How easy it is to slide along the constraint. Higher values make the joint stiffer. DriveMode : Specifies whether the constraint will apply a force to push the actors. NoDrive : The slider will not push the actors. ReachVelocity : The constraint will try to push at a speed of DriveTargetValue . ReachPosition : The constraint will try to push towards the relative position DriveTargetValue . DriveStrength : The maximum force the constraint can apply to try to reach its target.","title":"Component Properties"},{"location":"physics/constraints/jolt-slider-constraint-component/#see-also","text":"Jolt Constraints Jolt Actors Jolt Shapes","title":"See Also"},{"location":"physics/constraints/jolt-swing-twist-constraint-component/","text":"Jolt Swing-Twist Constraint Component The Jolt swing-twist constraint component is a constraint that links two actors in a similar way to a cone constraint but additionally constrains how much the actors may twist around their rotational axis. The swing-twist constraint is more complex and thus also less stable than other constraints. If possible prefer to use a cone constraint or a point constraint . Component Properties Shared Constraint Component Properties SwingLimitY , SwingLimitZ : These two angles make the constraint limit how far the child actor may tilt relative to the parent actor. This sets up a cone shape within which the child actor may be. This can be used to prevent unrealistic rotations for stiff objects. Friction : How easily the constraint rotates. Higher values make the constraint stiffer. LowerTwistLimit , UpperTwistLimit : How much the child actor may rotate around the main axis. See Also Jolt Constraints Jolt Actors Jolt Cone Constraint Component Jolt Point Constraint Component","title":"Jolt Swing-Twist Constraint Component"},{"location":"physics/constraints/jolt-swing-twist-constraint-component/#jolt-swing-twist-constraint-component","text":"The Jolt swing-twist constraint component is a constraint that links two actors in a similar way to a cone constraint but additionally constrains how much the actors may twist around their rotational axis. The swing-twist constraint is more complex and thus also less stable than other constraints. If possible prefer to use a cone constraint or a point constraint .","title":"Jolt Swing-Twist Constraint Component"},{"location":"physics/constraints/jolt-swing-twist-constraint-component/#component-properties","text":"Shared Constraint Component Properties SwingLimitY , SwingLimitZ : These two angles make the constraint limit how far the child actor may tilt relative to the parent actor. This sets up a cone shape within which the child actor may be. This can be used to prevent unrealistic rotations for stiff objects. Friction : How easily the constraint rotates. Higher values make the constraint stiffer. LowerTwistLimit , UpperTwistLimit : How much the child actor may rotate around the main axis.","title":"Component Properties"},{"location":"physics/constraints/jolt-swing-twist-constraint-component/#see-also","text":"Jolt Constraints Jolt Actors Jolt Cone Constraint Component Jolt Point Constraint Component","title":"See Also"},{"location":"physics/ragdolls/jolt-hitbox-component/","text":"Jolt Hitbox Component The Jolt hitbox component is used to add collider shapes to an animated mesh . The component must be attached next to another component that defines the skeleton to use, for instance an animated mesh component or a skeleton component . It will then use that skeleton to create the physics shapes. Consequently, the configuration of the hitbox shapes is set up through the skeleton asset . The hitboxes are usually used to be able to shoot an animated character. Although they share the collider setup with the Jolt ragdoll component , hitboxes and ragdolls are separate features that can be used independently of each other. For example, while a character is alive, it would use hitboxes, so that raycasts can determine where it would be hit, but it would not use a ragdoll component yet. Once a character dies, a ragdoll component would be activated to make it fall to the ground. The hitbox component could now be deactivated (which also makes sense for performance reasons), since it's functionality may not be needed anymore. Component Properties Query Shapes Only : If true, the shapes that get created act the same way as query shape actors . That means the shapes can be detected via raycasts and other shape queries (e.g. the projectile component will be able to hit it), but otherwise they don't participate in the physical simulation. If set to false, full kinematic shapes are used, which means the shapes will push all dynamic actors aside. This is rarely desired, usually one would rather use a character controller or a single kinematic actor in the form of a capsule to represent the animated mesh, but in some exceptional cases it might be useful. Update Threshold : How often the hitboxes are updated to follow the animation. At 0, they are updated every frame. If perfect alignment with the animation is not necessary, it is better for performance to use a larger time step. See Also Jolt Ragdoll Component Jolt Query Shape Actor Component Skeleton Asset Skeletal Animations","title":"Jolt Hitbox Component"},{"location":"physics/ragdolls/jolt-hitbox-component/#jolt-hitbox-component","text":"The Jolt hitbox component is used to add collider shapes to an animated mesh . The component must be attached next to another component that defines the skeleton to use, for instance an animated mesh component or a skeleton component . It will then use that skeleton to create the physics shapes. Consequently, the configuration of the hitbox shapes is set up through the skeleton asset . The hitboxes are usually used to be able to shoot an animated character. Although they share the collider setup with the Jolt ragdoll component , hitboxes and ragdolls are separate features that can be used independently of each other. For example, while a character is alive, it would use hitboxes, so that raycasts can determine where it would be hit, but it would not use a ragdoll component yet. Once a character dies, a ragdoll component would be activated to make it fall to the ground. The hitbox component could now be deactivated (which also makes sense for performance reasons), since it's functionality may not be needed anymore.","title":"Jolt Hitbox Component"},{"location":"physics/ragdolls/jolt-hitbox-component/#component-properties","text":"Query Shapes Only : If true, the shapes that get created act the same way as query shape actors . That means the shapes can be detected via raycasts and other shape queries (e.g. the projectile component will be able to hit it), but otherwise they don't participate in the physical simulation. If set to false, full kinematic shapes are used, which means the shapes will push all dynamic actors aside. This is rarely desired, usually one would rather use a character controller or a single kinematic actor in the form of a capsule to represent the animated mesh, but in some exceptional cases it might be useful. Update Threshold : How often the hitboxes are updated to follow the animation. At 0, they are updated every frame. If perfect alignment with the animation is not necessary, it is better for performance to use a larger time step.","title":"Component Properties"},{"location":"physics/ragdolls/jolt-hitbox-component/#see-also","text":"Jolt Ragdoll Component Jolt Query Shape Actor Component Skeleton Asset Skeletal Animations","title":"See Also"},{"location":"physics/ragdolls/jolt-ragdoll-component/","text":"Jolt Ragdoll Component Note Ragdolls are a work-in-progress feature. They are working, but the exact functionality may change in the future. The Jolt ragdoll component is used to physically simulate limp bodies. Ragdoll Configuration Ragdolls only work with skeletons that have a proper bone collider and joint setup. The most important bones need to have collider shapes . Additionally, bones that should be anatomically connected, need to have joints set up. Bones also must adhere to a physically plausible hierarchy, meaning that leg bones should be child bones of a hip bone, feet bones must be child bones of leg bones and so on. Unfortunately many assets don't strictly follow this rule, which often makes them unsuitable for use as a ragdoll. The ragdoll component works with uniform scaling , so you can create differently sized characters or objects. It does not work with non-uniform scaling. It is common to add a ragdoll component to a character, but set the component to inactive , and only activate the component when the character goes limp. Breakable Objects The ragdoll component can be used for a simple breaking effect. For this you need to build a mesh out of broken pieces and give each piece a bone. In the rest pose the mesh should look like one piece. Now you can use a visual script to determine under what conditions the object should shatter and then activate the ragdoll component. In this case the skeleton only needs to define shapes for the bones, but no joints between them. Thus each fragment will fall individually and the object looks like it breaks apart. Use the properties CenterPosition , CenterVelocity and CenterAngularVelocity to make the pieces fly away more convincingly. Important This feature is only experimental and very limited in functionality. Component Properties SelfCollision : Whether the individual bones of a ragdoll shall collide with each other. If disabled, they will pass through each other and only the joint constraints will prevent unnatural motion. Wether self collision works well or not on a given character highly depends on how the colliders for the bones are set up. StartMode : In which pose the ragdall should start: WithBindPose : The ragdoll starts immediately and uses the default bind pose (or rest pose) of the skeleton. WithNextAnimPose : The ragdoll waits for the next animation pose from and then starts from there. This requires a simple animation component or animation graph to be active. WithCurrentMeshPose : The ragdoll starts immediately with the current pose. This does not require another component to regularly provide new poses and thus can also be used with a skeleton pose component . GravityFactor : How much gravity to use. Mass : How heavy the ragdoll should be. StiffnessFactor : The overall stiffness of the joints. Each joint has an individual stiffness as defined in the skeleton asset , but when scaling characters up or down, it may be necessary to also scale the stiffness. OwnerVelocityScale : A ragdoll may get enabled while a character is moving, for example while it is running. The owner object velocity is then transferred to the ragdoll to have it continue falling into the direction, rather then suddenly stop and just fall down. This factor allows to tweak how much of that momentum to keep (or even exaggerate). CenterPosition : An experimental feature mainly meant for breakable objects (ragdolls with no joints). Specifies an offset where the center of the object should be, to apply an outwards force from. CenterVelocity , CenterAngularVelocity : What linear and angular velocity to set at start outwards from the CenterPosition on each bone. This makes it possible to build breakable objects that break apart when the ragdall gets activated. See Also Skeletal Animations Jolt Hitbox Component Skeleton Asset","title":"Jolt Ragdoll Component"},{"location":"physics/ragdolls/jolt-ragdoll-component/#jolt-ragdoll-component","text":"Note Ragdolls are a work-in-progress feature. They are working, but the exact functionality may change in the future. The Jolt ragdoll component is used to physically simulate limp bodies.","title":"Jolt Ragdoll Component"},{"location":"physics/ragdolls/jolt-ragdoll-component/#ragdoll-configuration","text":"Ragdolls only work with skeletons that have a proper bone collider and joint setup. The most important bones need to have collider shapes . Additionally, bones that should be anatomically connected, need to have joints set up. Bones also must adhere to a physically plausible hierarchy, meaning that leg bones should be child bones of a hip bone, feet bones must be child bones of leg bones and so on. Unfortunately many assets don't strictly follow this rule, which often makes them unsuitable for use as a ragdoll. The ragdoll component works with uniform scaling , so you can create differently sized characters or objects. It does not work with non-uniform scaling. It is common to add a ragdoll component to a character, but set the component to inactive , and only activate the component when the character goes limp.","title":"Ragdoll Configuration"},{"location":"physics/ragdolls/jolt-ragdoll-component/#breakable-objects","text":"The ragdoll component can be used for a simple breaking effect. For this you need to build a mesh out of broken pieces and give each piece a bone. In the rest pose the mesh should look like one piece. Now you can use a visual script to determine under what conditions the object should shatter and then activate the ragdoll component. In this case the skeleton only needs to define shapes for the bones, but no joints between them. Thus each fragment will fall individually and the object looks like it breaks apart. Use the properties CenterPosition , CenterVelocity and CenterAngularVelocity to make the pieces fly away more convincingly. Important This feature is only experimental and very limited in functionality.","title":"Breakable Objects"},{"location":"physics/ragdolls/jolt-ragdoll-component/#component-properties","text":"SelfCollision : Whether the individual bones of a ragdoll shall collide with each other. If disabled, they will pass through each other and only the joint constraints will prevent unnatural motion. Wether self collision works well or not on a given character highly depends on how the colliders for the bones are set up. StartMode : In which pose the ragdall should start: WithBindPose : The ragdoll starts immediately and uses the default bind pose (or rest pose) of the skeleton. WithNextAnimPose : The ragdoll waits for the next animation pose from and then starts from there. This requires a simple animation component or animation graph to be active. WithCurrentMeshPose : The ragdoll starts immediately with the current pose. This does not require another component to regularly provide new poses and thus can also be used with a skeleton pose component . GravityFactor : How much gravity to use. Mass : How heavy the ragdoll should be. StiffnessFactor : The overall stiffness of the joints. Each joint has an individual stiffness as defined in the skeleton asset , but when scaling characters up or down, it may be necessary to also scale the stiffness. OwnerVelocityScale : A ragdoll may get enabled while a character is moving, for example while it is running. The owner object velocity is then transferred to the ragdoll to have it continue falling into the direction, rather then suddenly stop and just fall down. This factor allows to tweak how much of that momentum to keep (or even exaggerate). CenterPosition : An experimental feature mainly meant for breakable objects (ragdolls with no joints). Specifies an offset where the center of the object should be, to apply an outwards force from. CenterVelocity , CenterAngularVelocity : What linear and angular velocity to set at start outwards from the CenterPosition on each bone. This makes it possible to build breakable objects that break apart when the ragdall gets activated.","title":"Component Properties"},{"location":"physics/ragdolls/jolt-ragdoll-component/#see-also","text":"Skeletal Animations Jolt Hitbox Component Skeleton Asset","title":"See Also"},{"location":"physics/special/jolt-character-controller/","text":"Character Controller A character controller is a special object in the physics engine that is used to move a character throughout a scene and make it collide with other geometry. A character controller is typically an upright capsule that abstractly represents the space that a character occupies. The character controller provides the following functionality: Move throughout a scene, collide with and slide along walls Fall to the ground, slide down steep slopes Climb up shallow slopes Step over small obstacles Climb stairs Jump Stand and crouch with different capsule sizes Push dynamic objects Get pushed by kinematic objects Ride on kinematic platforms On top of these basic features, the character controller implements many details of movement. For example, while jumping or falling, a game may allow the player some degree of control. Such details are very game specific, though, and there is no one-size-fits-all solution. Consequently, the character controller functionality is split up into multiple classes, and you are encouraged to implement your own logic: plJoltCharacterControllerComponent : A base class for Jolt character controllers. It gives access to the most important functionality and also adds some convenience functionality. plJoltDefaultCharacterComponent : An implementation of plJoltCharacterControllerComponent that is provided as an example and as a decent starting point. It implements behavior similar to old-school first-person shooter games, such as Half-Life 2. Depending on how significantly different behavior you want, you can either derive from this class and override some parts, or copy the entire code and rewrite everything as desired. Example The player object is often the most complicated object in a game. The character controller only provides the locomotion aspect, but this is often coupled tightly to the overall game logic. For example, the player may move slower or be disallowed to jump while carrying an object . Many of these aspects can be handled by an overall player logic script. Other aspects, like the details of the characters velocity while sliding down a slope or jumping through the air, have to be implemented directly inside a character controller component. The Testing Chambers sample contains a prefab called Player.plPrefab , which demonstrates how to build your own player object. The top level node contains a Default Character Controller component. You could replace this with a custom character controller component, to test out entirely different movement behavior. Note that the player object also uses an input component to funnel input into a script , which implements high level game logic, like weapon selection. See Also Character Controller","title":"Character Controller"},{"location":"physics/special/jolt-character-controller/#character-controller","text":"A character controller is a special object in the physics engine that is used to move a character throughout a scene and make it collide with other geometry. A character controller is typically an upright capsule that abstractly represents the space that a character occupies. The character controller provides the following functionality: Move throughout a scene, collide with and slide along walls Fall to the ground, slide down steep slopes Climb up shallow slopes Step over small obstacles Climb stairs Jump Stand and crouch with different capsule sizes Push dynamic objects Get pushed by kinematic objects Ride on kinematic platforms On top of these basic features, the character controller implements many details of movement. For example, while jumping or falling, a game may allow the player some degree of control. Such details are very game specific, though, and there is no one-size-fits-all solution. Consequently, the character controller functionality is split up into multiple classes, and you are encouraged to implement your own logic: plJoltCharacterControllerComponent : A base class for Jolt character controllers. It gives access to the most important functionality and also adds some convenience functionality. plJoltDefaultCharacterComponent : An implementation of plJoltCharacterControllerComponent that is provided as an example and as a decent starting point. It implements behavior similar to old-school first-person shooter games, such as Half-Life 2. Depending on how significantly different behavior you want, you can either derive from this class and override some parts, or copy the entire code and rewrite everything as desired.","title":"Character Controller"},{"location":"physics/special/jolt-character-controller/#example","text":"The player object is often the most complicated object in a game. The character controller only provides the locomotion aspect, but this is often coupled tightly to the overall game logic. For example, the player may move slower or be disallowed to jump while carrying an object . Many of these aspects can be handled by an overall player logic script. Other aspects, like the details of the characters velocity while sliding down a slope or jumping through the air, have to be implemented directly inside a character controller component. The Testing Chambers sample contains a prefab called Player.plPrefab , which demonstrates how to build your own player object. The top level node contains a Default Character Controller component. You could replace this with a custom character controller component, to test out entirely different movement behavior. Note that the player object also uses an input component to funnel input into a script , which implements high level game logic, like weapon selection.","title":"Example"},{"location":"physics/special/jolt-character-controller/#see-also","text":"Character Controller","title":"See Also"},{"location":"physics/special/jolt-cloth-sheet-component/","text":"Jolt Cloth Sheet Component The Jolt cloth sheet component simulates a square patch of cloth as it hangs and swings in the wind. It is meant for decorative purposes such as flags but it also interacts with other physics objects. Note that this interaction isn't very precise and prone to tunneling as well as getting tangled up inside dynamic objects. Jolt cloth sheets are affected by wind and contrary to the cloth sheet component they also interact with physics objects and collide with scene geometry. Component Properties Size : The physical size of the cloth sheet in the world. Segments : How detailed to simulate the cloth. CollisionLayer : The collision layer to use. Damping : How quickly the cloth loses energy while swinging. Higher values make it come to rest more quickly, low values make it swing for a longer time. WindInfluence : How strongly wind should make the cloth swing. GravityFactor : How strongly gravity pulls on the cloth. Flags : These define at which corners and edges the sheet of cloth is attached to the world. Material : The material used for rendering the cloth. Make sure to set it to two-sided for cloth that can be seen from both sides. TextureScale : Scale for the texture UV coordinates. Color : An additional tint-color for rendering. See Also Cloth Sheet Component Jolt Rope Component Wind","title":"Jolt Cloth Sheet Component"},{"location":"physics/special/jolt-cloth-sheet-component/#jolt-cloth-sheet-component","text":"The Jolt cloth sheet component simulates a square patch of cloth as it hangs and swings in the wind. It is meant for decorative purposes such as flags but it also interacts with other physics objects. Note that this interaction isn't very precise and prone to tunneling as well as getting tangled up inside dynamic objects. Jolt cloth sheets are affected by wind and contrary to the cloth sheet component they also interact with physics objects and collide with scene geometry.","title":"Jolt Cloth Sheet Component"},{"location":"physics/special/jolt-cloth-sheet-component/#component-properties","text":"Size : The physical size of the cloth sheet in the world. Segments : How detailed to simulate the cloth. CollisionLayer : The collision layer to use. Damping : How quickly the cloth loses energy while swinging. Higher values make it come to rest more quickly, low values make it swing for a longer time. WindInfluence : How strongly wind should make the cloth swing. GravityFactor : How strongly gravity pulls on the cloth. Flags : These define at which corners and edges the sheet of cloth is attached to the world. Material : The material used for rendering the cloth. Make sure to set it to two-sided for cloth that can be seen from both sides. TextureScale : Scale for the texture UV coordinates. Color : An additional tint-color for rendering.","title":"Component Properties"},{"location":"physics/special/jolt-cloth-sheet-component/#see-also","text":"Cloth Sheet Component Jolt Rope Component Wind","title":"See Also"},{"location":"physics/special/jolt-grab-object-component/","text":"Jolt Grab Object Component The Jolt grab object component enables a character controller to pick up physical items to carry around, drop or throw. The component is typically attached to the same object as the camera component . When triggered, it uses a raycast along its X axis to determine which physical object to potentially pick up. When it finds a non-kinematic dynamic actor , it checks whether a grabbable item component is available. If so, the information from that component is used to determine the best anchor at which to hold the object, otherwise it uses the object's bounding box to approximate a grab point. When it finds a suitable grab point, it attaches a constraint to an object that is specified to be the pivot point (see AttachTo property). That object has to have a kinematic actor and a dummy shape . The joint will then pull the grabbed item towards it and try to align its orientation according to the grabbed anchor. The grabbed item can then be dropped, or thrown away. All actions must be triggered from code, either C++ or TypeScript . The grabbed item still physically interacts with the environment. If such collisions hold the object too far back, the grab object component may decide to 'break' the joint and drop the object. In this case a plMsgPhysicsJointBroke event message is sent. Component Properties MaxGrabPointDistance : The maximum distance from this object for an individual grab point to be considered as a candidate. CollisionLayer : The collision layer to use for raycasting to detect which object to pick. SpringStiffness , SpringDamping : The stiffness and damping of the internally used constraint. Affects how stiff the object is held. Careful: This also determines how much force the held object can apply to other objects when you push against them. High values mean that the held object can push objects, that the character controller itself may not be able to push. BreakDistance : If the held object deviates more than this distance from the anchor point it is attached to, the hold will break. In this case a plMsgPhysicsJointBroke event message is raised. Set to zero to disable this feature. AttachTo : A reference to another game object, to which the held object will be attached to. The target object must have a kinematic Jolt actor (and a dummy Jolt shape ), such that a constraint can be attached. The reference may point to this component's owner object. However, using a different object allows you to place the held object in a more suitable location. GrabAnyObjectWithSize : If this is non-zero, objects that have no grabbable item component can be picked up as well, as long as their bounding box is smaller than this value. See Also Grabbable Item Component Jolt Constraints Jolt Actors","title":"Jolt Grab Object Component"},{"location":"physics/special/jolt-grab-object-component/#jolt-grab-object-component","text":"The Jolt grab object component enables a character controller to pick up physical items to carry around, drop or throw. The component is typically attached to the same object as the camera component . When triggered, it uses a raycast along its X axis to determine which physical object to potentially pick up. When it finds a non-kinematic dynamic actor , it checks whether a grabbable item component is available. If so, the information from that component is used to determine the best anchor at which to hold the object, otherwise it uses the object's bounding box to approximate a grab point. When it finds a suitable grab point, it attaches a constraint to an object that is specified to be the pivot point (see AttachTo property). That object has to have a kinematic actor and a dummy shape . The joint will then pull the grabbed item towards it and try to align its orientation according to the grabbed anchor. The grabbed item can then be dropped, or thrown away. All actions must be triggered from code, either C++ or TypeScript . The grabbed item still physically interacts with the environment. If such collisions hold the object too far back, the grab object component may decide to 'break' the joint and drop the object. In this case a plMsgPhysicsJointBroke event message is sent.","title":"Jolt Grab Object Component"},{"location":"physics/special/jolt-grab-object-component/#component-properties","text":"MaxGrabPointDistance : The maximum distance from this object for an individual grab point to be considered as a candidate. CollisionLayer : The collision layer to use for raycasting to detect which object to pick. SpringStiffness , SpringDamping : The stiffness and damping of the internally used constraint. Affects how stiff the object is held. Careful: This also determines how much force the held object can apply to other objects when you push against them. High values mean that the held object can push objects, that the character controller itself may not be able to push. BreakDistance : If the held object deviates more than this distance from the anchor point it is attached to, the hold will break. In this case a plMsgPhysicsJointBroke event message is raised. Set to zero to disable this feature. AttachTo : A reference to another game object, to which the held object will be attached to. The target object must have a kinematic Jolt actor (and a dummy Jolt shape ), such that a constraint can be attached. The reference may point to this component's owner object. However, using a different object allows you to place the held object in a more suitable location. GrabAnyObjectWithSize : If this is non-zero, objects that have no grabbable item component can be picked up as well, as long as their bounding box is smaller than this value.","title":"Component Properties"},{"location":"physics/special/jolt-grab-object-component/#see-also","text":"Grabbable Item Component Jolt Constraints Jolt Actors","title":"See Also"},{"location":"physics/special/jolt-rope-component/","text":"Jolt Rope Component The Jolt rope component is used to physically simulate ropes, cables and chains. Ropes can be attached to walls as decorative elements (cables, wires), or they can even be attached to dynamic physics objects to link them together. This can be used as a gameplay feature. If all you need is a decorative rope, that doesn't react to physical stimuli (except wind ), prefer to use a fake rope component , as that has a much lower performance overhead. Setting Up a Rope A rope requires two anchor points between which it hangs. One anchor point is the rope object position itself, for the other one typically uses a dummy game object. The Anchor object reference is used to select which one to use. In the object hierarchy it typically looks like this: The position of the anchors can be moved in the 3D viewport to position the rope as desired. The approximate shape of the simulated rope will be shown as a preview. Use the Slack property to make the rope sag. Run the scene to see the final shape and behavior. Rendering With just the rope simulation component, you won't be able to see the rope, at all. You also need to attach a rope render component to the same game object. Examples The Testing Chambers project contains a dedicated Ropes scene with many examples. Simulation Stability See Dynamic Actors - Simulation Stability . Overall the same guidelines to prevent stability issues apply here as well. Component Properties Anchor1 , Anchor2 : A reference to an object whose position determines one end of the rope. If only one anchor is specified, the position of the rope component's owner object is used as the other end. Anchor1Constraint , Anchor2Constraint : How the rope is attached to each anchor: None : The rope is not attached to the object, at all, and once the simulation starts, it will fall down at that end. Point : The rope is attached with a point constraint and thus can freely rotate around that end. Fixed : The rope won't be able to rotate around that end point. The orientation of the anchor is used to specified the direction of the rope there. Cone : The rope can rotate around that anchor, but only within a cone of MaxBend opening. Similar to Fixed , the orientation of the anchor defines the starting direction of the rope. If an anchor object is attached to a dynamic actor , the rope will pull that actor. Otherwise, the rope will be fixed at that static location. If the rope is not attached at one or both ends it is free to move away from there. Mass : The total mass of the rope. It will be distributed equally among all pieces. Pieces : How many individual pieces the rope is made up of. More pieces look prettier, but cost more performance and may decrease the simulation stability. Slack : How much slack the rope has. A value of zero means the rope is hung perfectly straight between its anchors. Positive values make the rope sag downwards. Negative values are also allowed, they make the rope hang upside down. This is useful to create a longer rope that shouldn't spawn inside the ground. The rope can thus be placed above the ground and it will simply fall down after creation. Thickness : How thick the simulated rope is. This may be very different from how thick it is rendered. A thinner rope will have more simulation issues, such as tunneling through other geometry. BendStiffness : Determines how hard it is to deform the rope. MaxBend , MaxTwist : These angles restrict how much each individual pieces in the rope can bend or twist relative to its neighboring piece. Low angles mean the rope is very stiff. A very flexible rope would use values above 45 degrees. CollisionLayer : The collision layer defines with which other objects the rope collides. Surface : The surface defines how slippery or bouncy the rope is. GravityFactor : How much gravity affects the rope. SelfCollision : Whether the rope should be able to collide with itself. ContinuousCollisionDetection : If enabled, the physics simulation tries harder to prevent the rope from passing through other objects. This costs additional performance. See Also Fake Rope Component Rope Render Component Jolt Actors","title":"Jolt Rope Component"},{"location":"physics/special/jolt-rope-component/#jolt-rope-component","text":"The Jolt rope component is used to physically simulate ropes, cables and chains. Ropes can be attached to walls as decorative elements (cables, wires), or they can even be attached to dynamic physics objects to link them together. This can be used as a gameplay feature. If all you need is a decorative rope, that doesn't react to physical stimuli (except wind ), prefer to use a fake rope component , as that has a much lower performance overhead.","title":"Jolt Rope Component"},{"location":"physics/special/jolt-rope-component/#setting-up-a-rope","text":"A rope requires two anchor points between which it hangs. One anchor point is the rope object position itself, for the other one typically uses a dummy game object. The Anchor object reference is used to select which one to use. In the object hierarchy it typically looks like this: The position of the anchors can be moved in the 3D viewport to position the rope as desired. The approximate shape of the simulated rope will be shown as a preview. Use the Slack property to make the rope sag. Run the scene to see the final shape and behavior.","title":"Setting Up a Rope"},{"location":"physics/special/jolt-rope-component/#rendering","text":"With just the rope simulation component, you won't be able to see the rope, at all. You also need to attach a rope render component to the same game object.","title":"Rendering"},{"location":"physics/special/jolt-rope-component/#examples","text":"The Testing Chambers project contains a dedicated Ropes scene with many examples.","title":"Examples"},{"location":"physics/special/jolt-rope-component/#simulation-stability","text":"See Dynamic Actors - Simulation Stability . Overall the same guidelines to prevent stability issues apply here as well.","title":"Simulation Stability"},{"location":"physics/special/jolt-rope-component/#component-properties","text":"Anchor1 , Anchor2 : A reference to an object whose position determines one end of the rope. If only one anchor is specified, the position of the rope component's owner object is used as the other end. Anchor1Constraint , Anchor2Constraint : How the rope is attached to each anchor: None : The rope is not attached to the object, at all, and once the simulation starts, it will fall down at that end. Point : The rope is attached with a point constraint and thus can freely rotate around that end. Fixed : The rope won't be able to rotate around that end point. The orientation of the anchor is used to specified the direction of the rope there. Cone : The rope can rotate around that anchor, but only within a cone of MaxBend opening. Similar to Fixed , the orientation of the anchor defines the starting direction of the rope. If an anchor object is attached to a dynamic actor , the rope will pull that actor. Otherwise, the rope will be fixed at that static location. If the rope is not attached at one or both ends it is free to move away from there. Mass : The total mass of the rope. It will be distributed equally among all pieces. Pieces : How many individual pieces the rope is made up of. More pieces look prettier, but cost more performance and may decrease the simulation stability. Slack : How much slack the rope has. A value of zero means the rope is hung perfectly straight between its anchors. Positive values make the rope sag downwards. Negative values are also allowed, they make the rope hang upside down. This is useful to create a longer rope that shouldn't spawn inside the ground. The rope can thus be placed above the ground and it will simply fall down after creation. Thickness : How thick the simulated rope is. This may be very different from how thick it is rendered. A thinner rope will have more simulation issues, such as tunneling through other geometry. BendStiffness : Determines how hard it is to deform the rope. MaxBend , MaxTwist : These angles restrict how much each individual pieces in the rope can bend or twist relative to its neighboring piece. Low angles mean the rope is very stiff. A very flexible rope would use values above 45 degrees. CollisionLayer : The collision layer defines with which other objects the rope collides. Surface : The surface defines how slippery or bouncy the rope is. GravityFactor : How much gravity affects the rope. SelfCollision : Whether the rope should be able to collide with itself. ContinuousCollisionDetection : If enabled, the physics simulation tries harder to prevent the rope from passing through other objects. This costs additional performance.","title":"Component Properties"},{"location":"physics/special/jolt-rope-component/#see-also","text":"Fake Rope Component Rope Render Component Jolt Actors","title":"See Also"},{"location":"runtime/filesystem/","text":"FileSystem Accessing files is one of those things that you probably need to do very early in your project. Although the POSIX functions fopen, fread, etc. are available on most platforms, they are not always the best choice. PlasmaEngine provides multiple layers of abstractions for accessing files, or more generally, data streams that look like files. Overview The following is a list of abstractions that you should be aware of. Each one will be explained in more detail below. Streams ( plStreamReader , plStreamWriter ) are the basis for all reading and writing of abstract data streams. These are the two classes that you will use whenever something that acts like a file is passed around, even though it does not need to represent an actual file. Low Level File Abstraction ( plOSFile ): This is the abstraction layer that implements reading and writing to actual files on disk in a mostly platform independent manner. This is what you typically should not need to use, at all. However, when something is not possible through the higher level abstractions, you might need to do it through this interface. High Level File System ( plFileSystem , plFileReader , plFileWriter ): Through plFileSystem you can configure a virtual file-system. plFileReader and plFileWriter should be what you typically use to open 'files' in that virtual file-system. plFileReader and plFileWriter implement the respective stream interfaces plStreamReader and plStreamWriter , so once you have opened a file through these classes, you can pass them to any function that works on streams. Streams A 'stream' is simply a series of bytes. This data stream can come from a physical file, through a network or it can be generated fully procedurally. Streams typically come in two forms: Ones that you can read from and ones that you can write to. In PlasmaEngine those two types of streams are represented with the interface classes plStreamReader and plStreamWriter respectively. The stream interfaces are reduced in functionality to what all types of data streams can provide. Reading is always done from the current read position. Writing will always append data at the end of the stream. You cannot seek to an arbitrary position (when reading you can skip ahead) and you cannot re-read the same data or overwrite previously written data. Although this can be limiting in a few scenarios, it is absolutely sufficient for the vast majority of use cases. For the few cases where you really need to set the read or write position, you can revert to another abstraction layer, e.g. on an plOSFile you have more control. Inside the core engine we typically do not care about files, at all. Therefore you will not find many functions that take a file-name string. In the few cases that this is so, those functions are usually just convenience functions that internally open a file and then pass along the file as a stream. Instead most engine functions work directly on streams. This decouples the engine from working on the concept of files and enables to work on data from any kind of source. For example, this makes it easy to embed a file in an archive or to read it from a network, or even both combined. Memory Streams A very frequently used implementation of streams is the 'memory stream'. This represents a stream of data that exists in RAM only. The data is stored in an plMemoryStreamStorage object. You can have multiple readers through the plMemoryStreamReader class and you could theoretically have multiple writers through the plMemoryStreamWriter class (though having more than one will probably not work in any useful way). Plasma Engine often uses memory streams to store incoming data for fast access later. For example the plResourceManager reads files from disk on a separate thread into a memory stream. This allows to do the slow file reading in parallel without blocking the main thread. Once all data is read into the memory stream, the main thread can then read the data directly from memory and have a guaranteed latency. The interface for reading from file or the memory stream is identical, so the code that actually interprets the file content does not need to know about this optimization at all. Other Types of Streams Plasma Engine comes with a variety of stream implementations: Compressed Streams : Through plCompressedStreamReaderZstd and plCompressedStreamWriterZstd you can easily add compression to your data streams. These classes take another stream as input or output, so you can pass it a file or memory stream to work upon. Chunk Streams : A chunk stream is basically a stream that is divided into distinct parts which are fully separated from each other. This allows to handle one stream (e.g. a file) as if it were actually multiple streams, which is useful when you want to package multiple files into one. The useful feature of these streams is that code often reads a stream until it ends (e.g. nothing more can be read). When you package multiple files into one stream, this behavior can end badly. A chunk stream enables you to prevent code from reading further than a specific point (a virtual 'end-of-stream' position). Chunk formats are also useful when you only want to read or update certain parts of a file, without knowing how the rest of the file format works. This is possible because the size of each chunk is stored in the stream which allows to skip or read an entire chunk and pass it through. Please note that you can combine different types of streams. For example you can write to a file by using plFileWriter , pass that stream to plChunkStreamWriter and then even use plCompressedStreamWriterZstd to compress individual chunks in the file. Using Standard Types with Streams The stream interfaces only provide the functions ReadBytes and WriteBytes . For most standard types Plasma Engine provides overloaded << and >> operators. E.g. you can read and write ints , floats , plVec3 , plString , etc. like this: write_stream << plVec3(1, 2, 3); write_stream << \"some string\"; plVec3 v; plString s; read_stream >> v; read_stream >> s; Using Complex Types with Streams To serialize and deserialize data that is in Plasma containers, you can also use functions like plStreamReader::ReadArray() , plStreamReader::ReadMap() , plStreamWriter::WriteArray() and plStreamWriter::WriteMap() . Those functions will try to serialize the container information and all the elements. For that to work, there must either be << and >> operaters overloaded for the element type, or the element types must have member functions with the following signature: plResult Serialize(plStreamWriter& stream) const; plResult Deserialize(plStreamReader& stream); Using these functions allows to return success or failure, which is not possible with the shift operators. If any element fails to de-/serialize, the whole operation (e.g. plStreamWriter::WriteArray() ) will fail and terminate early. Note: plStreamWriter::WriteArray() and similar functions are provided for convenience. However, when it is important to have full control over file versioning and backwards compatibility of a file format, it may be preferable to serialize containers manually. Low Level File Abstraction Reading and writing files is implemented through plOSFile . This class internally uses platform specific functions such as fopen on Linux and CreateFile on Windows. You should typically NOT use this class, there are higher level abstractions built upon plOSFile that you should prefer. Since plOSFile provides actual file access, it also has some file specific features, for instance you can set the file read or write position back and forth and you can get the OS specific file handle, such that you can use OS specific functions on that file yourself. Additionally there are static functions for deleting files, creating directory structures, querying whether a file exists, and so on. plOSFile is a thin abstraction over the operating system, it does not yet implement a higher level interface. Therefore it deliberately does not implement the stream interfaces, so you cannot pass an plOSFile instance to a function that takes an plStreamReader or plStreamWriter . Usually you only need to use plOSFile for some of the static functions that implement more infrequently used features, such as plOSFile::CopyFile or plOSFile::GetFileStats . These might not be available in higher level abstractions. Please be aware that plOSFile always requires platform specific absolute paths. There is no concept of a 'current working directory' or some root directory, to which relative paths could be used. plOSFile will check all incoming paths and assert that they are absolute. Therefore, if you need to use plOSFile , make sure to always convert any relative path to an absolute path before you pass it to plOSFile . Other Low Level File Operations Apart from plOSFile , there are a few classes that implement additional file system operations that are only thin abstractions over the operating system. plFileSystemIterator is a class that allows to iterate over all files and folders in some directory. In a game this should rarely be necessary, but in tools this can be very useful. Unfortunately it is not guaranteed that this feature can be implemented on all platforms, and each platform might have different features regarding wild-card usage and so on. Therefore this class is only available when the preprocessor define PLASMA_SUPPORTS_FILE_ITERATORS is defined as PLASMA_ON . plFileStats is a struct that provides information about a file or folder. One way to retrieve file stats is through plOSFile::GetFileStats() . Again, this feature cannot be implemented on all platforms, so it is only available when PLASMA_SUPPORTS_FILE_STATS is defined as PLASMA_ON . High Level File System Plasma Engine comes with a high level file system that is very flexible and powerful but might seem a bit confusing at first for people that are not used to working with abstractions on this level. A game engine is in many aspects simply a framework that manages resources. It ensures that resources can be found and accessed, are freed when not needed any more and otherwise are handled efficiently and often in an abstract manner that makes it easy to work with them. Working with files and everything that is similar enough so that it could be handled like a file, is the lowest level at which this 'managing of resources' begins. Plasma Engine does this by working with a virtual file system. The central class to configure this file system is plFileSystem . Data Directories The most important concept that you need to understand is that of 'data directories'. A data directory is basically a mount point that is added to the virtual file system. You can have an arbitrary number of those, but usually you will only need very few. A data directory is in the most common use case some folder on your harddisk. By adding it to the file system (using plFileSystem::AddDataDirectory ) you make that folder visible for the application. When you work with the virtual file system, you do NOT use plOSFile . Instead, you will most likely use plFileReader and plFileWriter . Those classes represent a single open file in the virtual file system. They implement the stream interfaces mentioned above. When you open a file through these classes, you should use relative paths. Absolute paths will only work if a data directory above the given path is also mounted. When you open a file through a relative path, all data directories are searched (in the reverse order in which they were added) for the file. So as long as the file exists in one data directory, it can be opened. If it exists in multiple data directories with the same relative path, you will typically get the file from the data directory that was added last. The use case here is, that you often want to have some 'base' data directory that contains general files and than additional data directories that add project or even level specific files to the file system. You will typically add the 'base' data directory right at startup of your game. Later you add your project specific data directories. Since you often want your files in the project data directory to take precedence over the 'base' data directory, it makes sense for the file system to search the directories in the reverse order in which they were added. One use case for multiple data directories with the same files in it is localization. Suppose you have a folder with all your sounds, including speech that is recorded in English. When you want to add another language, you can just create a folder 'German' that only contains the sound files which need localization. When the user wants German language support, you simply mount the German sounds folder AFTER the other data directory. Now whenever a sound file gets loaded that is both in the German sounds data directory and the default data directory, the German sound will be preferred. Everything that is not there falls back to the default data directory. Thus when some sound file was forgotten to be included in the German localization, the game will at least play the sound in English. Advanced Data Directories Although the most common use case is to mount folders as data directories, Plasma Engine's concept of data directories goes much further. Basically anything can be 'mounted' as a data directory. For example you could mount a zip file and then use it like a read-only folder. Or you could mount a folder on a remote PC and have the data sent over a network. This is possible because data directories are an abstract concept. To create your own type of data directory, you need to implement three classes, derived from plDataDirectoryType , plDataDirectoryReader and plDataDirectoryWriter respectively. Therefore, when you add a data directory to plFileSystem , the engine needs to know how exactly to mount this type. This is what 'data directory factories' are necessary for. So for example, if you want to support simple folders and zip files as data directories, you need to register one factory for 'folders' and one factory for 'zip archives'. Now when you mount a data directory with the name \"My/Test/Archive.zip\", the engine will ask each factory, whether it can handle this path. The 'folder factory' will detect that this is a file and not a folder and therefore decline. The 'zip factory' detects that this is a zip file and therefore creates a data directory which provides the functionality to read files from the archive as if it were a real folder. For example you could then open a file with the path \"My/Test/Archive.zip/Some/Compressed/File.txt\" through plFileReader and it would just work as if the file was located in a real folder. That is why you always need to register these factories first, before you can add any data directories to the virtual file system. If you want to implement your own data directory type, please have a look at plDataDirectory::FolderType . Setting up the File System Setting up the file system is very easy to do, once you know what is involved and why. The most basic configuration looks like this: // register a factory that can handle simple folders // this is actually already done automatically through the plStartup system // plFileSystem::RegisterDataDirectoryFactory(plDataDirectory::FolderType::Factory); // mount the application directory (where the binary is located) as a data directory plFileSystem::AddDataDirectory(plOSFile::GetApplicationDirectory()); That's all. Now you can read and write files in that folder like this: plFileWriter FileOut; if (FileOut.Open(\"SubFolder/Test.txt\") == PLASMA_SUCCESS) { // \"SubFolder\" will be created automatically, if it does not exist yet FileOut << \"This is a string\"; FileOut.Close(); // will also be called automatically when FileOut goes out of scope } plFileReader FileIn; if (FileIn.Open(\"SubFolder/Test.txt\") == PLASMA_SUCCESS) { plString s; FileIn >> s; FileIn.Close(); // will also be called automatically when FileIn goes out of scope PLASMA_ASSERT_DEV(s == \"This is a string\", \"The read string is incorrect: '%s'\", s.GetData()); } Obviously you can now add further data directories through plFileSystem::AddDataDirectory() . There are several additional features to configure the system for your exact use case. Please have a look at the API documentation for further details. Resolving Paths One thing that comes up once in a while is that you need to convert a path from relative to absolute or vice versa. For example, should you actually need to use an plOSFile function and therefore you require the absolute path to a file, of which you only know the relative path (in some data directory), you need a way to query this information. This is what plFileSystem::ResolvePath() is for. Basically you give it some path, and it will return under which absolute path and under which relative path the file was found. It also needs to know whether you want to read or write the file. If you want to read it, ie. the file is supposed to already exist, it will search for the file in all data directories. If you want to write it, it will return under which path the file would end up. See Also","title":"FileSystem"},{"location":"runtime/filesystem/#filesystem","text":"Accessing files is one of those things that you probably need to do very early in your project. Although the POSIX functions fopen, fread, etc. are available on most platforms, they are not always the best choice. PlasmaEngine provides multiple layers of abstractions for accessing files, or more generally, data streams that look like files.","title":"FileSystem"},{"location":"runtime/filesystem/#overview","text":"The following is a list of abstractions that you should be aware of. Each one will be explained in more detail below. Streams ( plStreamReader , plStreamWriter ) are the basis for all reading and writing of abstract data streams. These are the two classes that you will use whenever something that acts like a file is passed around, even though it does not need to represent an actual file. Low Level File Abstraction ( plOSFile ): This is the abstraction layer that implements reading and writing to actual files on disk in a mostly platform independent manner. This is what you typically should not need to use, at all. However, when something is not possible through the higher level abstractions, you might need to do it through this interface. High Level File System ( plFileSystem , plFileReader , plFileWriter ): Through plFileSystem you can configure a virtual file-system. plFileReader and plFileWriter should be what you typically use to open 'files' in that virtual file-system. plFileReader and plFileWriter implement the respective stream interfaces plStreamReader and plStreamWriter , so once you have opened a file through these classes, you can pass them to any function that works on streams.","title":"Overview"},{"location":"runtime/filesystem/#streams","text":"A 'stream' is simply a series of bytes. This data stream can come from a physical file, through a network or it can be generated fully procedurally. Streams typically come in two forms: Ones that you can read from and ones that you can write to. In PlasmaEngine those two types of streams are represented with the interface classes plStreamReader and plStreamWriter respectively. The stream interfaces are reduced in functionality to what all types of data streams can provide. Reading is always done from the current read position. Writing will always append data at the end of the stream. You cannot seek to an arbitrary position (when reading you can skip ahead) and you cannot re-read the same data or overwrite previously written data. Although this can be limiting in a few scenarios, it is absolutely sufficient for the vast majority of use cases. For the few cases where you really need to set the read or write position, you can revert to another abstraction layer, e.g. on an plOSFile you have more control. Inside the core engine we typically do not care about files, at all. Therefore you will not find many functions that take a file-name string. In the few cases that this is so, those functions are usually just convenience functions that internally open a file and then pass along the file as a stream. Instead most engine functions work directly on streams. This decouples the engine from working on the concept of files and enables to work on data from any kind of source. For example, this makes it easy to embed a file in an archive or to read it from a network, or even both combined.","title":"Streams"},{"location":"runtime/filesystem/#memory-streams","text":"A very frequently used implementation of streams is the 'memory stream'. This represents a stream of data that exists in RAM only. The data is stored in an plMemoryStreamStorage object. You can have multiple readers through the plMemoryStreamReader class and you could theoretically have multiple writers through the plMemoryStreamWriter class (though having more than one will probably not work in any useful way). Plasma Engine often uses memory streams to store incoming data for fast access later. For example the plResourceManager reads files from disk on a separate thread into a memory stream. This allows to do the slow file reading in parallel without blocking the main thread. Once all data is read into the memory stream, the main thread can then read the data directly from memory and have a guaranteed latency. The interface for reading from file or the memory stream is identical, so the code that actually interprets the file content does not need to know about this optimization at all.","title":"Memory Streams"},{"location":"runtime/filesystem/#other-types-of-streams","text":"Plasma Engine comes with a variety of stream implementations: Compressed Streams : Through plCompressedStreamReaderZstd and plCompressedStreamWriterZstd you can easily add compression to your data streams. These classes take another stream as input or output, so you can pass it a file or memory stream to work upon. Chunk Streams : A chunk stream is basically a stream that is divided into distinct parts which are fully separated from each other. This allows to handle one stream (e.g. a file) as if it were actually multiple streams, which is useful when you want to package multiple files into one. The useful feature of these streams is that code often reads a stream until it ends (e.g. nothing more can be read). When you package multiple files into one stream, this behavior can end badly. A chunk stream enables you to prevent code from reading further than a specific point (a virtual 'end-of-stream' position). Chunk formats are also useful when you only want to read or update certain parts of a file, without knowing how the rest of the file format works. This is possible because the size of each chunk is stored in the stream which allows to skip or read an entire chunk and pass it through. Please note that you can combine different types of streams. For example you can write to a file by using plFileWriter , pass that stream to plChunkStreamWriter and then even use plCompressedStreamWriterZstd to compress individual chunks in the file.","title":"Other Types of Streams"},{"location":"runtime/filesystem/#using-standard-types-with-streams","text":"The stream interfaces only provide the functions ReadBytes and WriteBytes . For most standard types Plasma Engine provides overloaded << and >> operators. E.g. you can read and write ints , floats , plVec3 , plString , etc. like this: write_stream << plVec3(1, 2, 3); write_stream << \"some string\"; plVec3 v; plString s; read_stream >> v; read_stream >> s;","title":"Using Standard Types with Streams"},{"location":"runtime/filesystem/#using-complex-types-with-streams","text":"To serialize and deserialize data that is in Plasma containers, you can also use functions like plStreamReader::ReadArray() , plStreamReader::ReadMap() , plStreamWriter::WriteArray() and plStreamWriter::WriteMap() . Those functions will try to serialize the container information and all the elements. For that to work, there must either be << and >> operaters overloaded for the element type, or the element types must have member functions with the following signature: plResult Serialize(plStreamWriter& stream) const; plResult Deserialize(plStreamReader& stream); Using these functions allows to return success or failure, which is not possible with the shift operators. If any element fails to de-/serialize, the whole operation (e.g. plStreamWriter::WriteArray() ) will fail and terminate early. Note: plStreamWriter::WriteArray() and similar functions are provided for convenience. However, when it is important to have full control over file versioning and backwards compatibility of a file format, it may be preferable to serialize containers manually.","title":"Using Complex Types with Streams"},{"location":"runtime/filesystem/#low-level-file-abstraction","text":"Reading and writing files is implemented through plOSFile . This class internally uses platform specific functions such as fopen on Linux and CreateFile on Windows. You should typically NOT use this class, there are higher level abstractions built upon plOSFile that you should prefer. Since plOSFile provides actual file access, it also has some file specific features, for instance you can set the file read or write position back and forth and you can get the OS specific file handle, such that you can use OS specific functions on that file yourself. Additionally there are static functions for deleting files, creating directory structures, querying whether a file exists, and so on. plOSFile is a thin abstraction over the operating system, it does not yet implement a higher level interface. Therefore it deliberately does not implement the stream interfaces, so you cannot pass an plOSFile instance to a function that takes an plStreamReader or plStreamWriter . Usually you only need to use plOSFile for some of the static functions that implement more infrequently used features, such as plOSFile::CopyFile or plOSFile::GetFileStats . These might not be available in higher level abstractions. Please be aware that plOSFile always requires platform specific absolute paths. There is no concept of a 'current working directory' or some root directory, to which relative paths could be used. plOSFile will check all incoming paths and assert that they are absolute. Therefore, if you need to use plOSFile , make sure to always convert any relative path to an absolute path before you pass it to plOSFile .","title":"Low Level File Abstraction"},{"location":"runtime/filesystem/#other-low-level-file-operations","text":"Apart from plOSFile , there are a few classes that implement additional file system operations that are only thin abstractions over the operating system. plFileSystemIterator is a class that allows to iterate over all files and folders in some directory. In a game this should rarely be necessary, but in tools this can be very useful. Unfortunately it is not guaranteed that this feature can be implemented on all platforms, and each platform might have different features regarding wild-card usage and so on. Therefore this class is only available when the preprocessor define PLASMA_SUPPORTS_FILE_ITERATORS is defined as PLASMA_ON . plFileStats is a struct that provides information about a file or folder. One way to retrieve file stats is through plOSFile::GetFileStats() . Again, this feature cannot be implemented on all platforms, so it is only available when PLASMA_SUPPORTS_FILE_STATS is defined as PLASMA_ON .","title":"Other Low Level File Operations"},{"location":"runtime/filesystem/#high-level-file-system","text":"Plasma Engine comes with a high level file system that is very flexible and powerful but might seem a bit confusing at first for people that are not used to working with abstractions on this level. A game engine is in many aspects simply a framework that manages resources. It ensures that resources can be found and accessed, are freed when not needed any more and otherwise are handled efficiently and often in an abstract manner that makes it easy to work with them. Working with files and everything that is similar enough so that it could be handled like a file, is the lowest level at which this 'managing of resources' begins. Plasma Engine does this by working with a virtual file system. The central class to configure this file system is plFileSystem .","title":"High Level File System"},{"location":"runtime/filesystem/#data-directories","text":"The most important concept that you need to understand is that of 'data directories'. A data directory is basically a mount point that is added to the virtual file system. You can have an arbitrary number of those, but usually you will only need very few. A data directory is in the most common use case some folder on your harddisk. By adding it to the file system (using plFileSystem::AddDataDirectory ) you make that folder visible for the application. When you work with the virtual file system, you do NOT use plOSFile . Instead, you will most likely use plFileReader and plFileWriter . Those classes represent a single open file in the virtual file system. They implement the stream interfaces mentioned above. When you open a file through these classes, you should use relative paths. Absolute paths will only work if a data directory above the given path is also mounted. When you open a file through a relative path, all data directories are searched (in the reverse order in which they were added) for the file. So as long as the file exists in one data directory, it can be opened. If it exists in multiple data directories with the same relative path, you will typically get the file from the data directory that was added last. The use case here is, that you often want to have some 'base' data directory that contains general files and than additional data directories that add project or even level specific files to the file system. You will typically add the 'base' data directory right at startup of your game. Later you add your project specific data directories. Since you often want your files in the project data directory to take precedence over the 'base' data directory, it makes sense for the file system to search the directories in the reverse order in which they were added. One use case for multiple data directories with the same files in it is localization. Suppose you have a folder with all your sounds, including speech that is recorded in English. When you want to add another language, you can just create a folder 'German' that only contains the sound files which need localization. When the user wants German language support, you simply mount the German sounds folder AFTER the other data directory. Now whenever a sound file gets loaded that is both in the German sounds data directory and the default data directory, the German sound will be preferred. Everything that is not there falls back to the default data directory. Thus when some sound file was forgotten to be included in the German localization, the game will at least play the sound in English.","title":"Data Directories"},{"location":"runtime/filesystem/#advanced-data-directories","text":"Although the most common use case is to mount folders as data directories, Plasma Engine's concept of data directories goes much further. Basically anything can be 'mounted' as a data directory. For example you could mount a zip file and then use it like a read-only folder. Or you could mount a folder on a remote PC and have the data sent over a network. This is possible because data directories are an abstract concept. To create your own type of data directory, you need to implement three classes, derived from plDataDirectoryType , plDataDirectoryReader and plDataDirectoryWriter respectively. Therefore, when you add a data directory to plFileSystem , the engine needs to know how exactly to mount this type. This is what 'data directory factories' are necessary for. So for example, if you want to support simple folders and zip files as data directories, you need to register one factory for 'folders' and one factory for 'zip archives'. Now when you mount a data directory with the name \"My/Test/Archive.zip\", the engine will ask each factory, whether it can handle this path. The 'folder factory' will detect that this is a file and not a folder and therefore decline. The 'zip factory' detects that this is a zip file and therefore creates a data directory which provides the functionality to read files from the archive as if it were a real folder. For example you could then open a file with the path \"My/Test/Archive.zip/Some/Compressed/File.txt\" through plFileReader and it would just work as if the file was located in a real folder. That is why you always need to register these factories first, before you can add any data directories to the virtual file system. If you want to implement your own data directory type, please have a look at plDataDirectory::FolderType .","title":"Advanced Data Directories"},{"location":"runtime/filesystem/#setting-up-the-file-system","text":"Setting up the file system is very easy to do, once you know what is involved and why. The most basic configuration looks like this: // register a factory that can handle simple folders // this is actually already done automatically through the plStartup system // plFileSystem::RegisterDataDirectoryFactory(plDataDirectory::FolderType::Factory); // mount the application directory (where the binary is located) as a data directory plFileSystem::AddDataDirectory(plOSFile::GetApplicationDirectory()); That's all. Now you can read and write files in that folder like this: plFileWriter FileOut; if (FileOut.Open(\"SubFolder/Test.txt\") == PLASMA_SUCCESS) { // \"SubFolder\" will be created automatically, if it does not exist yet FileOut << \"This is a string\"; FileOut.Close(); // will also be called automatically when FileOut goes out of scope } plFileReader FileIn; if (FileIn.Open(\"SubFolder/Test.txt\") == PLASMA_SUCCESS) { plString s; FileIn >> s; FileIn.Close(); // will also be called automatically when FileIn goes out of scope PLASMA_ASSERT_DEV(s == \"This is a string\", \"The read string is incorrect: '%s'\", s.GetData()); } Obviously you can now add further data directories through plFileSystem::AddDataDirectory() . There are several additional features to configure the system for your exact use case. Please have a look at the API documentation for further details.","title":"Setting up the File System"},{"location":"runtime/filesystem/#resolving-paths","text":"One thing that comes up once in a while is that you need to convert a path from relative to absolute or vice versa. For example, should you actually need to use an plOSFile function and therefore you require the absolute path to a file, of which you only know the relative path (in some data directory), you need a way to query this information. This is what plFileSystem::ResolvePath() is for. Basically you give it some path, and it will return under which absolute path and under which relative path the file was found. It also needs to know whether you want to read or write the file. If you want to read it, ie. the file is supposed to already exist, it will search for the file in all data directories. If you want to write it, it will return under which path the file would end up.","title":"Resolving Paths"},{"location":"runtime/filesystem/#see-also","text":"","title":"See Also"},{"location":"runtime/reflection-system/","text":"Reflection System The Plasma Engine reflection system allows to inspect structs and classes at runtime. It is used primarily for communication with tools and serialization. The reflection system is macro-based, meaning that it is not generated automatically but needs to be written manually for each type, member, etc that needs to be known at runtime. Types There are four distinct types that can be represented by reflection: classes, structs, enums and bitflags. Each is represented by the plRTTI class that stores the type information. Classes Classes are separated into two types: dynamic and static reflected. Dynamic classes derive from plReflectedClass which allows you to determine its type using plReflectedClass::GetDynamicRTTI() . So with a pointer to an plReflectedClass you can access its type information. A static reflected class does not derive from plReflectedClass so it is not possible to get the RTTI information in a common way. However, if you know the type of a variable you can use the template function plGetStaticRTTI to retrieve the plRTTI instance of a specific type. Alternatively, you can also search for a type by name using plRTTI::FindTypeByName() . plReflectedClass* pTest = new plDynamicTestClass; const plRTTI* pRtti = pTest->GetDynamicRTTI(); const plRTTI* pRtti2 = plGetStaticRTTI<plDynamicTestClass>(); const plRTTI* pRtti3 = plRTTI::FindTypeByName(\"plDynamicTestClass\"); Declaring a dynamic class involves deriving from plReflectedClass , adding the PLASMA_ADD_DYNAMIC_REFLECTION(SELF, BASE_TYPE) macro into the class body and adding a PLASMA_BEGIN_DYNAMIC_REFLECTED_TYPE(Type, Version, AllocatorType) block into a compilation unit. //Header class plDynamicTestClass : public plReflectedClass { PLASMA_ADD_DYNAMIC_REFLECTION(plDynamicTestClass, plReflectedClass); }; //Cpp PLASMA_BEGIN_DYNAMIC_REFLECTED_TYPE(plDynamicTestClass, 1, plRTTIDefaultAllocator<plDynamicTestClass>) PLASMA_END_DYNAMIC_REFLECTED_TYPE Declaring a static class is very similar to declaring a dynamic class. However, you need to declare the type outside the class via PLASMA_DECLARE_REFLECTABLE_TYPE(Linkage, TYPE) and use PLASMA_BEGIN_STATIC_REFLECTED_TYPE(Type, BaseType, Version, AllocatorType) in a compilation unit. If a class has no base class, use the dummy class plNoBase instead. // Header class plStaticTestClass { }; PLASMA_DECLARE_REFLECTABLE_TYPE(PLASMA_NO_LINKAGE, plStaticTestClass); // Cpp PLASMA_BEGIN_STATIC_REFLECTED_TYPE(plStaticTestClass, plNoBase, 1, plRTTIDefaultAllocator<plStaticTestClass>); PLASMA_END_STATIC_REFLECTED_TYPE Structs Structs are identical to static reflected classes so you can use the exact same macros. Enums Enums are limited to structured enums, i.e. those used by the plEnum class. Declaration is similar to static classes, but you use PLASMA_BEGIN_STATIC_REFLECTED_ENUM(Type, Version) instead in the compilation unit code. // Header struct plExampleEnum { typedef plInt8 StorageType; enum Enum { Value1 = 1, // normal value Value2 = -2, // normal value Value3 = 4, // normal value Default = Value1 // Default initialization value (required) }; }; PLASMA_DECLARE_REFLECTABLE_TYPE(PLASMA_NO_LINKAGE, plExampleEnum); // Cpp PLASMA_BEGIN_STATIC_REFLECTED_ENUM(plExampleEnum, 1) PLASMA_ENUM_CONSTANTS(plExampleEnum::Value1, plExampleEnum::Value2) PLASMA_ENUM_CONSTANT(plExampleEnum::Value3), PLASMA_END_STATIC_REFLECTED_ENUM The enum constants can either be declared via PLASMA_ENUM_CONSTANTS() or PLASMA_ENUM_CONSTANT(Value) inside the begin / end block of the enum declaration. An enum type can be identified by its base type which is always the dummy plEnumBase . Bitflags Bitflags are limited to structured bitflags, i.e. those used by the plBitflags class. Declaration is similar to static classes, but you use PLASMA_BEGIN_STATIC_REFLECTED_BITFLAGS(Type, Version) instead in the compilation unit code. // Header struct plExampleBitflags { typedef plUInt64 StorageType; enum Enum : plUInt64 { Value1 = PLASMA_BIT(0), // normal value Value2 = PLASMA_BIT(31), // normal value Value3 = PLASMA_BIT(63), // normal value Default = Value1 // Default initialization value (required) }; struct Bits { StorageType Value1 : 1; StorageType Padding : 30; StorageType Value2 : 1; StorageType Padding2 : 31; StorageType Value3 : 1; }; }; PLASMA_DECLARE_REFLECTABLE_TYPE(PLASMA_NO_LINKAGE, plExampleBitflags); // Cpp PLASMA_BEGIN_STATIC_REFLECTED_BITFLAGS(plExampleBitflags, 1) PLASMA_BITFLAGS_CONSTANTS(plExampleBitflags::Value1, plExampleBitflags::Value2) PLASMA_BITFLAGS_CONSTANT(plExampleBitflags::Value3), PLASMA_END_STATIC_REFLECTED_BITFLAGS(); The bitflags constants can either be declared via PLASMA_BITFLAGS_CONSTANTS() or PLASMA_BITFLAGS_CONSTANT(Value) inside the begin / end block of the bitflags declaration. A bitflags type can be identified by its base type which is always the dummy plBitflagsBase . Properties Properties are the most important information in a type as they define the data inside it. The properties of a type can be accessed via plRTTI::GetProperties() . There are different categories of properties, each deriving from plAbstractProperty . The type of property can be determined by calling plAbstractProperty::GetCategory() . Properties are added via the property macros inside the PLASMA_BEGIN_PROPERTIES() / PLASMA_END_PROPERTIES() block of the type declaration like this: PLASMA_BEGIN_STATIC_REFLECTED_TYPE(plStaticTestClass, plNoBase, 1, plRTTIDefaultAllocator<plStaticTestClass>) { PLASMA_BEGIN_PROPERTIES { PLASMA_CONSTANT_PROPERTY(\"Constant\", 5), PLASMA_MEMBER_PROPERTY(\"Member\", m_fFloat), PLASMA_ACCESSOR_PROPERTY(\"MemberAccessor\", GetInt, SetInt), PLASMA_ARRAY_MEMBER_PROPERTY(\"Array\", m_Deque), PLASMA_ARRAY_ACCESSOR_PROPERTY(\"ArrayAccessor\", GetCount, GetValue, SetValue, Insert, Remove), PLASMA_SET_MEMBER_PROPERTY(\"Set\", m_SetMember), PLASMA_SET_ACCESSOR_PROPERTY(\"SetAccessor\", GetSet, SetInsert, SetRemove), } PLASMA_END_PROPERTIES } PLASMA_END_STATIC_REFLECTED_TYPE(); Constants Constants are declared via PLASMA_CONSTANT_PROPERTY(PropertyName, Value) . The value is stored within the property so no instance of the class is necessary to access it. To access the constant, cast the property to plAbstractConstantProperty and call plAbstractConstantProperty::GetPropertyType() to determine the constant type. Then either cast to plTypedConstantProperty of the matching type, or if the type is not known to you at compile time, use plAbstractConstantProperty::GetPropertyPointer() to access its data. Members There are two types of member properties, direct member properties and accessor properties. The first has direct access to the memory location of the property in the class while the later uses functions to get and set the property's value. Direct member properties are declared via PLASMA_MEMBER_PROPERTY(PropertyName, MemberName) while accessor properties are declared via PLASMA_ACCESSOR_PROPERTY(PropertyName, Getter, Setter) . The getter and setter functions must have the following signature: Type GetterFunc() const; void SetterFunc(Type value); Type can be decorated with const and reference but must be consistent between get and set function. The available macros are the following: PLASMA_MEMBER_PROPERTY(\"Member\", m_fFloat1), PLASMA_MEMBER_PROPERTY_READ_ONLY(\"MemberRO\", m_vProperty3), PLASMA_ACCESSOR_PROPERTY(\"MemberAccessor\", GetInt, SetInt), PLASMA_ACCESSOR_PROPERTY_READ_ONLY(\"MemberAccessorRO\", GetInt), To access an instance's member variable value, cast the property to plAbstractMemberProperty and call plAbstractMemberProperty::GetPropertyType() to determine the member type. Then either cast to plTypedMemberProperty of the matching type, or if the type is not known to you at compile time, use plAbstractMemberProperty::GetPropertyPointer() or plAbstractMemberProperty::GetValuePtr() and plAbstractMemberProperty::SetValuePtr() to access its data. The first solution will only return a valid pointer if the property is a direct member property. Arrays Array properties are very similar to member properties, they just handle arrays instead of single values. Direct array properties are declared via PLASMA_ARRAY_MEMBER_PROPERTY(PropertyName, MemberName) while accessor array properties are declared via PLASMA_ARRAY_ACCESSOR_PROPERTY(PropertyName, GetCount, Getter, Setter, Insert, Remove) . The accessor interface functions must have the following signature: plUInt32 GetCount() const; Type GetValue(plUInt32 uiIndex) const; void SetValue(plUInt32 uiIndex, Type value); void Insert(plUInt32 uiIndex, Type value); void Remove(plUInt32 uiIndex); The available macros are the following: PLASMA_ARRAY_ACCESSOR_PROPERTY(\"ArrayAccessor\", GetCount, GetValue, SetValue, Insert, Remove), PLASMA_ARRAY_ACCESSOR_PROPERTY_READ_ONLY(\"ArrayAccessorRO\", GetCount, GetValue), PLASMA_ARRAY_MEMBER_PROPERTY(\"Hybrid\", m_Hybrid), PLASMA_ARRAY_MEMBER_PROPERTY(\"Dynamic\", m_Dynamic), PLASMA_ARRAY_MEMBER_PROPERTY_READ_ONLY(\"Deque\", m_Deque), To access an instance's array, cast the property to plAbstractArrayProperty and call plAbstractArrayProperty::GetElementType() to determine the element type. From here you can use the various functions inside plAbstractArrayProperty to manipulate an instance's array. Sets Set properties are very similar to member properties, they just handle sets instead of single values. Direct set properties are declared via PLASMA_SET_MEMBER_PROPERTY(PropertyName, MemberName) while accessor set properties are declared via PLASMA_SET_ACCESSOR_PROPERTY(PropertyName, GetValues, Insert, Remove) . The accessor interface functions must have the following signature: void Insert(Type value); void Remove(Type value); Container<Type> GetValues() const; The available macros are the following: PLASMA_SET_ACCESSOR_PROPERTY(\"SetAccessor\", GetValues, Insert, Remove), PLASMA_SET_ACCESSOR_PROPERTY_READ_ONLY(\"SetAccessorRO\", GetValues), PLASMA_SET_MEMBER_PROPERTY(\"Set\", m_SetMember), PLASMA_SET_MEMBER_PROPERTY_READ_ONLY(\"SetRO\", m_SetMember), To access an instance's set, cast the property to plAbstractSetProperty and call plAbstractSetProperty::GetElementType() to determine the element type. From here you can use the various functions inside plAbstractSetProperty to manipulate an instance's set. Flags Types as well as properties have flags that quickly let you determine the kind of type / property you are dealing with. For types, plRTTI::GetTypeFlags() lets you access its plTypeFlags::Enum flags which are automatically deduced from the type at compile time. Properties can have flags as well, plAbstractMemberProperty::GetFlags() , plAbstractArrayProperty::GetFlags() and plAbstractSetProperty::GetFlags() let you access the plPropertyFlags::Enum flags of the handled property type. The only difference here is that besides automatically deduced flags there are also user-defined flags that can be added during declaration of the property by using plAbstractMemberProperty::AddFlags and the variants on the other property categories: PLASMA_ACCESSOR_PROPERTY(\"ArraysPtr\", GetArrays, SetArrays)->AddFlags(plPropertyFlags::PointerOwner), Limitations No two types can share the same name. Each property name must be unique within its type. Only constants that are a basic type (i.e. can be stored inside an plVariant ) will be available to tools. A pointer to a type cannot be its own type, the only exception to this is const char*.","title":"Reflection System"},{"location":"runtime/reflection-system/#reflection-system","text":"The Plasma Engine reflection system allows to inspect structs and classes at runtime. It is used primarily for communication with tools and serialization. The reflection system is macro-based, meaning that it is not generated automatically but needs to be written manually for each type, member, etc that needs to be known at runtime.","title":"Reflection System"},{"location":"runtime/reflection-system/#types","text":"There are four distinct types that can be represented by reflection: classes, structs, enums and bitflags. Each is represented by the plRTTI class that stores the type information.","title":"Types"},{"location":"runtime/reflection-system/#classes","text":"Classes are separated into two types: dynamic and static reflected. Dynamic classes derive from plReflectedClass which allows you to determine its type using plReflectedClass::GetDynamicRTTI() . So with a pointer to an plReflectedClass you can access its type information. A static reflected class does not derive from plReflectedClass so it is not possible to get the RTTI information in a common way. However, if you know the type of a variable you can use the template function plGetStaticRTTI to retrieve the plRTTI instance of a specific type. Alternatively, you can also search for a type by name using plRTTI::FindTypeByName() . plReflectedClass* pTest = new plDynamicTestClass; const plRTTI* pRtti = pTest->GetDynamicRTTI(); const plRTTI* pRtti2 = plGetStaticRTTI<plDynamicTestClass>(); const plRTTI* pRtti3 = plRTTI::FindTypeByName(\"plDynamicTestClass\"); Declaring a dynamic class involves deriving from plReflectedClass , adding the PLASMA_ADD_DYNAMIC_REFLECTION(SELF, BASE_TYPE) macro into the class body and adding a PLASMA_BEGIN_DYNAMIC_REFLECTED_TYPE(Type, Version, AllocatorType) block into a compilation unit. //Header class plDynamicTestClass : public plReflectedClass { PLASMA_ADD_DYNAMIC_REFLECTION(plDynamicTestClass, plReflectedClass); }; //Cpp PLASMA_BEGIN_DYNAMIC_REFLECTED_TYPE(plDynamicTestClass, 1, plRTTIDefaultAllocator<plDynamicTestClass>) PLASMA_END_DYNAMIC_REFLECTED_TYPE Declaring a static class is very similar to declaring a dynamic class. However, you need to declare the type outside the class via PLASMA_DECLARE_REFLECTABLE_TYPE(Linkage, TYPE) and use PLASMA_BEGIN_STATIC_REFLECTED_TYPE(Type, BaseType, Version, AllocatorType) in a compilation unit. If a class has no base class, use the dummy class plNoBase instead. // Header class plStaticTestClass { }; PLASMA_DECLARE_REFLECTABLE_TYPE(PLASMA_NO_LINKAGE, plStaticTestClass); // Cpp PLASMA_BEGIN_STATIC_REFLECTED_TYPE(plStaticTestClass, plNoBase, 1, plRTTIDefaultAllocator<plStaticTestClass>); PLASMA_END_STATIC_REFLECTED_TYPE","title":"Classes"},{"location":"runtime/reflection-system/#structs","text":"Structs are identical to static reflected classes so you can use the exact same macros.","title":"Structs"},{"location":"runtime/reflection-system/#enums","text":"Enums are limited to structured enums, i.e. those used by the plEnum class. Declaration is similar to static classes, but you use PLASMA_BEGIN_STATIC_REFLECTED_ENUM(Type, Version) instead in the compilation unit code. // Header struct plExampleEnum { typedef plInt8 StorageType; enum Enum { Value1 = 1, // normal value Value2 = -2, // normal value Value3 = 4, // normal value Default = Value1 // Default initialization value (required) }; }; PLASMA_DECLARE_REFLECTABLE_TYPE(PLASMA_NO_LINKAGE, plExampleEnum); // Cpp PLASMA_BEGIN_STATIC_REFLECTED_ENUM(plExampleEnum, 1) PLASMA_ENUM_CONSTANTS(plExampleEnum::Value1, plExampleEnum::Value2) PLASMA_ENUM_CONSTANT(plExampleEnum::Value3), PLASMA_END_STATIC_REFLECTED_ENUM The enum constants can either be declared via PLASMA_ENUM_CONSTANTS() or PLASMA_ENUM_CONSTANT(Value) inside the begin / end block of the enum declaration. An enum type can be identified by its base type which is always the dummy plEnumBase .","title":"Enums"},{"location":"runtime/reflection-system/#bitflags","text":"Bitflags are limited to structured bitflags, i.e. those used by the plBitflags class. Declaration is similar to static classes, but you use PLASMA_BEGIN_STATIC_REFLECTED_BITFLAGS(Type, Version) instead in the compilation unit code. // Header struct plExampleBitflags { typedef plUInt64 StorageType; enum Enum : plUInt64 { Value1 = PLASMA_BIT(0), // normal value Value2 = PLASMA_BIT(31), // normal value Value3 = PLASMA_BIT(63), // normal value Default = Value1 // Default initialization value (required) }; struct Bits { StorageType Value1 : 1; StorageType Padding : 30; StorageType Value2 : 1; StorageType Padding2 : 31; StorageType Value3 : 1; }; }; PLASMA_DECLARE_REFLECTABLE_TYPE(PLASMA_NO_LINKAGE, plExampleBitflags); // Cpp PLASMA_BEGIN_STATIC_REFLECTED_BITFLAGS(plExampleBitflags, 1) PLASMA_BITFLAGS_CONSTANTS(plExampleBitflags::Value1, plExampleBitflags::Value2) PLASMA_BITFLAGS_CONSTANT(plExampleBitflags::Value3), PLASMA_END_STATIC_REFLECTED_BITFLAGS(); The bitflags constants can either be declared via PLASMA_BITFLAGS_CONSTANTS() or PLASMA_BITFLAGS_CONSTANT(Value) inside the begin / end block of the bitflags declaration. A bitflags type can be identified by its base type which is always the dummy plBitflagsBase .","title":"Bitflags"},{"location":"runtime/reflection-system/#properties","text":"Properties are the most important information in a type as they define the data inside it. The properties of a type can be accessed via plRTTI::GetProperties() . There are different categories of properties, each deriving from plAbstractProperty . The type of property can be determined by calling plAbstractProperty::GetCategory() . Properties are added via the property macros inside the PLASMA_BEGIN_PROPERTIES() / PLASMA_END_PROPERTIES() block of the type declaration like this: PLASMA_BEGIN_STATIC_REFLECTED_TYPE(plStaticTestClass, plNoBase, 1, plRTTIDefaultAllocator<plStaticTestClass>) { PLASMA_BEGIN_PROPERTIES { PLASMA_CONSTANT_PROPERTY(\"Constant\", 5), PLASMA_MEMBER_PROPERTY(\"Member\", m_fFloat), PLASMA_ACCESSOR_PROPERTY(\"MemberAccessor\", GetInt, SetInt), PLASMA_ARRAY_MEMBER_PROPERTY(\"Array\", m_Deque), PLASMA_ARRAY_ACCESSOR_PROPERTY(\"ArrayAccessor\", GetCount, GetValue, SetValue, Insert, Remove), PLASMA_SET_MEMBER_PROPERTY(\"Set\", m_SetMember), PLASMA_SET_ACCESSOR_PROPERTY(\"SetAccessor\", GetSet, SetInsert, SetRemove), } PLASMA_END_PROPERTIES } PLASMA_END_STATIC_REFLECTED_TYPE();","title":"Properties"},{"location":"runtime/reflection-system/#constants","text":"Constants are declared via PLASMA_CONSTANT_PROPERTY(PropertyName, Value) . The value is stored within the property so no instance of the class is necessary to access it. To access the constant, cast the property to plAbstractConstantProperty and call plAbstractConstantProperty::GetPropertyType() to determine the constant type. Then either cast to plTypedConstantProperty of the matching type, or if the type is not known to you at compile time, use plAbstractConstantProperty::GetPropertyPointer() to access its data.","title":"Constants"},{"location":"runtime/reflection-system/#members","text":"There are two types of member properties, direct member properties and accessor properties. The first has direct access to the memory location of the property in the class while the later uses functions to get and set the property's value. Direct member properties are declared via PLASMA_MEMBER_PROPERTY(PropertyName, MemberName) while accessor properties are declared via PLASMA_ACCESSOR_PROPERTY(PropertyName, Getter, Setter) . The getter and setter functions must have the following signature: Type GetterFunc() const; void SetterFunc(Type value); Type can be decorated with const and reference but must be consistent between get and set function. The available macros are the following: PLASMA_MEMBER_PROPERTY(\"Member\", m_fFloat1), PLASMA_MEMBER_PROPERTY_READ_ONLY(\"MemberRO\", m_vProperty3), PLASMA_ACCESSOR_PROPERTY(\"MemberAccessor\", GetInt, SetInt), PLASMA_ACCESSOR_PROPERTY_READ_ONLY(\"MemberAccessorRO\", GetInt), To access an instance's member variable value, cast the property to plAbstractMemberProperty and call plAbstractMemberProperty::GetPropertyType() to determine the member type. Then either cast to plTypedMemberProperty of the matching type, or if the type is not known to you at compile time, use plAbstractMemberProperty::GetPropertyPointer() or plAbstractMemberProperty::GetValuePtr() and plAbstractMemberProperty::SetValuePtr() to access its data. The first solution will only return a valid pointer if the property is a direct member property.","title":"Members"},{"location":"runtime/reflection-system/#arrays","text":"Array properties are very similar to member properties, they just handle arrays instead of single values. Direct array properties are declared via PLASMA_ARRAY_MEMBER_PROPERTY(PropertyName, MemberName) while accessor array properties are declared via PLASMA_ARRAY_ACCESSOR_PROPERTY(PropertyName, GetCount, Getter, Setter, Insert, Remove) . The accessor interface functions must have the following signature: plUInt32 GetCount() const; Type GetValue(plUInt32 uiIndex) const; void SetValue(plUInt32 uiIndex, Type value); void Insert(plUInt32 uiIndex, Type value); void Remove(plUInt32 uiIndex); The available macros are the following: PLASMA_ARRAY_ACCESSOR_PROPERTY(\"ArrayAccessor\", GetCount, GetValue, SetValue, Insert, Remove), PLASMA_ARRAY_ACCESSOR_PROPERTY_READ_ONLY(\"ArrayAccessorRO\", GetCount, GetValue), PLASMA_ARRAY_MEMBER_PROPERTY(\"Hybrid\", m_Hybrid), PLASMA_ARRAY_MEMBER_PROPERTY(\"Dynamic\", m_Dynamic), PLASMA_ARRAY_MEMBER_PROPERTY_READ_ONLY(\"Deque\", m_Deque), To access an instance's array, cast the property to plAbstractArrayProperty and call plAbstractArrayProperty::GetElementType() to determine the element type. From here you can use the various functions inside plAbstractArrayProperty to manipulate an instance's array.","title":"Arrays"},{"location":"runtime/reflection-system/#sets","text":"Set properties are very similar to member properties, they just handle sets instead of single values. Direct set properties are declared via PLASMA_SET_MEMBER_PROPERTY(PropertyName, MemberName) while accessor set properties are declared via PLASMA_SET_ACCESSOR_PROPERTY(PropertyName, GetValues, Insert, Remove) . The accessor interface functions must have the following signature: void Insert(Type value); void Remove(Type value); Container<Type> GetValues() const; The available macros are the following: PLASMA_SET_ACCESSOR_PROPERTY(\"SetAccessor\", GetValues, Insert, Remove), PLASMA_SET_ACCESSOR_PROPERTY_READ_ONLY(\"SetAccessorRO\", GetValues), PLASMA_SET_MEMBER_PROPERTY(\"Set\", m_SetMember), PLASMA_SET_MEMBER_PROPERTY_READ_ONLY(\"SetRO\", m_SetMember), To access an instance's set, cast the property to plAbstractSetProperty and call plAbstractSetProperty::GetElementType() to determine the element type. From here you can use the various functions inside plAbstractSetProperty to manipulate an instance's set.","title":"Sets"},{"location":"runtime/reflection-system/#flags","text":"Types as well as properties have flags that quickly let you determine the kind of type / property you are dealing with. For types, plRTTI::GetTypeFlags() lets you access its plTypeFlags::Enum flags which are automatically deduced from the type at compile time. Properties can have flags as well, plAbstractMemberProperty::GetFlags() , plAbstractArrayProperty::GetFlags() and plAbstractSetProperty::GetFlags() let you access the plPropertyFlags::Enum flags of the handled property type. The only difference here is that besides automatically deduced flags there are also user-defined flags that can be added during declaration of the property by using plAbstractMemberProperty::AddFlags and the variants on the other property categories: PLASMA_ACCESSOR_PROPERTY(\"ArraysPtr\", GetArrays, SetArrays)->AddFlags(plPropertyFlags::PointerOwner),","title":"Flags"},{"location":"runtime/reflection-system/#limitations","text":"No two types can share the same name. Each property name must be unique within its type. Only constants that are a basic type (i.e. can be stored inside an plVariant ) will be available to tools. A pointer to a type cannot be its own type, the only exception to this is const char*.","title":"Limitations"},{"location":"runtime/resource-management/","text":"Resource Management Resource management is fully functional, but currently undocumented. See Also","title":"Resource Management"},{"location":"runtime/resource-management/#resource-management","text":"Resource management is fully functional, but currently undocumented.","title":"Resource Management"},{"location":"runtime/resource-management/#see-also","text":"","title":"See Also"},{"location":"runtime/application/application/","text":"Application See Game States The classes plApplication , plGameApplicationBase and plGameApplication are fully functional, but currently undocumented. See Also Common Application Features Game States","title":"Application"},{"location":"runtime/application/application/#application","text":"See Game States The classes plApplication , plGameApplicationBase and plGameApplication are fully functional, but currently undocumented.","title":"Application"},{"location":"runtime/application/application/#see-also","text":"Common Application Features Game States","title":"See Also"},{"location":"runtime/application/common-application-features/","text":"Common Application Features All applications that are built on top of plGameApplication provide a number of useful features for developers. In-game console Press F1 to toggle the in-game console . See its documentation for further details. Reload Resources Press F4 to instruct the engine to reload all resources (TODO) . This can be useful, if for example, you are working on a shader (TODO) and want to see the result of your changes inside the game, without having to restart the game. Reloading resources works for all assets that are used directly by the engine (e.g. shaders (TODO) ). For some assets it will work, after the editor processed the input assets . For example for textures and materials and many others. Some resources, though, are not reloadable, e.g. things that get instantiated at runtime, such as prefabs . Show Frames Per Second Press F5 to toggle the display of the FPS counter. Take a Profiling Capture Press F8 to take a capture of the profiling data. See the profiling documentation for details. Take a RenderDoc Capture Press F11 to take a RenderDoc capture. See the RenderDoc integration documentation for details. Take a Screenshot Press F12 to take a screenshot. Screenshots are stored in the appdata data directory (see the log output ). On Windows this can be found by typing %appdata% into the Windows Explorer. See Also Application (TODO) plPlayer Profiling RenderDoc Integration","title":"Common Application Features"},{"location":"runtime/application/common-application-features/#common-application-features","text":"All applications that are built on top of plGameApplication provide a number of useful features for developers.","title":"Common Application Features"},{"location":"runtime/application/common-application-features/#in-game-console","text":"Press F1 to toggle the in-game console . See its documentation for further details.","title":"In-game console"},{"location":"runtime/application/common-application-features/#reload-resources","text":"Press F4 to instruct the engine to reload all resources (TODO) . This can be useful, if for example, you are working on a shader (TODO) and want to see the result of your changes inside the game, without having to restart the game. Reloading resources works for all assets that are used directly by the engine (e.g. shaders (TODO) ). For some assets it will work, after the editor processed the input assets . For example for textures and materials and many others. Some resources, though, are not reloadable, e.g. things that get instantiated at runtime, such as prefabs .","title":"Reload Resources"},{"location":"runtime/application/common-application-features/#show-frames-per-second","text":"Press F5 to toggle the display of the FPS counter.","title":"Show Frames Per Second"},{"location":"runtime/application/common-application-features/#take-a-profiling-capture","text":"Press F8 to take a capture of the profiling data. See the profiling documentation for details.","title":"Take a Profiling Capture"},{"location":"runtime/application/common-application-features/#take-a-renderdoc-capture","text":"Press F11 to take a RenderDoc capture. See the RenderDoc integration documentation for details.","title":"Take a RenderDoc Capture"},{"location":"runtime/application/common-application-features/#take-a-screenshot","text":"Press F12 to take a screenshot. Screenshots are stored in the appdata data directory (see the log output ). On Windows this can be found by typing %appdata% into the Windows Explorer.","title":"Take a Screenshot"},{"location":"runtime/application/common-application-features/#see-also","text":"Application (TODO) plPlayer Profiling RenderDoc Integration","title":"See Also"},{"location":"runtime/application/game-state/","text":"Game States Most game code is implemented by writing custom components . However, components always work in the context of an object, be it a single game object or an entire prefab . The most that a single component can be responsible for, is to do high level logic for a level, by acting as a global message handler . However, for a full game you need a layer of control that is outside the world, where you can do logic like what level to load, what to do when the player dies or reaches their goal, how to display a main menu for the game settings and level selection, and so on. Most of these things would be possible with world components alone, but it would be cumbersome. Especially switching from one level to another is difficult, if some of your overall game logic has to be transitioned as well. Game States are this extra layer. A game state sits between the application (TODO) layer and the world . A game state is in so far optional, in that the engine will create an instance of plFallbackGameState , if no custom game state is available. The diagram above shows, that the editor skips the game state in simulate mode . In practice that means that the editor will not allocate any game state when the scene is being edited or only simulated. Only when you enter play-the-game mode , will it create a game state, which can then take over full control for the windows, input and the main camera. Game State Responsibilities The typical things that a game state controls are: Spawning one or multiple windows Setting up the main render pipeline Creating a world and loading a scene into it Unless it's run in the editor, where it is handed an existing world Setting up input devices and bindings Processing main input (not component specific input) Setting up and controlling the main camera Spawning the player prefab Displaying game UI Providing a main menu Saving and restoring global state (progression, high-scores, etc) For example when you have a player start point component in a scene, the component itself doesn't do anything, it just holds some data. Instead, when you enter play-the-game mode, the active game state can (but is not required to) use the information from these components to spawn a player prefab. Similarly, most scenes have a camera component whose usage hint is set to 'Main Camera' (this may be part of the player prefab). This camera defines what part of the scene will be shown on screen. At least that's how it appears. In reality it is the game state that controls the camera for the main render target. It's simply a feature of the plFallbackGameState , that it searches the world for an appropriate camera component and applies that to the main camera. If it doesn't find any such camera component, it provides simple WASD fly-camera controls. You can even cycle through the different camera components in a scene using Page Up and Page Down . As you can see, by implementing a custom game state, you can gain control over many things that otherwise appear to be built-in. Game State Instantiation It is the responsibility of the plGameApplication to instantiate a game state. By default this is done right at application startup, but if you write your own application (TODO) you could handle this differently. For example the editor only instantiates the game state for play-the-game mode. The application knows what game states are available through the reflection information . When a game state is needed, all available ones are instantiated and 'asked' (via plGameStateBase::DeterminePriority() ) whether it is the right one to handle the situation. The game state that is the best fit will be kept and gets control. The idea is, that there are typically only few game states available anyway. Usually you have the built-in plFallbackGameState and then you have one other game state implementation from your custom game plugin. The latter one will take precedence. You could have multiple game states, for example when you have multiple game plugins loaded simultaneously, but then they would need to somehow figure out which one should get activated (e.g. through command line arguments). See Also Custom Code Application (TODO)","title":"Game States"},{"location":"runtime/application/game-state/#game-states","text":"Most game code is implemented by writing custom components . However, components always work in the context of an object, be it a single game object or an entire prefab . The most that a single component can be responsible for, is to do high level logic for a level, by acting as a global message handler . However, for a full game you need a layer of control that is outside the world, where you can do logic like what level to load, what to do when the player dies or reaches their goal, how to display a main menu for the game settings and level selection, and so on. Most of these things would be possible with world components alone, but it would be cumbersome. Especially switching from one level to another is difficult, if some of your overall game logic has to be transitioned as well. Game States are this extra layer. A game state sits between the application (TODO) layer and the world . A game state is in so far optional, in that the engine will create an instance of plFallbackGameState , if no custom game state is available. The diagram above shows, that the editor skips the game state in simulate mode . In practice that means that the editor will not allocate any game state when the scene is being edited or only simulated. Only when you enter play-the-game mode , will it create a game state, which can then take over full control for the windows, input and the main camera.","title":"Game States"},{"location":"runtime/application/game-state/#game-state-responsibilities","text":"The typical things that a game state controls are: Spawning one or multiple windows Setting up the main render pipeline Creating a world and loading a scene into it Unless it's run in the editor, where it is handed an existing world Setting up input devices and bindings Processing main input (not component specific input) Setting up and controlling the main camera Spawning the player prefab Displaying game UI Providing a main menu Saving and restoring global state (progression, high-scores, etc) For example when you have a player start point component in a scene, the component itself doesn't do anything, it just holds some data. Instead, when you enter play-the-game mode, the active game state can (but is not required to) use the information from these components to spawn a player prefab. Similarly, most scenes have a camera component whose usage hint is set to 'Main Camera' (this may be part of the player prefab). This camera defines what part of the scene will be shown on screen. At least that's how it appears. In reality it is the game state that controls the camera for the main render target. It's simply a feature of the plFallbackGameState , that it searches the world for an appropriate camera component and applies that to the main camera. If it doesn't find any such camera component, it provides simple WASD fly-camera controls. You can even cycle through the different camera components in a scene using Page Up and Page Down . As you can see, by implementing a custom game state, you can gain control over many things that otherwise appear to be built-in.","title":"Game State Responsibilities"},{"location":"runtime/application/game-state/#game-state-instantiation","text":"It is the responsibility of the plGameApplication to instantiate a game state. By default this is done right at application startup, but if you write your own application (TODO) you could handle this differently. For example the editor only instantiates the game state for play-the-game mode. The application knows what game states are available through the reflection information . When a game state is needed, all available ones are instantiated and 'asked' (via plGameStateBase::DeterminePriority() ) whether it is the right one to handle the situation. The game state that is the best fit will be kept and gets control. The idea is, that there are typically only few game states available anyway. Usually you have the built-in plFallbackGameState and then you have one other game state implementation from your custom game plugin. The latter one will take precedence. You could have multiple game states, for example when you have multiple game plugins loaded simultaneously, but then they would need to somehow figure out which one should get activated (e.g. through command line arguments).","title":"Game State Instantiation"},{"location":"runtime/application/game-state/#see-also","text":"Custom Code Application (TODO)","title":"See Also"},{"location":"runtime/configuration/actor-system/","text":"Actor System The actor system is fully functional but currently undocumented. The actor system is about creating windows and render targets for the right hardware. It's name may sound more interesting, than it actually is :D See Also","title":"Actor System"},{"location":"runtime/configuration/actor-system/#actor-system","text":"The actor system is fully functional but currently undocumented. The actor system is about creating windows and render targets for the right hardware. It's name may sound more interesting, than it actually is :D","title":"Actor System"},{"location":"runtime/configuration/actor-system/#see-also","text":"","title":"See Also"},{"location":"runtime/configuration/interfaces/","text":"Singleton Interfaces Singletons are classes of which there should only be a single instance throughout the lifetime of the process. Although Plasma uses the singleton pattern quite extensively for built-in classes, such as plTaskSystem and plResourceManager , those classes don't use dedicated singleton infrastructure. Instead, they only expose static functions, and there is no need for any instance. Accessing such singletons is trivial, as you can always call their functions directly. However, there is another type of singleton, which does require special handling. There are cases where you want to define an interface to make certain functionality available, but you may have different implementations. Only one implementation should ever be active, though. Concrete examples are the integrations of third party libraries. For example there is an plFrameCaptureInterface . This class defines an interface through which plGameApplicationBase can do a capture of the rendered frame, which can be used for debugging graphics issues. However, how such a frame capture could be taken, depends on the platform, the installed tools, the used graphics API and so on. This functionality may be available or not and the exact implementation that is needed can differ drastically. Therefore, we want to be able to dynamically load the necessary implementation and make it available through the abstract interface. For the plFrameCaptureInterface we have an implementation by our RenderDoc integration . In the future we might have a second implementation for PIX or some other platform specific tool. Using the singleton infrastructure, we can simply load an engine plugin that contains an implementation, and from that plugin register our implementation for that interface. Other code can then query for an instance of this interface and, if available, use it without knowing anything about the implementation, and without the need to link against that library. Implementing Singletons This section shows all the pieces needed for a singleton. Interface Base Class First, you need to have a virtual base class that declares the actual interface. /// \\brief Pure virtual interface for demonstrating the singleton work flow /// /// This declaration would typically be in a shared location, that all code can #include class PrintInterface { public: virtual ~PrintInterface() = default; virtual void Print(const plFormatString& text) = 0; }; This is the class through which other code will later access the functionality, so it must be in a shared location. Interface Implementation Next, you need one or more implementations of your interface. You can, of course, have zero implementations, if all you want to provide is the option for future extensibility, and your code should generally be able to handle the fact that no implementation is currently loaded. /// \\brief Implementation of the PrintInterface, just forwards the text to plLog::Info() /// /// This would typically be in a different plugin than the interface and would be allocated by that plugin on startup. class PrintImplementation : public PrintInterface { PLASMA_DECLARE_SINGLETON_OF_INTERFACE(PrintImplementation, PrintInterface); public: PrintImplementation(); virtual void Print(const plFormatString& text) override; private: // needed for the startup system to be able to call the private function below PLASMA_MAKE_SUBSYSTEM_STARTUP_FRIEND(SampleGamePluginStartupGroup, SampleGamePluginMainStartup); void OnCoreSystemsStartup() { /* we could do something important here */ } }; Note the PLASMA_DECLARE_SINGLETON_OF_INTERFACE macro. This adds one part of the required functionality. For one, this class adds a function to query the one and only instance of your class ( GetSingleton() ). Also, it prevents you from creating two instances of this class, as that would violate the singleton contract. Finally, you need to add this to you cpp file: PLASMA_IMPLEMENT_SINGLETON(PrintImplementation); PrintImplementation::PrintImplementation() : m_SingletonRegistrar(this) // needed for automatic registration { } The macro again inserts vital code for your singleton to work. The constructor also has to follow the pattern shown above. You can now implement the desired behavior for the overridden functions. Instantiating Singletons The Plasma singleton infrastructure does not automatically create an instance of singleton classes. It is up to you whether, when and how you create your instance. The most common way to do this, is to leverage the startup system to hook into the engine startup process at the right time. For details, read that chapter, but here is what you would typically do. At startup you instantiate your singleton implementation: ON_CORESYSTEMS_STARTUP { // allocate an implementation of PrintInterface s_PrintInterface = PLASMA_DEFAULT_NEW(PrintImplementation); s_PrintInterface->OnCoreSystemsStartup(); s_PrintInterface->Print(\"Called ON_CORESYSTEMS_STARTUP\"); } And at shutdown you make sure to clean it up again: ON_CORESYSTEMS_SHUTDOWN { s_PrintInterface->Print(\"Called ON_CORESYSTEMS_SHUTDOWN\"); // clean up the s_PrintInterface, otherwise we would get asserts about memory leaks at shutdown s_PrintInterface.Clear(); } Accessing Singletons There are two ways that you can access your singleton instance. In a piece of code that knows for certain that it will only run in conjunction with a specific singleton implementation, you can access it directly: PrintImplementation::GetSingleton()->Print(\"Called ON_HIGHLEVELSYSTEMS_SHUTDOWN\"); This is the most efficient way. However, use cases for this should be relatively rare. The more common situation is, when you want to get the implementation for an interface. To do so, you need to go through plSingletonRegistry : plSingletonRegistry::GetSingletonInstance<PrintInterface>()->Print(\"Called ON_HIGHLEVELSYSTEMS_STARTUP\"); Here we don't need to know anything about the implementation and therefore have no link dependency on the library that provides it. This is how most code would access a singleton implementation. Be aware that this requires a more expensive lookup, so locally cache the result, if you want to do multiple function calls on it. See Also Startup System Engine Plugins","title":"Singleton Interfaces"},{"location":"runtime/configuration/interfaces/#singleton-interfaces","text":"Singletons are classes of which there should only be a single instance throughout the lifetime of the process. Although Plasma uses the singleton pattern quite extensively for built-in classes, such as plTaskSystem and plResourceManager , those classes don't use dedicated singleton infrastructure. Instead, they only expose static functions, and there is no need for any instance. Accessing such singletons is trivial, as you can always call their functions directly. However, there is another type of singleton, which does require special handling. There are cases where you want to define an interface to make certain functionality available, but you may have different implementations. Only one implementation should ever be active, though. Concrete examples are the integrations of third party libraries. For example there is an plFrameCaptureInterface . This class defines an interface through which plGameApplicationBase can do a capture of the rendered frame, which can be used for debugging graphics issues. However, how such a frame capture could be taken, depends on the platform, the installed tools, the used graphics API and so on. This functionality may be available or not and the exact implementation that is needed can differ drastically. Therefore, we want to be able to dynamically load the necessary implementation and make it available through the abstract interface. For the plFrameCaptureInterface we have an implementation by our RenderDoc integration . In the future we might have a second implementation for PIX or some other platform specific tool. Using the singleton infrastructure, we can simply load an engine plugin that contains an implementation, and from that plugin register our implementation for that interface. Other code can then query for an instance of this interface and, if available, use it without knowing anything about the implementation, and without the need to link against that library.","title":"Singleton Interfaces"},{"location":"runtime/configuration/interfaces/#implementing-singletons","text":"This section shows all the pieces needed for a singleton.","title":"Implementing Singletons"},{"location":"runtime/configuration/interfaces/#interface-base-class","text":"First, you need to have a virtual base class that declares the actual interface. /// \\brief Pure virtual interface for demonstrating the singleton work flow /// /// This declaration would typically be in a shared location, that all code can #include class PrintInterface { public: virtual ~PrintInterface() = default; virtual void Print(const plFormatString& text) = 0; }; This is the class through which other code will later access the functionality, so it must be in a shared location.","title":"Interface Base Class"},{"location":"runtime/configuration/interfaces/#interface-implementation","text":"Next, you need one or more implementations of your interface. You can, of course, have zero implementations, if all you want to provide is the option for future extensibility, and your code should generally be able to handle the fact that no implementation is currently loaded. /// \\brief Implementation of the PrintInterface, just forwards the text to plLog::Info() /// /// This would typically be in a different plugin than the interface and would be allocated by that plugin on startup. class PrintImplementation : public PrintInterface { PLASMA_DECLARE_SINGLETON_OF_INTERFACE(PrintImplementation, PrintInterface); public: PrintImplementation(); virtual void Print(const plFormatString& text) override; private: // needed for the startup system to be able to call the private function below PLASMA_MAKE_SUBSYSTEM_STARTUP_FRIEND(SampleGamePluginStartupGroup, SampleGamePluginMainStartup); void OnCoreSystemsStartup() { /* we could do something important here */ } }; Note the PLASMA_DECLARE_SINGLETON_OF_INTERFACE macro. This adds one part of the required functionality. For one, this class adds a function to query the one and only instance of your class ( GetSingleton() ). Also, it prevents you from creating two instances of this class, as that would violate the singleton contract. Finally, you need to add this to you cpp file: PLASMA_IMPLEMENT_SINGLETON(PrintImplementation); PrintImplementation::PrintImplementation() : m_SingletonRegistrar(this) // needed for automatic registration { } The macro again inserts vital code for your singleton to work. The constructor also has to follow the pattern shown above. You can now implement the desired behavior for the overridden functions.","title":"Interface Implementation"},{"location":"runtime/configuration/interfaces/#instantiating-singletons","text":"The Plasma singleton infrastructure does not automatically create an instance of singleton classes. It is up to you whether, when and how you create your instance. The most common way to do this, is to leverage the startup system to hook into the engine startup process at the right time. For details, read that chapter, but here is what you would typically do. At startup you instantiate your singleton implementation: ON_CORESYSTEMS_STARTUP { // allocate an implementation of PrintInterface s_PrintInterface = PLASMA_DEFAULT_NEW(PrintImplementation); s_PrintInterface->OnCoreSystemsStartup(); s_PrintInterface->Print(\"Called ON_CORESYSTEMS_STARTUP\"); } And at shutdown you make sure to clean it up again: ON_CORESYSTEMS_SHUTDOWN { s_PrintInterface->Print(\"Called ON_CORESYSTEMS_SHUTDOWN\"); // clean up the s_PrintInterface, otherwise we would get asserts about memory leaks at shutdown s_PrintInterface.Clear(); }","title":"Instantiating Singletons"},{"location":"runtime/configuration/interfaces/#accessing-singletons","text":"There are two ways that you can access your singleton instance. In a piece of code that knows for certain that it will only run in conjunction with a specific singleton implementation, you can access it directly: PrintImplementation::GetSingleton()->Print(\"Called ON_HIGHLEVELSYSTEMS_SHUTDOWN\"); This is the most efficient way. However, use cases for this should be relatively rare. The more common situation is, when you want to get the implementation for an interface. To do so, you need to go through plSingletonRegistry : plSingletonRegistry::GetSingletonInstance<PrintInterface>()->Print(\"Called ON_HIGHLEVELSYSTEMS_STARTUP\"); Here we don't need to know anything about the implementation and therefore have no link dependency on the library that provides it. This is how most code would access a singleton implementation. Be aware that this requires a more expensive lookup, so locally cache the result, if you want to do multiple function calls on it.","title":"Accessing Singletons"},{"location":"runtime/configuration/interfaces/#see-also","text":"Startup System Engine Plugins","title":"See Also"},{"location":"runtime/configuration/startup/","text":"Startup System Initializing an engine and shutting it properly down again, is a surprisingly difficult task. There are many steps involved, some of which have hard requirements on their ordering. Also, some functionality can only be initialized when at least a window, and potentially even a graphics API is available, which is not the case for command line tools. Once plugins are added to the mix, which can be loaded and unloaded at any time, it becomes impossible to manually set up this process. Therefore, Plasma uses a dedicated startup system , to handle this complexity automatically for you. Startup System Concept The concept of the startup system is simple. For every 'thing' in the engine you write code how to initialize it and shut it down again. 'Things' in the startup system are referred to as subsystems . You then define what other subsystems you depend on, so that your startup code should run after your dependencies, and your shutdown code should run before your dependencies. All of this is then (automatically) given to the startup system, and when it comes time to fully boot up the engine, that system sorts all subsystems by their dependencies and executes them in the right order. Conversely, it executes all shutdown code in the reverse order. Two Phase Startup A lot of code can be initialized easily in all applications. However, some code strictly requires a window or graphics API to work with and could never be initialized successfully in a command line application. Therefore, the startup system splits the engine initialization into two phases: core systems startup (phase 1) and high level systems startup (phase 2). For command line applications, we would only ever run phase 1. In a proper game, we would first run phase 1, then create our window and rendering API and finally run phase 2. This way, when we don't need things like a renderer or the input system , we simply exclude all high level systems from being initialized. Dependencies Some subsystems depend on other subsystems to be initialized. Therefore the startup system requires you to provide a name for every subsystem and also a group . The name can be arbitrary but has to be unique. The group name obviously does not need to be unique, as multiple subsystems can be part of the same group. When you declare a dependency on another subsystem, you can then either specify it by its direct name, or you can also just declare a dependency on an entire group. The latter is very common, as it is often easier, and you rarely have very strict dependencies on a single subsystem. Startup System Usage In practice, to use the startup system, you need to add a block of code to some cpp file. You can copy this code from Foundation/Configuration/Startup.h and then just fill out the parts that you require. // clang-format off PLASMA_BEGIN_SUBSYSTEM_DECLARATION(SampleGamePluginStartupGroup, SampleGamePluginMainStartup) // list all the subsystems that we want to be initialized first BEGIN_SUBSYSTEM_DEPENDENCIES \"Foundation\", // all subsystems from the 'Foundation' group (this is redundant, because `Core` already depends on `Foundation`) \"Core\" // and all subsystems from the 'Core' group END_SUBSYSTEM_DEPENDENCIES ON_CORESYSTEMS_STARTUP { // allocate an implementation of PrintInterface s_PrintInterface = PLASMA_DEFAULT_NEW(PrintImplementation); s_PrintInterface->OnCoreSystemsStartup(); s_PrintInterface->Print(\"Called ON_CORESYSTEMS_STARTUP\"); } ON_CORESYSTEMS_SHUTDOWN { s_PrintInterface->Print(\"Called ON_CORESYSTEMS_SHUTDOWN\"); // clean up the s_PrintInterface, otherwise we would get asserts about memory leaks at shutdown s_PrintInterface.Clear(); } ON_HIGHLEVELSYSTEMS_STARTUP { // we can query 'an implementation of PrintInterface' through the plSingletonRegistry plSingletonRegistry::GetSingletonInstance<PrintInterface>()->Print(\"Called ON_HIGHLEVELSYSTEMS_STARTUP\"); } ON_HIGHLEVELSYSTEMS_SHUTDOWN { // we could also query 'the one instance of the PrintImplementation singleton' PrintImplementation::GetSingleton()->Print(\"Called ON_HIGHLEVELSYSTEMS_SHUTDOWN\"); } PLASMA_END_SUBSYSTEM_DECLARATION; Here we give our subsystem the name SampleGamePluginMainStartup and we put it into the group SampleGamePluginStartupGroup . Both names could be used by other subsystems to reference this as a dependency. We then specify that this subsystem should be initialized only after all the Foundation and Core subsystems have been booted. Both groups contain many subsystems, but we don't need to care about those details. Now when the application starts running, at some point it will run all the ON_CORESYSTEMS_STARTUP code blocks (in a sorted order). Here, we use that hook to set up our singleton . Later, the game will execute the ON_HIGHLEVELSYSTEMS_STARTUP block, and at shutdown it will first execute ON_HIGHLEVELSYSTEMS_SHUTDOWN and finally ON_CORESYSTEMS_SHUTDOWN shortly before the application closes. Command line applications would not execute the high level startup code blocks. Also, when a plugin is loaded or unloaded, the system ensures to call all the right startup and shutdown functions for subsystems from those plugins. How to know about dependencies A practical problem you may be wondering about, is how you would know the names of potential subsystem dependencies, or how you would even know what subsystems exist. In practice, this is rarely a problem. Most subsystems only depend on the Foundation or the Core group of subsystems. If you have any other dependencies, you are typically quite aware of them, and know where in the code they are set up and thus, where you can look up their names. However, you can also use plInspector to discover all the available subsystems, their names, and see what other subsystems they depend on See Also plInspector Singleton Interfaces Sample Game Plugin","title":"Startup System"},{"location":"runtime/configuration/startup/#startup-system","text":"Initializing an engine and shutting it properly down again, is a surprisingly difficult task. There are many steps involved, some of which have hard requirements on their ordering. Also, some functionality can only be initialized when at least a window, and potentially even a graphics API is available, which is not the case for command line tools. Once plugins are added to the mix, which can be loaded and unloaded at any time, it becomes impossible to manually set up this process. Therefore, Plasma uses a dedicated startup system , to handle this complexity automatically for you.","title":"Startup System"},{"location":"runtime/configuration/startup/#startup-system-concept","text":"The concept of the startup system is simple. For every 'thing' in the engine you write code how to initialize it and shut it down again. 'Things' in the startup system are referred to as subsystems . You then define what other subsystems you depend on, so that your startup code should run after your dependencies, and your shutdown code should run before your dependencies. All of this is then (automatically) given to the startup system, and when it comes time to fully boot up the engine, that system sorts all subsystems by their dependencies and executes them in the right order. Conversely, it executes all shutdown code in the reverse order.","title":"Startup System Concept"},{"location":"runtime/configuration/startup/#two-phase-startup","text":"A lot of code can be initialized easily in all applications. However, some code strictly requires a window or graphics API to work with and could never be initialized successfully in a command line application. Therefore, the startup system splits the engine initialization into two phases: core systems startup (phase 1) and high level systems startup (phase 2). For command line applications, we would only ever run phase 1. In a proper game, we would first run phase 1, then create our window and rendering API and finally run phase 2. This way, when we don't need things like a renderer or the input system , we simply exclude all high level systems from being initialized.","title":"Two Phase Startup"},{"location":"runtime/configuration/startup/#dependencies","text":"Some subsystems depend on other subsystems to be initialized. Therefore the startup system requires you to provide a name for every subsystem and also a group . The name can be arbitrary but has to be unique. The group name obviously does not need to be unique, as multiple subsystems can be part of the same group. When you declare a dependency on another subsystem, you can then either specify it by its direct name, or you can also just declare a dependency on an entire group. The latter is very common, as it is often easier, and you rarely have very strict dependencies on a single subsystem.","title":"Dependencies"},{"location":"runtime/configuration/startup/#startup-system-usage","text":"In practice, to use the startup system, you need to add a block of code to some cpp file. You can copy this code from Foundation/Configuration/Startup.h and then just fill out the parts that you require. // clang-format off PLASMA_BEGIN_SUBSYSTEM_DECLARATION(SampleGamePluginStartupGroup, SampleGamePluginMainStartup) // list all the subsystems that we want to be initialized first BEGIN_SUBSYSTEM_DEPENDENCIES \"Foundation\", // all subsystems from the 'Foundation' group (this is redundant, because `Core` already depends on `Foundation`) \"Core\" // and all subsystems from the 'Core' group END_SUBSYSTEM_DEPENDENCIES ON_CORESYSTEMS_STARTUP { // allocate an implementation of PrintInterface s_PrintInterface = PLASMA_DEFAULT_NEW(PrintImplementation); s_PrintInterface->OnCoreSystemsStartup(); s_PrintInterface->Print(\"Called ON_CORESYSTEMS_STARTUP\"); } ON_CORESYSTEMS_SHUTDOWN { s_PrintInterface->Print(\"Called ON_CORESYSTEMS_SHUTDOWN\"); // clean up the s_PrintInterface, otherwise we would get asserts about memory leaks at shutdown s_PrintInterface.Clear(); } ON_HIGHLEVELSYSTEMS_STARTUP { // we can query 'an implementation of PrintInterface' through the plSingletonRegistry plSingletonRegistry::GetSingletonInstance<PrintInterface>()->Print(\"Called ON_HIGHLEVELSYSTEMS_STARTUP\"); } ON_HIGHLEVELSYSTEMS_SHUTDOWN { // we could also query 'the one instance of the PrintImplementation singleton' PrintImplementation::GetSingleton()->Print(\"Called ON_HIGHLEVELSYSTEMS_SHUTDOWN\"); } PLASMA_END_SUBSYSTEM_DECLARATION; Here we give our subsystem the name SampleGamePluginMainStartup and we put it into the group SampleGamePluginStartupGroup . Both names could be used by other subsystems to reference this as a dependency. We then specify that this subsystem should be initialized only after all the Foundation and Core subsystems have been booted. Both groups contain many subsystems, but we don't need to care about those details. Now when the application starts running, at some point it will run all the ON_CORESYSTEMS_STARTUP code blocks (in a sorted order). Here, we use that hook to set up our singleton . Later, the game will execute the ON_HIGHLEVELSYSTEMS_STARTUP block, and at shutdown it will first execute ON_HIGHLEVELSYSTEMS_SHUTDOWN and finally ON_CORESYSTEMS_SHUTDOWN shortly before the application closes. Command line applications would not execute the high level startup code blocks. Also, when a plugin is loaded or unloaded, the system ensures to call all the right startup and shutdown functions for subsystems from those plugins.","title":"Startup System Usage"},{"location":"runtime/configuration/startup/#how-to-know-about-dependencies","text":"A practical problem you may be wondering about, is how you would know the names of potential subsystem dependencies, or how you would even know what subsystems exist. In practice, this is rarely a problem. Most subsystems only depend on the Foundation or the Core group of subsystems. If you have any other dependencies, you are typically quite aware of them, and know where in the code they are set up and thus, where you can look up their names. However, you can also use plInspector to discover all the available subsystems, their names, and see what other subsystems they depend on","title":"How to know about dependencies"},{"location":"runtime/configuration/startup/#see-also","text":"plInspector Singleton Interfaces Sample Game Plugin","title":"See Also"},{"location":"runtime/world/component-managers/","text":"Component Managers A component manager is a world module whose purpose it is to create, store and update components of a single type. For every component type, there is exactly one component manager to handle that type. Simple Component Managers There are two types of simple component managers: Ones that don't update their components, at all. Ones that call a simple Update() function once per frame on their components. No Update Component Manager A component manager that doesn't update its components is declared like this: using SendMsgComponentManager = plComponentManager<class SendMsgComponent, plBlockStorageType::Compact>; We can simply instantiate the plComponentManager template and not override anything. Therefore this component manager doesn't have any update function and so the component type that it manages is never updated. That doesn't mean that the component type in question can't do things periodically. In fact the SendMsgComponent does update its state regularly, but it triggers its own update through messaging , which is more efficient for components that only need to wake up every once in a while. Simple Update Component Manager Many component types need to be updated every frame, but it is sufficient if the component manager just calls a simple Update() function. Creating a component manager for this scenario looks like this: using DisplayMsgComponentManager = plComponentManagerSimple<class DisplayMsgComponent, plComponentUpdateType::WhenSimulating, plBlockStorageType::FreeList>; That is literally all. The template plComponentManagerSimple will take care of the required update function setup. All you need to do then, is to add a (non-virtual) Update() function to the component type, which the component manager will call for all active components each frame. The plComponentUpdateType option determines whether the component manager will call the Update() function only while the world simulation is running (during a game) or also when it is not running, meaning when editing a scene. For things that should show up even while looking at a paused scene in the editor, you need to use plComponentUpdateType::Always . Non-Simple Component Managers The vast majority of component managers are very simple, but they can also be much more complex. This is mostly the case when the manager needs to synchronize state between components and other systems. Another reason to write a more complex component manager is efficiency. If the manager can track which components need updating and which ones can be ignored, it can skip the update for many components. Or it can update only a number of components each frame to amortize costs. To write a more complex component manager you basically just register your own update functions and then do whatever needs to be done there. See the chapter about world modules for how to do that. Note: When you write your own update function, don't forget to skip inactive components. Otherwise deactivating a component or object hierarchy has no effect on your component type. See plComponentManagerSimple::SimpleUpdate() for an example. Component Storage Both component managers above were configured with a plBlockStorageType option. This determines what happens when a component gets deleted from the world. If the component manager is set to plBlockStorageType::FreeList , the unused memory block will be put into a free-list and reused when a new component is allocated. In the mean time, the component manager needs to skip these unused memory blocks, every time it iterates over all components. For components that have very short lifespans or are frequently created and destroyed, this can be more efficient. The main reason to use this, though, is for components that can't be relocated in memory. If a component would crash when it is copied to a different memory location, then using the free-list option prevents this. If the component manager is set to plBlockStorageType::Compact , then an unused memory block will be filled right away by relocating the last valid component to that freed up slot. This prevents memory fragmentation, which wastes performance when iterating over large arrays of components, of which many elements are unused. For components which are mostly long lived, this option gives better performance. If in doubt, both options are fine. The plComponentManagerSimple defaults to plBlockStorageType::FreeList as this mode has fewer restrictions. See Also World Modules Components","title":"Component Managers"},{"location":"runtime/world/component-managers/#component-managers","text":"A component manager is a world module whose purpose it is to create, store and update components of a single type. For every component type, there is exactly one component manager to handle that type.","title":"Component Managers"},{"location":"runtime/world/component-managers/#simple-component-managers","text":"There are two types of simple component managers: Ones that don't update their components, at all. Ones that call a simple Update() function once per frame on their components.","title":"Simple Component Managers"},{"location":"runtime/world/component-managers/#no-update-component-manager","text":"A component manager that doesn't update its components is declared like this: using SendMsgComponentManager = plComponentManager<class SendMsgComponent, plBlockStorageType::Compact>; We can simply instantiate the plComponentManager template and not override anything. Therefore this component manager doesn't have any update function and so the component type that it manages is never updated. That doesn't mean that the component type in question can't do things periodically. In fact the SendMsgComponent does update its state regularly, but it triggers its own update through messaging , which is more efficient for components that only need to wake up every once in a while.","title":"No Update Component Manager"},{"location":"runtime/world/component-managers/#simple-update-component-manager","text":"Many component types need to be updated every frame, but it is sufficient if the component manager just calls a simple Update() function. Creating a component manager for this scenario looks like this: using DisplayMsgComponentManager = plComponentManagerSimple<class DisplayMsgComponent, plComponentUpdateType::WhenSimulating, plBlockStorageType::FreeList>; That is literally all. The template plComponentManagerSimple will take care of the required update function setup. All you need to do then, is to add a (non-virtual) Update() function to the component type, which the component manager will call for all active components each frame. The plComponentUpdateType option determines whether the component manager will call the Update() function only while the world simulation is running (during a game) or also when it is not running, meaning when editing a scene. For things that should show up even while looking at a paused scene in the editor, you need to use plComponentUpdateType::Always .","title":"Simple Update Component Manager"},{"location":"runtime/world/component-managers/#non-simple-component-managers","text":"The vast majority of component managers are very simple, but they can also be much more complex. This is mostly the case when the manager needs to synchronize state between components and other systems. Another reason to write a more complex component manager is efficiency. If the manager can track which components need updating and which ones can be ignored, it can skip the update for many components. Or it can update only a number of components each frame to amortize costs. To write a more complex component manager you basically just register your own update functions and then do whatever needs to be done there. See the chapter about world modules for how to do that. Note: When you write your own update function, don't forget to skip inactive components. Otherwise deactivating a component or object hierarchy has no effect on your component type. See plComponentManagerSimple::SimpleUpdate() for an example.","title":"Non-Simple Component Managers"},{"location":"runtime/world/component-managers/#component-storage","text":"Both component managers above were configured with a plBlockStorageType option. This determines what happens when a component gets deleted from the world. If the component manager is set to plBlockStorageType::FreeList , the unused memory block will be put into a free-list and reused when a new component is allocated. In the mean time, the component manager needs to skip these unused memory blocks, every time it iterates over all components. For components that have very short lifespans or are frequently created and destroyed, this can be more efficient. The main reason to use this, though, is for components that can't be relocated in memory. If a component would crash when it is copied to a different memory location, then using the free-list option prevents this. If the component manager is set to plBlockStorageType::Compact , then an unused memory block will be filled right away by relocating the last valid component to that freed up slot. This prevents memory fragmentation, which wastes performance when iterating over large arrays of components, of which many elements are unused. For components which are mostly long lived, this option gives better performance. If in doubt, both options are fine. The plComponentManagerSimple defaults to plBlockStorageType::FreeList as this mode has fewer restrictions.","title":"Component Storage"},{"location":"runtime/world/component-managers/#see-also","text":"World Modules Components","title":"See Also"},{"location":"runtime/world/components/","text":"Components For an introduction what a component is and how it fits into the overall picture, see The World / Scenegraph System . This documentation focuses on the C++ plComponent class. The functionality exposed through other mechanisms, such as TypeScript , may be more limited in scope, but ultimately maps to the C++ implementation. Components are the fundamental building blocks with which to make the engine do things. Components act as glue between systems like the renderer and the user. They expose the available functionality to the editor and they control when and how each system is used. This document describes how components work. Owner A component is always attached to a game object . This 'owner' can be queried with plComponent::GetOwner() . There is a brief moment where a component is not attached to an owner, which is when it is being prepared for destruction. If you run into such a situation use plComponent::IsActiveAndInitialized() to filter them out. Component Manager Every component has a component manager . You can access it with plComponent::GetOwningManager() . To get the component manager for a specific component type, you need to query the world . See plWorld::GetOrCreateComponentManager() and plWorld::GetOrCreateManagerForComponentType() . Creating Components The most convenient way to create a component of a specific type is to call the static function CreateComponent() on the component manager for that type. plMeshComponent* pMesh; pWorld->GetOrCreateComponentManager<plMeshComponentManager>()->CreateComponent(pObject, pMesh); Deleting Components To delete a component, just call plComponent::DeleteComponent() on it. Note that deleting individual components is relatively rare, it is more common to just delete the entire object. Also be aware that deleted components are immediately deinitialized. They will still exist in a semi-usable state until the end of the frame, but if other code tries to access the component within the same frame, it may see it in an 'unexpected' state. If necessary, that code can check plComponent::IsActiveAndInitialized() to prevent working with just deleted components. You can also delete a component only through its handle, if you have the corresponding component manager . Component Handles When you need to reference components across frames, you should always store handles to them, never pointers. See the chapter about object lifetime for details. To convert a handle to a (temporary) pointer, use plWorld::TryGetComponent() . Querying Components from Game Objects When you have a game object you can get a list of all attached components with plGameObject::GetComponents() . However, typically you want to get a component of a specific type. Use plGameObject::TryGetComponentOfBaseType() for that: plMeshComponent* pMesh = nullptr; if (pObject->TryGetComponentOfBaseType(pMesh)) { pMesh->DeleteComponent(); } Iterating over all Components You can iterate over all components of one type by calling plComponentManager::GetComponents() . This returns an iterator with which you can efficiently access all components managed by that component manager. Be aware that some components may not be active, so you should skip those. You can also access all components on a game object using plGameObject::GetComponents() . Component Reflection Block All component types must use reflection . Only reflected members show up as properties in the editor. An example block looks like this: PLASMA_BEGIN_COMPONENT_TYPE(DebugRenderComponent, 2, plComponentMode::Static) { PLASMA_BEGIN_PROPERTIES { PLASMA_MEMBER_PROPERTY(\"Size\", m_fSize)->AddAttributes(new plDefaultValueAttribute(1), new plClampValueAttribute(0, 10)), PLASMA_MEMBER_PROPERTY(\"Color\", m_Color)->AddAttributes(new plDefaultValueAttribute(plColor::White)), PLASMA_ACCESSOR_PROPERTY(\"Texture\", GetTextureFile, SetTextureFile)->AddAttributes(new plAssetBrowserAttribute(\"Texture 2D\")), PLASMA_BITFLAGS_MEMBER_PROPERTY(\"Render\", DebugRenderComponentMask, m_RenderTypes)->AddAttributes(new plDefaultValueAttribute(DebugRenderComponentMask::Box)), } PLASMA_END_PROPERTIES; PLASMA_BEGIN_ATTRIBUTES { new plCategoryAttribute(\"SampleGamePlugin\"), // Component menu group } PLASMA_END_ATTRIBUTES; PLASMA_BEGIN_MESSAGEHANDLERS { PLASMA_MESSAGE_HANDLER(plMsgSetColor, OnSetColor) } PLASMA_END_MESSAGEHANDLERS; PLASMA_BEGIN_FUNCTIONS { PLASMA_SCRIPT_FUNCTION_PROPERTY(SetRandomColor) } PLASMA_END_FUNCTIONS; } PLASMA_END_COMPONENT_TYPE The properties section lists all the members that should be editable. Components can have 'virtual' properties, that don't exist as members, but use accessors (functions). Properties can have attributes to configure how they show up in the editor. The attributes section can additionally specify type specific properties. For example, here we tell the editor where in the component menu this component should appear. The message handler section is important to enable messaging . The functions section is used to expose certain member functions to the reflection system, such that script bindings, such as TypeScript can call these functions. At the moment there is no documentation that lists all the available options. It is best to get inspiration by looking at the code for existing components. Component Activation There are three important states for components: Whether they are initialized Whether they are active Whether they are simulating You can hook into changes to these states by overriding plComponent::Initialize() / plComponent::Deinitialize() , plComponent::OnActivated() / plComponent::OnDeactivated() and plComponent::OnSimulationStarted() . The most important function to override is plComponent::OnSimulationStarted() . This is almost always the function where you want to set up your component. It is called when the component is fully initialized, active and the world is actively simulating (the game is running). In the editor, it is only called after you start running a scene , not while you are editing. Since most game code should not do anything while the scene is being edited, you typically don't need to set up anything before this time. Components can be 'active' or 'inactive'. This can be used to switch them on and off at will. The active flag on game objects affects this, but components can also be deactivated individually with plComponent::SetActiveFlag() . When a component is not active, its component manager will typically not update it anymore. If you want to properly support switching components on and off at any time, you often need to be careful to restore state properly. plComponent::OnActivated() and plComponent::OnDeactivated() will be called every time a component's active state changes. Additionally, if the world is being simulated, plComponent::OnSimulationStarted() will also be called after each call to plComponent::OnActivated() . It should be extremely rare that you need to override plComponent::Initialize() or plComponent::Deinitialize() . For all the details on the activation functions, refer to the API Docs . Caution: A common mistake is to override a function like plComponent::OnActivated() but to not call its base class implementation ( SUPER::OnActivated() ). It is good practice to always do so. Forced Activation If for some reason a component must access another component during its own setup, and requires that other component to be set up first, you can enforce this by calling plComponent::EnsureSimulationStarted() on the other component. An example is a physics joint component. To set up a joint, the component needs to access two rigid body components. Both must be already set up themselves, otherwise the joint component cannot link the two. Therefore, when the joint component is being set up, it calls plComponent::EnsureSimulationStarted() on both rigid body components, to make sure it can access valid data. User Flags plComponent::SetUserFlag and plComponent::GetUserFlag can be used to store up to 8 bits of user flags. This should only be used internally, to reduce memory consumption. Dynamic and Static Components In the component reflection block you have to specify whether a component is 'dynamic' or 'static': PLASMA_BEGIN_COMPONENT_TYPE(DemoComponent, 3 /* version */, plComponentMode::Dynamic) This information tells the editor whether this component type attempts to modify the owner's transformation (position, rotation, scale). If any dynamic component is attached to a game object , the entire object will be marked as dynamic and will have its transform updated every frame. If only static components are attached, the game object can be marked as static as well, and costs less performance. See Static vs. Dynamic Objects . Serialization and Versioning When editing a scene or prefab, the editor will serialize components purely based on reflection information. That means only the properties that are marked up through reflection and are therefore visible to the user are serialized. This format is robust to change (and allows for patches), but is not efficient. For the runtime format, that a shipping game should use, scenes are exported . This is a binary serialization format and every component has full control over what data it writes and how it encodes the data. When you run a scene in plPlayer the editor will serialize the scene to the binary format, and the player will deserialize it. If a component doesn't properly serialize all its data, the results can range from misconfigured components to crashes during loading. To implement proper serialization, you need to override plComponent::SerializeComponent() and plComponent::DeserializeComponent() . During serialization you simply write data to a stream, as you like: void DemoComponent::SerializeComponent(plWorldWriter& stream) const { SUPER::SerializeComponent(stream); auto& s = stream.GetStream(); s << m_fAmplitude; s << m_Speed; } Don't forget to call SUPER::SerializeComponent() to include the data of the base class. When you deserialize a component, you need to handle versioning . Every component type has a version number, which is specified in the component reflection block: PLASMA_BEGIN_COMPONENT_TYPE(DemoComponent, 3 /* version */, plComponentMode::Dynamic) The version number should be increased every time the serialization format of the component type has to change. During deserialization you can query the version number with which this component data was written. You than have to handle converting older formats as appropriate: void DemoComponent::DeserializeComponent(plWorldReader& stream) { SUPER::DeserializeComponent(stream); const plUInt32 uiVersion = stream.GetComponentTypeVersion(GetStaticRTTI()); auto& s = stream.GetStream(); s >> m_fAmplitude; if (uiVersion <= 2) { // up to version 2 the angle was stored as a float in degree // convert this to plAngle float fDegree; s >> fDegree; m_Speed = plAngle::Degree(fDegree); } else { s >> m_Speed; } } Custom Components You can extend the engine with custom components: Custom Components with C++ Custom Components with TypeScript See Also Custom Code The World / Scenegraph System Game Objects","title":"Components"},{"location":"runtime/world/components/#components","text":"For an introduction what a component is and how it fits into the overall picture, see The World / Scenegraph System . This documentation focuses on the C++ plComponent class. The functionality exposed through other mechanisms, such as TypeScript , may be more limited in scope, but ultimately maps to the C++ implementation. Components are the fundamental building blocks with which to make the engine do things. Components act as glue between systems like the renderer and the user. They expose the available functionality to the editor and they control when and how each system is used. This document describes how components work.","title":"Components"},{"location":"runtime/world/components/#owner","text":"A component is always attached to a game object . This 'owner' can be queried with plComponent::GetOwner() . There is a brief moment where a component is not attached to an owner, which is when it is being prepared for destruction. If you run into such a situation use plComponent::IsActiveAndInitialized() to filter them out.","title":"Owner"},{"location":"runtime/world/components/#component-manager","text":"Every component has a component manager . You can access it with plComponent::GetOwningManager() . To get the component manager for a specific component type, you need to query the world . See plWorld::GetOrCreateComponentManager() and plWorld::GetOrCreateManagerForComponentType() .","title":"Component Manager"},{"location":"runtime/world/components/#creating-components","text":"The most convenient way to create a component of a specific type is to call the static function CreateComponent() on the component manager for that type. plMeshComponent* pMesh; pWorld->GetOrCreateComponentManager<plMeshComponentManager>()->CreateComponent(pObject, pMesh);","title":"Creating Components"},{"location":"runtime/world/components/#deleting-components","text":"To delete a component, just call plComponent::DeleteComponent() on it. Note that deleting individual components is relatively rare, it is more common to just delete the entire object. Also be aware that deleted components are immediately deinitialized. They will still exist in a semi-usable state until the end of the frame, but if other code tries to access the component within the same frame, it may see it in an 'unexpected' state. If necessary, that code can check plComponent::IsActiveAndInitialized() to prevent working with just deleted components. You can also delete a component only through its handle, if you have the corresponding component manager .","title":"Deleting Components"},{"location":"runtime/world/components/#component-handles","text":"When you need to reference components across frames, you should always store handles to them, never pointers. See the chapter about object lifetime for details. To convert a handle to a (temporary) pointer, use plWorld::TryGetComponent() .","title":"Component Handles"},{"location":"runtime/world/components/#querying-components-from-game-objects","text":"When you have a game object you can get a list of all attached components with plGameObject::GetComponents() . However, typically you want to get a component of a specific type. Use plGameObject::TryGetComponentOfBaseType() for that: plMeshComponent* pMesh = nullptr; if (pObject->TryGetComponentOfBaseType(pMesh)) { pMesh->DeleteComponent(); }","title":"Querying Components from Game Objects"},{"location":"runtime/world/components/#iterating-over-all-components","text":"You can iterate over all components of one type by calling plComponentManager::GetComponents() . This returns an iterator with which you can efficiently access all components managed by that component manager. Be aware that some components may not be active, so you should skip those. You can also access all components on a game object using plGameObject::GetComponents() .","title":"Iterating over all Components"},{"location":"runtime/world/components/#component-reflection-block","text":"All component types must use reflection . Only reflected members show up as properties in the editor. An example block looks like this: PLASMA_BEGIN_COMPONENT_TYPE(DebugRenderComponent, 2, plComponentMode::Static) { PLASMA_BEGIN_PROPERTIES { PLASMA_MEMBER_PROPERTY(\"Size\", m_fSize)->AddAttributes(new plDefaultValueAttribute(1), new plClampValueAttribute(0, 10)), PLASMA_MEMBER_PROPERTY(\"Color\", m_Color)->AddAttributes(new plDefaultValueAttribute(plColor::White)), PLASMA_ACCESSOR_PROPERTY(\"Texture\", GetTextureFile, SetTextureFile)->AddAttributes(new plAssetBrowserAttribute(\"Texture 2D\")), PLASMA_BITFLAGS_MEMBER_PROPERTY(\"Render\", DebugRenderComponentMask, m_RenderTypes)->AddAttributes(new plDefaultValueAttribute(DebugRenderComponentMask::Box)), } PLASMA_END_PROPERTIES; PLASMA_BEGIN_ATTRIBUTES { new plCategoryAttribute(\"SampleGamePlugin\"), // Component menu group } PLASMA_END_ATTRIBUTES; PLASMA_BEGIN_MESSAGEHANDLERS { PLASMA_MESSAGE_HANDLER(plMsgSetColor, OnSetColor) } PLASMA_END_MESSAGEHANDLERS; PLASMA_BEGIN_FUNCTIONS { PLASMA_SCRIPT_FUNCTION_PROPERTY(SetRandomColor) } PLASMA_END_FUNCTIONS; } PLASMA_END_COMPONENT_TYPE The properties section lists all the members that should be editable. Components can have 'virtual' properties, that don't exist as members, but use accessors (functions). Properties can have attributes to configure how they show up in the editor. The attributes section can additionally specify type specific properties. For example, here we tell the editor where in the component menu this component should appear. The message handler section is important to enable messaging . The functions section is used to expose certain member functions to the reflection system, such that script bindings, such as TypeScript can call these functions. At the moment there is no documentation that lists all the available options. It is best to get inspiration by looking at the code for existing components.","title":"Component Reflection Block"},{"location":"runtime/world/components/#component-activation","text":"There are three important states for components: Whether they are initialized Whether they are active Whether they are simulating You can hook into changes to these states by overriding plComponent::Initialize() / plComponent::Deinitialize() , plComponent::OnActivated() / plComponent::OnDeactivated() and plComponent::OnSimulationStarted() . The most important function to override is plComponent::OnSimulationStarted() . This is almost always the function where you want to set up your component. It is called when the component is fully initialized, active and the world is actively simulating (the game is running). In the editor, it is only called after you start running a scene , not while you are editing. Since most game code should not do anything while the scene is being edited, you typically don't need to set up anything before this time. Components can be 'active' or 'inactive'. This can be used to switch them on and off at will. The active flag on game objects affects this, but components can also be deactivated individually with plComponent::SetActiveFlag() . When a component is not active, its component manager will typically not update it anymore. If you want to properly support switching components on and off at any time, you often need to be careful to restore state properly. plComponent::OnActivated() and plComponent::OnDeactivated() will be called every time a component's active state changes. Additionally, if the world is being simulated, plComponent::OnSimulationStarted() will also be called after each call to plComponent::OnActivated() . It should be extremely rare that you need to override plComponent::Initialize() or plComponent::Deinitialize() . For all the details on the activation functions, refer to the API Docs . Caution: A common mistake is to override a function like plComponent::OnActivated() but to not call its base class implementation ( SUPER::OnActivated() ). It is good practice to always do so.","title":"Component Activation"},{"location":"runtime/world/components/#forced-activation","text":"If for some reason a component must access another component during its own setup, and requires that other component to be set up first, you can enforce this by calling plComponent::EnsureSimulationStarted() on the other component. An example is a physics joint component. To set up a joint, the component needs to access two rigid body components. Both must be already set up themselves, otherwise the joint component cannot link the two. Therefore, when the joint component is being set up, it calls plComponent::EnsureSimulationStarted() on both rigid body components, to make sure it can access valid data.","title":"Forced Activation"},{"location":"runtime/world/components/#user-flags","text":"plComponent::SetUserFlag and plComponent::GetUserFlag can be used to store up to 8 bits of user flags. This should only be used internally, to reduce memory consumption.","title":"User Flags"},{"location":"runtime/world/components/#dynamic-and-static-components","text":"In the component reflection block you have to specify whether a component is 'dynamic' or 'static': PLASMA_BEGIN_COMPONENT_TYPE(DemoComponent, 3 /* version */, plComponentMode::Dynamic) This information tells the editor whether this component type attempts to modify the owner's transformation (position, rotation, scale). If any dynamic component is attached to a game object , the entire object will be marked as dynamic and will have its transform updated every frame. If only static components are attached, the game object can be marked as static as well, and costs less performance. See Static vs. Dynamic Objects .","title":"Dynamic and Static Components"},{"location":"runtime/world/components/#serialization-and-versioning","text":"When editing a scene or prefab, the editor will serialize components purely based on reflection information. That means only the properties that are marked up through reflection and are therefore visible to the user are serialized. This format is robust to change (and allows for patches), but is not efficient. For the runtime format, that a shipping game should use, scenes are exported . This is a binary serialization format and every component has full control over what data it writes and how it encodes the data. When you run a scene in plPlayer the editor will serialize the scene to the binary format, and the player will deserialize it. If a component doesn't properly serialize all its data, the results can range from misconfigured components to crashes during loading. To implement proper serialization, you need to override plComponent::SerializeComponent() and plComponent::DeserializeComponent() . During serialization you simply write data to a stream, as you like: void DemoComponent::SerializeComponent(plWorldWriter& stream) const { SUPER::SerializeComponent(stream); auto& s = stream.GetStream(); s << m_fAmplitude; s << m_Speed; } Don't forget to call SUPER::SerializeComponent() to include the data of the base class. When you deserialize a component, you need to handle versioning . Every component type has a version number, which is specified in the component reflection block: PLASMA_BEGIN_COMPONENT_TYPE(DemoComponent, 3 /* version */, plComponentMode::Dynamic) The version number should be increased every time the serialization format of the component type has to change. During deserialization you can query the version number with which this component data was written. You than have to handle converting older formats as appropriate: void DemoComponent::DeserializeComponent(plWorldReader& stream) { SUPER::DeserializeComponent(stream); const plUInt32 uiVersion = stream.GetComponentTypeVersion(GetStaticRTTI()); auto& s = stream.GetStream(); s >> m_fAmplitude; if (uiVersion <= 2) { // up to version 2 the angle was stored as a float in degree // convert this to plAngle float fDegree; s >> fDegree; m_Speed = plAngle::Degree(fDegree); } else { s >> m_Speed; } }","title":"Serialization and Versioning"},{"location":"runtime/world/components/#custom-components","text":"You can extend the engine with custom components: Custom Components with C++ Custom Components with TypeScript","title":"Custom Components"},{"location":"runtime/world/components/#see-also","text":"Custom Code The World / Scenegraph System Game Objects","title":"See Also"},{"location":"runtime/world/game-objects/","text":"Game Objects For an introduction what a game object is and how it fits into the overall picture, see The World / Scenegraph System . This documentation focuses on the C++ plGameObject class. The functionality exposed through other mechanisms, such as TypeScript , may be more limited in scope, but ultimately maps to the C++ implementation. Creating Game Objects You create new game objects by calling plWorld::CreateObject() . This function takes a plGameObjectDesc . This is used to initialize the game object, all options can be changed later on, however things like changing the 'dynamic' state can be expensive to switch. Deleting Game Objects Use plWorld::DeleteObjectDelayed() to remove an object from the world. This will also delete all child nodes and attached components. This function puts the object into a deletion queue and only deallocates it at the end of the frame, so code that still tries to access the object in the same frame will not be affected. The chapter about object lifetime explains this in more detail. Object Transforms Game objects have a position, rotation and scaling. These values are separated into local and global . The local transform represents the offset to the parent . If a game object has no parent, the local and global transform are identical. You can query or modify either, though in the editor the property grid only allows you to set the local transform. The global transform is computed from the local transform and the global transform of the parent (recursively). For dynamic objects (see below) the global transform is recomputed regularly. Static objects will not be updated after their initial placement. Static vs. Dynamic Objects Static game objects are objects that are considered to never move. Dynamic objects, however, can move around the scene arbitrarily. Internally the engine separates these two types of objects into different memory regions. The object transform for dynamic objects is updated every frame . That means from a performance perspective it makes no difference whether a dynamic object was moved in a frame or not. The transforms for static objects, however, are only updated when it is needed (after creation). If you try to move a static object, you will see warnings in the log in development builds, and the object will not move. When you build a scene in the editor, you generally don't need to worry about this. Each component type is flagged to either be dynamic (meaning it may modify its owner's position) or static. From the attached components, the editor will automatically detect whether a game object must be created as static or dynamic. However, in some cases you may know that an object will end up having dynamic components later. So to prevent a costly switch from static to dynamic, you can force a game object to be dynamic, by selecting this mode from the properties. The renderer also caches rendering state for static objects. This does not mean that a static object cannot change rendering state dynamically, but when it does, the code must ensure to properly invalidate the render caches. If switching from static to dynamic fixes a rendering issue, some attached component doesn't handle this cache invalidation correctly. Performance Considerations For performance reasons it is always best to keep all objects static that don't need to be moved. Due to the render data caching, this can save even more CPU time. However, the code paths to update dynamic objects are still quite heavily optimized. A current CPU can easily handle updating 100.000+ dynamic objects at interactive framerates. Active Flag Game objects have an 'active flag'. By default all objects are marked active. If the active flag is removed, all components on that object get deactivated. That means they will not get updated further and in general the object is treated as if it is not part of the world anymore. The active flag propagates down to child objects. Using plGameObject::GetActiveFlag() you can check the state of the active flag on a given game object. However, even if the flag is set, the game object can still be deactivated, if any one of its parents has been deactivated. You can check this with plGameObject::IsActive() . Lifetime and Referencing Game Objects When deleting a game object, it typically stays alive till the end of the frame, to make writing robust code easier. You should, however, never store pointers to game objects across frames, as objects can be relocated in memory. Instead, always use handles ( plGameObjectHandle ) to store references to game objects. The chapter about object lifetime explains this in more detail. Components can also reference objects from their properties. These references are also based on handles. Tags Game objects can have tags . These are used to control things like whether the object will cast shadows. However, they are mostly at your disposal to tag objects with game play relevant information. Iterating over Game Objects You can iterate over all objects in a world using plWorld::GetObjects() . This will return the objects in an arbitrary order, but is the more efficient way. You can also traverse the object hierarchy with plWorld::Traverse() . This allows you to list the objects either in a depth-first or a breadth-first order. When you have a specific game object, you can also iterate over its children with plGameObject::GetChildren() . Finding Objects There are multiple ways to find specific objects, or objects relative to some parent node. Global Keys You can assign a game object a global key . This is a string that should be unique across all objects within the world. That includes all game objects from all prefab instances, so you must be very careful with this. If the same global key is used twice, one of them will be ignored. You can query for a game object by global key using plWorld::TryGetObjectWithGlobalKey() . Global keys can be useful to find unique objects, like the one player object (in a single player game), or level specific items. Finding Child Objects Within an object hierarchy, you can use the name of objects to search for certain child nodes. These functions are available: plGameObject::FindChildByName() plGameObject::FindChildByPath() plGameObject::SearchForChildByNameSequence() plGameObject::SearchForChildrenByNameSequence() Coordinate System The coordinate system of the world is configurable. To make it easier to not hard code assumptions about which axis represents what direction, the game objects provide functions to query the local axis: plGameObject::GetGlobalDirForwards() plGameObject::GetGlobalDirRight() plGameObject::GetGlobalDirUp() These functions return the respective directions in global space considering the worlds coordinate system and the objects own global rotation. Messaging You can send messages to all components attached to an object, or the entire hierarchy below an object. You can also send event messages , which will 'bubble up' the hierarchy until they find a component to handle it. See the chapter about messaging for details. Team ID All game objects store a 16 bit team ID . This value can be used to identify which team or faction an object belongs to. The team ID has no functionality by itself, you can use it or ignore it. The one feature that the team ID has, is that it is automatically propagated for you when components create objects or instantiate prefabs . This way, when a player with team ID 3 shoots, the bullet prefab that gets instantiated by the spawn component will automatically be assigned team ID 3 as well. Thus when that bullet hits another player, your code can easily attribute a kill to a team, or filter out friendly fire. Although it would be possible to implement something similar entirely with custom components, only by having this in the basic game object, is it possible to trace this information even through built in components, meaning you don't need to reimplement basic functionality like the spawn component or the projectile component . See Also The World / Scenegraph System Components Object Lifetime Messaging","title":"Game Objects"},{"location":"runtime/world/game-objects/#game-objects","text":"For an introduction what a game object is and how it fits into the overall picture, see The World / Scenegraph System . This documentation focuses on the C++ plGameObject class. The functionality exposed through other mechanisms, such as TypeScript , may be more limited in scope, but ultimately maps to the C++ implementation.","title":"Game Objects"},{"location":"runtime/world/game-objects/#creating-game-objects","text":"You create new game objects by calling plWorld::CreateObject() . This function takes a plGameObjectDesc . This is used to initialize the game object, all options can be changed later on, however things like changing the 'dynamic' state can be expensive to switch.","title":"Creating Game Objects"},{"location":"runtime/world/game-objects/#deleting-game-objects","text":"Use plWorld::DeleteObjectDelayed() to remove an object from the world. This will also delete all child nodes and attached components. This function puts the object into a deletion queue and only deallocates it at the end of the frame, so code that still tries to access the object in the same frame will not be affected. The chapter about object lifetime explains this in more detail.","title":"Deleting Game Objects"},{"location":"runtime/world/game-objects/#object-transforms","text":"Game objects have a position, rotation and scaling. These values are separated into local and global . The local transform represents the offset to the parent . If a game object has no parent, the local and global transform are identical. You can query or modify either, though in the editor the property grid only allows you to set the local transform. The global transform is computed from the local transform and the global transform of the parent (recursively). For dynamic objects (see below) the global transform is recomputed regularly. Static objects will not be updated after their initial placement.","title":"Object Transforms"},{"location":"runtime/world/game-objects/#static-vs-dynamic-objects","text":"Static game objects are objects that are considered to never move. Dynamic objects, however, can move around the scene arbitrarily. Internally the engine separates these two types of objects into different memory regions. The object transform for dynamic objects is updated every frame . That means from a performance perspective it makes no difference whether a dynamic object was moved in a frame or not. The transforms for static objects, however, are only updated when it is needed (after creation). If you try to move a static object, you will see warnings in the log in development builds, and the object will not move. When you build a scene in the editor, you generally don't need to worry about this. Each component type is flagged to either be dynamic (meaning it may modify its owner's position) or static. From the attached components, the editor will automatically detect whether a game object must be created as static or dynamic. However, in some cases you may know that an object will end up having dynamic components later. So to prevent a costly switch from static to dynamic, you can force a game object to be dynamic, by selecting this mode from the properties. The renderer also caches rendering state for static objects. This does not mean that a static object cannot change rendering state dynamically, but when it does, the code must ensure to properly invalidate the render caches. If switching from static to dynamic fixes a rendering issue, some attached component doesn't handle this cache invalidation correctly.","title":"Static vs. Dynamic Objects"},{"location":"runtime/world/game-objects/#performance-considerations","text":"For performance reasons it is always best to keep all objects static that don't need to be moved. Due to the render data caching, this can save even more CPU time. However, the code paths to update dynamic objects are still quite heavily optimized. A current CPU can easily handle updating 100.000+ dynamic objects at interactive framerates.","title":"Performance Considerations"},{"location":"runtime/world/game-objects/#active-flag","text":"Game objects have an 'active flag'. By default all objects are marked active. If the active flag is removed, all components on that object get deactivated. That means they will not get updated further and in general the object is treated as if it is not part of the world anymore. The active flag propagates down to child objects. Using plGameObject::GetActiveFlag() you can check the state of the active flag on a given game object. However, even if the flag is set, the game object can still be deactivated, if any one of its parents has been deactivated. You can check this with plGameObject::IsActive() .","title":"Active Flag"},{"location":"runtime/world/game-objects/#lifetime-and-referencing-game-objects","text":"When deleting a game object, it typically stays alive till the end of the frame, to make writing robust code easier. You should, however, never store pointers to game objects across frames, as objects can be relocated in memory. Instead, always use handles ( plGameObjectHandle ) to store references to game objects. The chapter about object lifetime explains this in more detail. Components can also reference objects from their properties. These references are also based on handles.","title":"Lifetime and Referencing Game Objects"},{"location":"runtime/world/game-objects/#tags","text":"Game objects can have tags . These are used to control things like whether the object will cast shadows. However, they are mostly at your disposal to tag objects with game play relevant information.","title":"Tags"},{"location":"runtime/world/game-objects/#iterating-over-game-objects","text":"You can iterate over all objects in a world using plWorld::GetObjects() . This will return the objects in an arbitrary order, but is the more efficient way. You can also traverse the object hierarchy with plWorld::Traverse() . This allows you to list the objects either in a depth-first or a breadth-first order. When you have a specific game object, you can also iterate over its children with plGameObject::GetChildren() .","title":"Iterating over Game Objects"},{"location":"runtime/world/game-objects/#finding-objects","text":"There are multiple ways to find specific objects, or objects relative to some parent node.","title":"Finding Objects"},{"location":"runtime/world/game-objects/#global-keys","text":"You can assign a game object a global key . This is a string that should be unique across all objects within the world. That includes all game objects from all prefab instances, so you must be very careful with this. If the same global key is used twice, one of them will be ignored. You can query for a game object by global key using plWorld::TryGetObjectWithGlobalKey() . Global keys can be useful to find unique objects, like the one player object (in a single player game), or level specific items.","title":"Global Keys"},{"location":"runtime/world/game-objects/#finding-child-objects","text":"Within an object hierarchy, you can use the name of objects to search for certain child nodes. These functions are available: plGameObject::FindChildByName() plGameObject::FindChildByPath() plGameObject::SearchForChildByNameSequence() plGameObject::SearchForChildrenByNameSequence()","title":"Finding Child Objects"},{"location":"runtime/world/game-objects/#coordinate-system","text":"The coordinate system of the world is configurable. To make it easier to not hard code assumptions about which axis represents what direction, the game objects provide functions to query the local axis: plGameObject::GetGlobalDirForwards() plGameObject::GetGlobalDirRight() plGameObject::GetGlobalDirUp() These functions return the respective directions in global space considering the worlds coordinate system and the objects own global rotation.","title":"Coordinate System"},{"location":"runtime/world/game-objects/#messaging","text":"You can send messages to all components attached to an object, or the entire hierarchy below an object. You can also send event messages , which will 'bubble up' the hierarchy until they find a component to handle it. See the chapter about messaging for details.","title":"Messaging"},{"location":"runtime/world/game-objects/#team-id","text":"All game objects store a 16 bit team ID . This value can be used to identify which team or faction an object belongs to. The team ID has no functionality by itself, you can use it or ignore it. The one feature that the team ID has, is that it is automatically propagated for you when components create objects or instantiate prefabs . This way, when a player with team ID 3 shoots, the bullet prefab that gets instantiated by the spawn component will automatically be assigned team ID 3 as well. Thus when that bullet hits another player, your code can easily attribute a kill to a team, or filter out friendly fire. Although it would be possible to implement something similar entirely with custom components, only by having this in the basic game object, is it possible to trace this information even through built in components, meaning you don't need to reimplement basic functionality like the spawn component or the projectile component .","title":"Team ID"},{"location":"runtime/world/game-objects/#see-also","text":"The World / Scenegraph System Components Object Lifetime Messaging","title":"See Also"},{"location":"runtime/world/object-lifetime/","text":"Object Lifetime The lifetime of game objects and components is tightly controlled by the world that they belong to. Neither are objects reference counted, nor garbage collected. You have full control over the destruction of objects, but by default 'deleted' objects are not destroyed before the end of the frame, to make writing robust code easy. The lifetime of objects is directly linked to the object hierarchy. If a game object gets deleted, that also deletes all child nodes and all attached components. Referencing Objects In C++ you can of course always hold pointers to anything. Within a single frame, it is fine to reference game objects and components by pointers. However, once the next frame starts, you have to assume that those pointers are invalid. Not only can objects be deleted, but even live objects can be moved around in memory. This 'compacting' is an optimization and can happen to any object between frames. Therefore, instead of keeping pointers to objects, you should always use handles . Specifically plGameObjectHandle for plGameObject references, and plComponentHandle for plComponent (and derived) types. Handles act like weak pointers . Once you have a handle to an object, you can keep it around forever. When you need to access the actual object, you call plWorld::TryGetObject() or plWorld::TryGetComponent() . If the object is still alive at that time, you get back a pointer. That pointer is guaranteed to stay valid until the end of the frame, so you don't need to call the TryGet... function again. As a rule of thumb, you should never have plGameObject* or plComponent* types as class members. Pointers to these types should be limited to local function variables. Deleting Game Objects To delete a game object, call plWorld::DeleteObjectDelayed() . This will put the object into a deletion queue, and will remove the object at the end of the frame. This guarantees that all code that tries to access the object within this frame will work correctly. You can also call plWorld::DeleteObjectNow() . This will indeed delete the object right at that instant. The only situation where it is ok to call this, is in tools where you modify a world in a single threaded way and you know that no other code can ever access objects. Here, having an object not destroyed immediately may be undesirable. Deleting Components To delete a component, get its component manager and call DeleteComponent() on it. The component won't be deallocated right away, that is deferred till the end of the frame. However, it will be deactivated and deinitialized immediately. Therefore, if other code tries to access such a component, it will get valid memory, but it may see a deinitialized component. Such a situation can be detected by calling plComponent::IsActiveAndInitialized() on the target. If you delete individual components during a frame (and not entire objects), code that accesses those components should be prepared to deal with deinitialized components. See Also Game Objects Components","title":"Object Lifetime"},{"location":"runtime/world/object-lifetime/#object-lifetime","text":"The lifetime of game objects and components is tightly controlled by the world that they belong to. Neither are objects reference counted, nor garbage collected. You have full control over the destruction of objects, but by default 'deleted' objects are not destroyed before the end of the frame, to make writing robust code easy. The lifetime of objects is directly linked to the object hierarchy. If a game object gets deleted, that also deletes all child nodes and all attached components.","title":"Object Lifetime"},{"location":"runtime/world/object-lifetime/#referencing-objects","text":"In C++ you can of course always hold pointers to anything. Within a single frame, it is fine to reference game objects and components by pointers. However, once the next frame starts, you have to assume that those pointers are invalid. Not only can objects be deleted, but even live objects can be moved around in memory. This 'compacting' is an optimization and can happen to any object between frames. Therefore, instead of keeping pointers to objects, you should always use handles . Specifically plGameObjectHandle for plGameObject references, and plComponentHandle for plComponent (and derived) types. Handles act like weak pointers . Once you have a handle to an object, you can keep it around forever. When you need to access the actual object, you call plWorld::TryGetObject() or plWorld::TryGetComponent() . If the object is still alive at that time, you get back a pointer. That pointer is guaranteed to stay valid until the end of the frame, so you don't need to call the TryGet... function again. As a rule of thumb, you should never have plGameObject* or plComponent* types as class members. Pointers to these types should be limited to local function variables.","title":"Referencing Objects"},{"location":"runtime/world/object-lifetime/#deleting-game-objects","text":"To delete a game object, call plWorld::DeleteObjectDelayed() . This will put the object into a deletion queue, and will remove the object at the end of the frame. This guarantees that all code that tries to access the object within this frame will work correctly. You can also call plWorld::DeleteObjectNow() . This will indeed delete the object right at that instant. The only situation where it is ok to call this, is in tools where you modify a world in a single threaded way and you know that no other code can ever access objects. Here, having an object not destroyed immediately may be undesirable.","title":"Deleting Game Objects"},{"location":"runtime/world/object-lifetime/#deleting-components","text":"To delete a component, get its component manager and call DeleteComponent() on it. The component won't be deallocated right away, that is deferred till the end of the frame. However, it will be deactivated and deinitialized immediately. Therefore, if other code tries to access such a component, it will get valid memory, but it may see a deinitialized component. Such a situation can be detected by calling plComponent::IsActiveAndInitialized() on the target. If you delete individual components during a frame (and not entire objects), code that accesses those components should be prepared to deal with deinitialized components.","title":"Deleting Components"},{"location":"runtime/world/object-lifetime/#see-also","text":"Game Objects Components","title":"See Also"},{"location":"runtime/world/spatial-system/","text":"Spatial System Every world has a spatial system . Spatial systems are responsible for sorting game objects by their position and size. They are utilized to efficiently find all objects within a volume, such as a box, a sphere or a view frustum. This is mainly used by the renderer to do frustum culling, but is also available to all other code. Obviously the spatial system needs to keep track of moving objects and update its index accordingly. Spatial System Setup plSpatialSystem is the base class for all spatial systems. During the construction of an plWorld , a custom implementation can be provided through the plWorldDesc . By default plSpatialSystem_RegularGrid is used, which is optimized to handle arbitrary situations with good performance. Implementing a custom spatial system can make sense when you have a highly specialized use case. For example, if you have a strictly tile-based 2D game, where you know that all sprites are below a fixed size, and you always have a dense grid without holes, you can write a spatial system that takes advantage of this knowledge and therefore outperforms the default implementation. However, unless you determine that the spatial system is a clear performance bottleneck, and you have domain specific knowledge that could be a big advantage to speed things up, there is no reason to consider writing your own. Since there is exactly one spatial system per world, it usually means that the choice of a system is made for a type of game. In theory, though, one could use different systems for different types of levels, as well. Accessing the Spatial System In C++ code you get access to the world's spatial system through plWorld::GetSpatialSystem() . When using other languages bindings the spatial system may not be exposed directly. For example, when using TypeScript , the most useful functions are exposed directly through pl.World , for example pl.World.FindObjectsInBox() and pl.World.FindObjectsInSphere() . Spatial Data Categories Every piece of spatial data is associated with a category . For example, rendering data is either in the category \"RenderStatic\" or \"RenderDynamic\". This is mainly used to separate spatial information into distinct groups, so that during a spatial query, data that is irrelevant can be filtered out quickly. For efficiency reasons, categories are represented with bitmasks internally, which is why there can only be up to 32 categories. You should assume that the core engine uses at least 5 categories already. Configuring Spatial Data Categories In C++ code you register a spatial data category through plSpatialData::RegisterCategory() . This will return a category object which can be used for spatial queries later: plSpatialData::Category RtsSelectableComponent::s_SelectableCategory = plSpatialData::RegisterCategory(\"Selectable\", plSpatialData::Flags::None); When using the editor, there are components, such as the marker component , which allow you to select a category from a predefined list. This list is project specific. When you click on such a dropdown box, the last entry allows you to open an editor to configure the available categories The Invalid Category Some components 'add' their bounds to a plMsgUpdateLocalBounds using plInvalidSpatialDataCategory . This means that they want to specify their bounds, but do not want to add anything to the spatial system. This is useful for components that do have a perceived size, such as physics shapes, which should be visible when selecting these objects in the editor, but where there is no benefit of inserting this into the spatial system. Exposing Game Objects to the Spatial System The spatial system only knows about game objects , it does not differentiate by components. However, which game objects are inserted into it and under which categories, is handled by components. The world sends the message plMsgUpdateLocalBounds to all components when it determines that an update is necessary. This can also be triggered manually by calling plGameObject::UpdateLocalBounds() when spatial data, such as which category to use, has been modified. Components can handle this message and add spatial information to it. For 3D objects one would use something like the bounding sphere of a mesh, but it is also possible to use more abstract spatial data. void RtsSelectableComponent::OnUpdateLocalBounds(plMsgUpdateLocalBounds& msg) { plBoundingBoxSphere bounds; bounds.m_fSphereRadius = m_fSelectionRadius; bounds.m_vCenter.SetZero(); bounds.m_vBoxHalfExtends.Set(m_fSelectionRadius); msg.AddBounds(bounds, s_SelectableCategory); } Don't forget to register the message handler in the reflection block : PLASMA_BEGIN_MESSAGEHANDLERS { PLASMA_MESSAGE_HANDLER(plMsgUpdateLocalBounds, OnUpdateLocalBounds) } PLASMA_END_MESSAGEHANDLERS; Querying the Spatial System Once you have spatial data inserted into the system, you can use it to efficiently query for objects within a volume. When calling functions such as plSpatialSystem::FindObjectsInSphere() you have to provide a bitmask of categories. That's because you can request to get objects from multiple categories at the same time. You can get this bitmask by calling plSpatialData::Category::GetBitmask() on a category object. void RtsGameState::InspectObjectsInArea(const plVec2& position, float radius, plSpatialSystem::QueryCallback callback) const { plBoundingSphere sphere(position.GetAsVec3(0), radius); plSpatialSystem::QueryParams queryParams; queryParams.m_uiCategoryBitmask = RtsSelectableComponent::s_SelectableCategory.GetBitmask(); m_pMainWorld->GetSpatialSystem()->FindObjectsInSphere(sphere, queryParams, callback); } In other language bindings you may instead need to pass in a list of all the desired categories by name. Spatial System vs. Physics Engines Both the spatial system, as well as physics engines allow you to do spatial queries. There are cases where a problem can be solved using either system, but generally they are meant to complement each other. If you want to query for things that already need to have a physical representation, and therefore will be handled by the physics engine anyway, it is best to leverage the physics engine to query for such objects. For example a shockwave effect that is supposed to push objects away, only makes sense to be applied to physically simulated objects. Therefore, querying which objects are close-by, to figure out what objects to apply the effect to, should be done through the physics engine, and there is no reason to even have information about these objects in the spatial system. On the other hand, things like the RtsSelectableComponent (see above) could be achieved by setting up fake physics actors so that they can be found with physics queries. The performance cost for doing so would be unnecessary high though, as the physics engine would perform additional maintenance that is ultimately not needed, and it may waste precious resources such as collision layers . Here, using the spatial system makes much more sense. Note: If you require doing raycasts or queries against meshes , you will need to use the physics engine, as the spatial system only works with very basic shapes. Spatial System vs. Tags The spatial data categories are very similar to tags . The difference is, that tags are set up on game objects and they don't have any spatial quality. A game object can have many tags, but not be registered spatially and therefore cannot be found through spatial queries. On the other hand, because of this, tags have nearly no performance overhead, whereas spatial data must be updated whenever an object moves. Ultimately, both systems can be used to solve many of the same problems. When you need to be able to inspect an area and find all objects of a certain kind, you should use spatial data, for example through a marker component . If, however, you need to semtantically label objects, but do not require to find them spatially, prefer tags to not waste performance. See Also Marker Component The World / Scenegraph System","title":"Spatial System"},{"location":"runtime/world/spatial-system/#spatial-system","text":"Every world has a spatial system . Spatial systems are responsible for sorting game objects by their position and size. They are utilized to efficiently find all objects within a volume, such as a box, a sphere or a view frustum. This is mainly used by the renderer to do frustum culling, but is also available to all other code. Obviously the spatial system needs to keep track of moving objects and update its index accordingly.","title":"Spatial System"},{"location":"runtime/world/spatial-system/#spatial-system-setup","text":"plSpatialSystem is the base class for all spatial systems. During the construction of an plWorld , a custom implementation can be provided through the plWorldDesc . By default plSpatialSystem_RegularGrid is used, which is optimized to handle arbitrary situations with good performance. Implementing a custom spatial system can make sense when you have a highly specialized use case. For example, if you have a strictly tile-based 2D game, where you know that all sprites are below a fixed size, and you always have a dense grid without holes, you can write a spatial system that takes advantage of this knowledge and therefore outperforms the default implementation. However, unless you determine that the spatial system is a clear performance bottleneck, and you have domain specific knowledge that could be a big advantage to speed things up, there is no reason to consider writing your own. Since there is exactly one spatial system per world, it usually means that the choice of a system is made for a type of game. In theory, though, one could use different systems for different types of levels, as well.","title":"Spatial System Setup"},{"location":"runtime/world/spatial-system/#accessing-the-spatial-system","text":"In C++ code you get access to the world's spatial system through plWorld::GetSpatialSystem() . When using other languages bindings the spatial system may not be exposed directly. For example, when using TypeScript , the most useful functions are exposed directly through pl.World , for example pl.World.FindObjectsInBox() and pl.World.FindObjectsInSphere() .","title":"Accessing the Spatial System"},{"location":"runtime/world/spatial-system/#spatial-data-categories","text":"Every piece of spatial data is associated with a category . For example, rendering data is either in the category \"RenderStatic\" or \"RenderDynamic\". This is mainly used to separate spatial information into distinct groups, so that during a spatial query, data that is irrelevant can be filtered out quickly. For efficiency reasons, categories are represented with bitmasks internally, which is why there can only be up to 32 categories. You should assume that the core engine uses at least 5 categories already.","title":"Spatial Data Categories"},{"location":"runtime/world/spatial-system/#configuring-spatial-data-categories","text":"In C++ code you register a spatial data category through plSpatialData::RegisterCategory() . This will return a category object which can be used for spatial queries later: plSpatialData::Category RtsSelectableComponent::s_SelectableCategory = plSpatialData::RegisterCategory(\"Selectable\", plSpatialData::Flags::None); When using the editor, there are components, such as the marker component , which allow you to select a category from a predefined list. This list is project specific. When you click on such a dropdown box, the last entry allows you to open an editor to configure the available categories","title":"Configuring Spatial Data Categories"},{"location":"runtime/world/spatial-system/#the-invalid-category","text":"Some components 'add' their bounds to a plMsgUpdateLocalBounds using plInvalidSpatialDataCategory . This means that they want to specify their bounds, but do not want to add anything to the spatial system. This is useful for components that do have a perceived size, such as physics shapes, which should be visible when selecting these objects in the editor, but where there is no benefit of inserting this into the spatial system.","title":"The Invalid Category"},{"location":"runtime/world/spatial-system/#exposing-game-objects-to-the-spatial-system","text":"The spatial system only knows about game objects , it does not differentiate by components. However, which game objects are inserted into it and under which categories, is handled by components. The world sends the message plMsgUpdateLocalBounds to all components when it determines that an update is necessary. This can also be triggered manually by calling plGameObject::UpdateLocalBounds() when spatial data, such as which category to use, has been modified. Components can handle this message and add spatial information to it. For 3D objects one would use something like the bounding sphere of a mesh, but it is also possible to use more abstract spatial data. void RtsSelectableComponent::OnUpdateLocalBounds(plMsgUpdateLocalBounds& msg) { plBoundingBoxSphere bounds; bounds.m_fSphereRadius = m_fSelectionRadius; bounds.m_vCenter.SetZero(); bounds.m_vBoxHalfExtends.Set(m_fSelectionRadius); msg.AddBounds(bounds, s_SelectableCategory); } Don't forget to register the message handler in the reflection block : PLASMA_BEGIN_MESSAGEHANDLERS { PLASMA_MESSAGE_HANDLER(plMsgUpdateLocalBounds, OnUpdateLocalBounds) } PLASMA_END_MESSAGEHANDLERS;","title":"Exposing Game Objects to the Spatial System"},{"location":"runtime/world/spatial-system/#querying-the-spatial-system","text":"Once you have spatial data inserted into the system, you can use it to efficiently query for objects within a volume. When calling functions such as plSpatialSystem::FindObjectsInSphere() you have to provide a bitmask of categories. That's because you can request to get objects from multiple categories at the same time. You can get this bitmask by calling plSpatialData::Category::GetBitmask() on a category object. void RtsGameState::InspectObjectsInArea(const plVec2& position, float radius, plSpatialSystem::QueryCallback callback) const { plBoundingSphere sphere(position.GetAsVec3(0), radius); plSpatialSystem::QueryParams queryParams; queryParams.m_uiCategoryBitmask = RtsSelectableComponent::s_SelectableCategory.GetBitmask(); m_pMainWorld->GetSpatialSystem()->FindObjectsInSphere(sphere, queryParams, callback); } In other language bindings you may instead need to pass in a list of all the desired categories by name.","title":"Querying the Spatial System"},{"location":"runtime/world/spatial-system/#spatial-system-vs-physics-engines","text":"Both the spatial system, as well as physics engines allow you to do spatial queries. There are cases where a problem can be solved using either system, but generally they are meant to complement each other. If you want to query for things that already need to have a physical representation, and therefore will be handled by the physics engine anyway, it is best to leverage the physics engine to query for such objects. For example a shockwave effect that is supposed to push objects away, only makes sense to be applied to physically simulated objects. Therefore, querying which objects are close-by, to figure out what objects to apply the effect to, should be done through the physics engine, and there is no reason to even have information about these objects in the spatial system. On the other hand, things like the RtsSelectableComponent (see above) could be achieved by setting up fake physics actors so that they can be found with physics queries. The performance cost for doing so would be unnecessary high though, as the physics engine would perform additional maintenance that is ultimately not needed, and it may waste precious resources such as collision layers . Here, using the spatial system makes much more sense. Note: If you require doing raycasts or queries against meshes , you will need to use the physics engine, as the spatial system only works with very basic shapes.","title":"Spatial System vs. Physics Engines"},{"location":"runtime/world/spatial-system/#spatial-system-vs-tags","text":"The spatial data categories are very similar to tags . The difference is, that tags are set up on game objects and they don't have any spatial quality. A game object can have many tags, but not be registered spatially and therefore cannot be found through spatial queries. On the other hand, because of this, tags have nearly no performance overhead, whereas spatial data must be updated whenever an object moves. Ultimately, both systems can be used to solve many of the same problems. When you need to be able to inspect an area and find all objects of a certain kind, you should use spatial data, for example through a marker component . If, however, you need to semtantically label objects, but do not require to find them spatially, prefer tags to not waste performance.","title":"Spatial System vs. Tags"},{"location":"runtime/world/spatial-system/#see-also","text":"Marker Component The World / Scenegraph System","title":"See Also"},{"location":"runtime/world/world-messaging/","text":"Messaging For an introduction what a message is and how it fits into the overall picture, see The World / Scenegraph System . This documentation focuses on the C++ plMessage class. The functionality exposed through other mechanisms, such as TypeScript , may be more limited in scope, but ultimately maps to the C++ implementation. Messages can be sent from any code. They can only be received by components , though, as the messaging system is implemented by plWorld . Declaring a Message A message has to be derived from plMessage and contain a helper macro that implements some message specific functionality: struct plMsgSetText : public plMessage { PLASMA_DECLARE_MESSAGE_TYPE(plMsgSetText, plMessage); plString m_sText; }; In some cpp file you then also need to implement the message and set up basic reflection information : PLASMA_IMPLEMENT_MESSAGE_TYPE(plMsgSetText); PLASMA_BEGIN_DYNAMIC_REFLECTED_TYPE(plMsgSetText, 1, plRTTIDefaultAllocator<plMsgSetText>) PLASMA_END_DYNAMIC_REFLECTED_TYPE; This is all that is needed to send and receive the message in C++ code. Note: The code above does not add reflection for each message member, as that is not necessary to make this message work. However, if you want to send and receive this message from non-C++ code, for example from TypeScript , then reflecting the members is necessary for the language binding to work. Be aware though, that language bindings may not support all types of reflected members and would ignore those. Message Handlers For each message type that your component is supposed to receive, you need to add a function that takes the respective component types as the only argument: void DisplayMsgComponent::OnSetText(plMsgSetText& msg) { m_sCurrentText = msg.m_sText; } void DisplayMsgComponent::OnSetColor(plMsgSetColor& msg) { m_TextColor = msg.m_Color; } Finally, you also need to register these functions as message handlers, in the component's reflection information: PLASMA_BEGIN_MESSAGEHANDLERS { PLASMA_MESSAGE_HANDLER(plMsgSetText, OnSetText), PLASMA_MESSAGE_HANDLER(plMsgSetColor, OnSetColor) } PLASMA_END_MESSAGEHANDLERS; Now this component is ready to receive messages of those types. Sending Messages To send a message, first create an instance on the stack (don't heap allocate them) and then call one of the SendMessage() functions: plMsgSetText textMsg; textMsg.m_sText = m_TextArray[idx]; pGameObject->SendMessageRecursive(textMsg); Message Routing There are several ways a message can be sent. Which function you use determines which components may see the message and also how efficient the delivery will be. You can send functions either through an plGameObject , through an plComponent or through an plWorld . It is differentiated between sending a message (direct) and posting a message (delayed). Messages sent through functions on plComponent will always only be received by exactly that component and no one else. Messages sent through plGameObject are broadcast to all components on that object. If one of the Recursive variants is used, the messages are additionally delivered to all components on all child objects. When sending messages through plWorld , you identify the target through a handle ( plGameObjectHandle or plComponentHandle ). In this case the world takes care of resolving the handle for you. If the target object does not exist anymore, the message won't be handled by anyone. If you call SendMessage() , the message is delivered immediately . That also means that all message handlers will access the same message object. This can be used to query information, as the message handler can write data back to the message. If the message is meant to be sent to multiple receivers, the code must take care to properly append or aggregate the results. Messages are never delivered multi-threaded, though. If you call PostMessage() , the message is delivered delayed . These messages are queued and delivered when their time has come. If the target does not exist anymore at that time, the message is discarded with no effect. Posted messages can't be used to retrieve a result. Internally, posted messages will be copied, so you still don't need to allocate them on the heap. Apart from a time delay, when posting a message you also have to specify a phase in which to deliver the message (see plObjectMsgQueueType ). This is used for special cases, where you want to tightly control at what time during the world update the message should arrive. For most cases using plObjectMsgQueueType::NextFrame is the right choice. Event Messages Event messages are a special type of message with a different kind of semantic. Regular messages are used to 'instruct' components to do something. For example to switch something on, or to apply a physical force to it. These things can be implemented differently by different component types, but generally the calling code assumes them to do something. When messages are broadcast (instead of sending them to a single component directly), all targeted components get the message. Event messages on the other hand, are used to 'inform' an object hierarchy that something happened . An example would be plMsgDamage which is used to inform an object that it received damage. For these kind of messages you only want one component to handle it. This will often be a very high-level component like a custom component representing the player or an NPC. These types of components are typically attached to the very top of a prefab structure and they are supposed to manage the overall game logic for this object. Therefore the message routing for event messages follows a different pattern: when you call plGameObject::SendEventMessage() on a node, the message is delivered to the closest parent node that has a component of type plEventMessageHandlerComponent . The plEventMessageHandlerComponent interface is only implemented by very few component types. Out of the box, only by plTypeScriptComponent and plVisualScriptComponent , meaning only components used to provide scripting functionality are currently capable of handling event messages. However, if you were to write your own NPC class, that is supposed to implement actual game logic (and not just path finding), then it makes sense to make it an plEventMessageHandlerComponent . Note that once an plEventMessageHandlerComponent is attached to a node, it will receive all event messages below that node hierarchy, no matter whether it has a message handler for it or not. It therefore prevents event messages from leaving the hierarchy by default. If an event message is supposed to 'bubble up' further, the message handler component must forward the message manually. Declaring Event Messages The only difference between regular messages and event messages is, that event messages must be derived from plEventMessage . Sending Event Messages You can send event messages to every object using plGameObject::SendEventMessage() . This will determine the closest parent to handle events on the fly and deliver the message accordingly. For components that regularly raise events to the same object hierarchy (their own), such as trigger components, it is more efficient to have a member of type plEventMessageSender<> . Sending a message through this object will cache the receiving target and be more efficient the second time. If a component that is an plEventMessageHandlerComponent itself wants to send an event message further up the hierarchy, it has to send the event to its own parent node. Caution: Event messages are also just regular messages and can be sent that way using SendMessage() or PostMessage() . If you accidentally use those functions, rather than SendEventMessage() or PostEventMessage() , your message will not get delivered as intended. Global Event Message Handlers If an event is sent to a hierarchy that has no event handler component, it is ultimately delivered to a global event handler . A global event handler is simply a component that has the HandleGlobalEvents property enabled. A global event handler can be useful as a catch-all level scripts. This way you can place buttons around a level, and have a single script that receives the message when one of them is pressed. Each button has its own script to implement its logic (when you can press it, how it changes its appearance and so on), but the button script then just raises a generic \"button pressed\" event on its own parent node. If those buttons don't have an event handling parent node, the message is delivered to the level script, which can then handle the logic of all those buttons. Be careful though when using multiple global event handler components. Every type of message is only delivered to a single handler, so as long as each global handler takes care of a different type, it will work as expected. Message Serialization For regular messages you don't need to implement any serialization, as they are short lived within the same process. However, if you intend to record messages or send them across a network, you can utilize plMessage::PackageForTransfer() and plMessage::ReplicatePackedMessage() . To make these functions work, you need to override and implement plMessage::Serialize() and plMessage::Deserialize() . See Also The World / Scenegraph System Sample Game Plugin","title":"Messaging"},{"location":"runtime/world/world-messaging/#messaging","text":"For an introduction what a message is and how it fits into the overall picture, see The World / Scenegraph System . This documentation focuses on the C++ plMessage class. The functionality exposed through other mechanisms, such as TypeScript , may be more limited in scope, but ultimately maps to the C++ implementation. Messages can be sent from any code. They can only be received by components , though, as the messaging system is implemented by plWorld .","title":"Messaging"},{"location":"runtime/world/world-messaging/#declaring-a-message","text":"A message has to be derived from plMessage and contain a helper macro that implements some message specific functionality: struct plMsgSetText : public plMessage { PLASMA_DECLARE_MESSAGE_TYPE(plMsgSetText, plMessage); plString m_sText; }; In some cpp file you then also need to implement the message and set up basic reflection information : PLASMA_IMPLEMENT_MESSAGE_TYPE(plMsgSetText); PLASMA_BEGIN_DYNAMIC_REFLECTED_TYPE(plMsgSetText, 1, plRTTIDefaultAllocator<plMsgSetText>) PLASMA_END_DYNAMIC_REFLECTED_TYPE; This is all that is needed to send and receive the message in C++ code. Note: The code above does not add reflection for each message member, as that is not necessary to make this message work. However, if you want to send and receive this message from non-C++ code, for example from TypeScript , then reflecting the members is necessary for the language binding to work. Be aware though, that language bindings may not support all types of reflected members and would ignore those.","title":"Declaring a Message"},{"location":"runtime/world/world-messaging/#message-handlers","text":"For each message type that your component is supposed to receive, you need to add a function that takes the respective component types as the only argument: void DisplayMsgComponent::OnSetText(plMsgSetText& msg) { m_sCurrentText = msg.m_sText; } void DisplayMsgComponent::OnSetColor(plMsgSetColor& msg) { m_TextColor = msg.m_Color; } Finally, you also need to register these functions as message handlers, in the component's reflection information: PLASMA_BEGIN_MESSAGEHANDLERS { PLASMA_MESSAGE_HANDLER(plMsgSetText, OnSetText), PLASMA_MESSAGE_HANDLER(plMsgSetColor, OnSetColor) } PLASMA_END_MESSAGEHANDLERS; Now this component is ready to receive messages of those types.","title":"Message Handlers"},{"location":"runtime/world/world-messaging/#sending-messages","text":"To send a message, first create an instance on the stack (don't heap allocate them) and then call one of the SendMessage() functions: plMsgSetText textMsg; textMsg.m_sText = m_TextArray[idx]; pGameObject->SendMessageRecursive(textMsg);","title":"Sending Messages"},{"location":"runtime/world/world-messaging/#message-routing","text":"There are several ways a message can be sent. Which function you use determines which components may see the message and also how efficient the delivery will be. You can send functions either through an plGameObject , through an plComponent or through an plWorld . It is differentiated between sending a message (direct) and posting a message (delayed). Messages sent through functions on plComponent will always only be received by exactly that component and no one else. Messages sent through plGameObject are broadcast to all components on that object. If one of the Recursive variants is used, the messages are additionally delivered to all components on all child objects. When sending messages through plWorld , you identify the target through a handle ( plGameObjectHandle or plComponentHandle ). In this case the world takes care of resolving the handle for you. If the target object does not exist anymore, the message won't be handled by anyone. If you call SendMessage() , the message is delivered immediately . That also means that all message handlers will access the same message object. This can be used to query information, as the message handler can write data back to the message. If the message is meant to be sent to multiple receivers, the code must take care to properly append or aggregate the results. Messages are never delivered multi-threaded, though. If you call PostMessage() , the message is delivered delayed . These messages are queued and delivered when their time has come. If the target does not exist anymore at that time, the message is discarded with no effect. Posted messages can't be used to retrieve a result. Internally, posted messages will be copied, so you still don't need to allocate them on the heap. Apart from a time delay, when posting a message you also have to specify a phase in which to deliver the message (see plObjectMsgQueueType ). This is used for special cases, where you want to tightly control at what time during the world update the message should arrive. For most cases using plObjectMsgQueueType::NextFrame is the right choice.","title":"Message Routing"},{"location":"runtime/world/world-messaging/#event-messages","text":"Event messages are a special type of message with a different kind of semantic. Regular messages are used to 'instruct' components to do something. For example to switch something on, or to apply a physical force to it. These things can be implemented differently by different component types, but generally the calling code assumes them to do something. When messages are broadcast (instead of sending them to a single component directly), all targeted components get the message. Event messages on the other hand, are used to 'inform' an object hierarchy that something happened . An example would be plMsgDamage which is used to inform an object that it received damage. For these kind of messages you only want one component to handle it. This will often be a very high-level component like a custom component representing the player or an NPC. These types of components are typically attached to the very top of a prefab structure and they are supposed to manage the overall game logic for this object. Therefore the message routing for event messages follows a different pattern: when you call plGameObject::SendEventMessage() on a node, the message is delivered to the closest parent node that has a component of type plEventMessageHandlerComponent . The plEventMessageHandlerComponent interface is only implemented by very few component types. Out of the box, only by plTypeScriptComponent and plVisualScriptComponent , meaning only components used to provide scripting functionality are currently capable of handling event messages. However, if you were to write your own NPC class, that is supposed to implement actual game logic (and not just path finding), then it makes sense to make it an plEventMessageHandlerComponent . Note that once an plEventMessageHandlerComponent is attached to a node, it will receive all event messages below that node hierarchy, no matter whether it has a message handler for it or not. It therefore prevents event messages from leaving the hierarchy by default. If an event message is supposed to 'bubble up' further, the message handler component must forward the message manually.","title":"Event Messages"},{"location":"runtime/world/world-messaging/#declaring-event-messages","text":"The only difference between regular messages and event messages is, that event messages must be derived from plEventMessage .","title":"Declaring Event Messages"},{"location":"runtime/world/world-messaging/#sending-event-messages","text":"You can send event messages to every object using plGameObject::SendEventMessage() . This will determine the closest parent to handle events on the fly and deliver the message accordingly. For components that regularly raise events to the same object hierarchy (their own), such as trigger components, it is more efficient to have a member of type plEventMessageSender<> . Sending a message through this object will cache the receiving target and be more efficient the second time. If a component that is an plEventMessageHandlerComponent itself wants to send an event message further up the hierarchy, it has to send the event to its own parent node. Caution: Event messages are also just regular messages and can be sent that way using SendMessage() or PostMessage() . If you accidentally use those functions, rather than SendEventMessage() or PostEventMessage() , your message will not get delivered as intended.","title":"Sending Event Messages"},{"location":"runtime/world/world-messaging/#global-event-message-handlers","text":"If an event is sent to a hierarchy that has no event handler component, it is ultimately delivered to a global event handler . A global event handler is simply a component that has the HandleGlobalEvents property enabled. A global event handler can be useful as a catch-all level scripts. This way you can place buttons around a level, and have a single script that receives the message when one of them is pressed. Each button has its own script to implement its logic (when you can press it, how it changes its appearance and so on), but the button script then just raises a generic \"button pressed\" event on its own parent node. If those buttons don't have an event handling parent node, the message is delivered to the level script, which can then handle the logic of all those buttons. Be careful though when using multiple global event handler components. Every type of message is only delivered to a single handler, so as long as each global handler takes care of a different type, it will work as expected.","title":"Global Event Message Handlers"},{"location":"runtime/world/world-messaging/#message-serialization","text":"For regular messages you don't need to implement any serialization, as they are short lived within the same process. However, if you intend to record messages or send them across a network, you can utilize plMessage::PackageForTransfer() and plMessage::ReplicatePackedMessage() . To make these functions work, you need to override and implement plMessage::Serialize() and plMessage::Deserialize() .","title":"Message Serialization"},{"location":"runtime/world/world-messaging/#see-also","text":"The World / Scenegraph System Sample Game Plugin","title":"See Also"},{"location":"runtime/world/world-modules/","text":"World Modules World modules are systems that are used to update certain aspects of a world. There can be only one instance of each world module for each world . A good example for a world module is the plPhysXWorldModule . This module is responsible for updating the physics world every frame. To do so, it hooks into two update phases of the world, once early in the frame, where it kicks off the physics simulation in a parallel task, and once late in the frame, where it fetches the results of the simulation and applies them to the world. Components represent individual pieces in the world. World modules represent large systems that provide the foundation for the components to work. World modules are frequently needed when integrating third party systems that require per frame updates to function. Creating and Instantiating World Modules You create a new world module class by deriving from plWorldModule . You never instantiate world modules yourself. Instead, call plWorld::GetOrCreateModule() . This will allocate the desired world module if necessary. Consequently, if no code path ever calls plWorld::GetOrCreateModule() , the respective world module will never be instantiated. Therefore, the lifetime and existence of a world module is often coupled to some component. Once a component is added to a world, its respective component manager (which also is a world module) is automatically instantiated. If those components request access to another world module, that will be instantiated, as well. Only few systems require a world module, without having some component type that would request its instantiation. For example, there is no need to instantiate a physics world module, if the scene doesn't contain any physics component. If you do need a system that is always running, consider putting it into a game state . And if you determine it really does need to be a world module, a custom game state may be the right place to do the initial call to plWorld::GetOrCreateModule() to instantiate the system. The more common approach, though, is to have a custom component type, which ensures to set up a world module. You would then put a single component of this type into each world. This also allows you to have properties on the component, with which you can configure the world module. Example: Wind World Module Code can query for the plWindWorldModuleInterface using plWorld::GetWorldModule<plWindWorldModuleInterface>() . If a world module that implements this interface exists, the function will return a valid pointer. Things like particle effects can then ask the system for a wind value at their location, to apply wind to particles. Wind can be implemented in different ways. From full 3D fluid simulations with turbulence, over simpler models, down to entirely basic models with just a randomly changing wind vector. What implementation you want may depend on your scene. Therefore, you choose the wind module by adding a corresponding component to the level. Out of the box you can have either no wind, or very simple wind. By adding an plSimpleWindComponent to a scene, that component will make sure a wind module of type plSimpleWindWorldModule is instantiated. Through the component's properties you can configure how the wind behaves. If you want different wind behavior, you can add your own implementation of plWindWorldModuleInterface through a plugin . You would then add your own wind component, which instantiates and configures your custom wind module. Update Functions The main feature of world modules is that they can hook into the world update and execute code at specific points. To do so, they need to register update functions using plWorldModule::RegisterUpdateFunction() . This should be done during plWorldModule::Initialize() . To register an update function, you need to fill out an UpdateFunctionDesc . This takes a delegate to the actual function that should be called, and requires you to give a unique name to that function. This way, other world modules can refer to your update function by name. This is useful, when you have dependencies between world modules. Say you need to run one part of the physics update, then a specific animation update and finally another part of the physics update. You can do so, by registering three update functions and set up dependencies. The world will then execute the update functions in the required order. Update Phases An important aspect of the update functions is in which update phase of the world they are executed. These are the steps in which the world is updated: Pre-async phase: The corresponding update functions are called synchronously in the order of their dependencies. Async phase: The update functions are called in batches, asynchronously on multiple threads. There is no guarantee in which order they are called. It is not allowed to access any data other than the components' own data during this phase. Post-async phase: Another synchronous phase like the pre-async phase. Object deletion: Dead objects and components are removed. Transform update: The global transformation of all dynamic objects is updated. Post-transform phase: Another synchronous phase like the pre-async phase, after the global transformation has been updated. The choice in which phase to run an update function affects performance, how you can access other components, and how recent some data is that you read. Many things must be updated in a single-threaded way. These would typically be done in the pre-async phase. Since everything runs single-threaded here, you can access other components, both to read and to modify them. If you have something that operates solely on the data of a single component and would be safe to be executed for multiple components at the same time, you should put this into the async phase. Your update function will automatically be distributed across multiple threads to speed things up. If you do have an async update, you may need to finalize or clean up some data afterwards, but in a single threaded way. Use the post async phase for that. In all of these phases you can modify the owner game object's local transform , but when you read the global transform you will get the value from the previous frame. For most use cases this is sufficient, but in a few cases you must have the absolutely latest global transform, to prevent things from lagging a frame behind. For those cases you use the post transform phase. Here you can read the latest global transform value that will be used by the renderer. You can still modify the local transform here, but it won't have an effect until the next frame. See Also Component Managers The World / Scenegraph System","title":"World Modules"},{"location":"runtime/world/world-modules/#world-modules","text":"World modules are systems that are used to update certain aspects of a world. There can be only one instance of each world module for each world . A good example for a world module is the plPhysXWorldModule . This module is responsible for updating the physics world every frame. To do so, it hooks into two update phases of the world, once early in the frame, where it kicks off the physics simulation in a parallel task, and once late in the frame, where it fetches the results of the simulation and applies them to the world. Components represent individual pieces in the world. World modules represent large systems that provide the foundation for the components to work. World modules are frequently needed when integrating third party systems that require per frame updates to function.","title":"World Modules"},{"location":"runtime/world/world-modules/#creating-and-instantiating-world-modules","text":"You create a new world module class by deriving from plWorldModule . You never instantiate world modules yourself. Instead, call plWorld::GetOrCreateModule() . This will allocate the desired world module if necessary. Consequently, if no code path ever calls plWorld::GetOrCreateModule() , the respective world module will never be instantiated. Therefore, the lifetime and existence of a world module is often coupled to some component. Once a component is added to a world, its respective component manager (which also is a world module) is automatically instantiated. If those components request access to another world module, that will be instantiated, as well. Only few systems require a world module, without having some component type that would request its instantiation. For example, there is no need to instantiate a physics world module, if the scene doesn't contain any physics component. If you do need a system that is always running, consider putting it into a game state . And if you determine it really does need to be a world module, a custom game state may be the right place to do the initial call to plWorld::GetOrCreateModule() to instantiate the system. The more common approach, though, is to have a custom component type, which ensures to set up a world module. You would then put a single component of this type into each world. This also allows you to have properties on the component, with which you can configure the world module.","title":"Creating and Instantiating World Modules"},{"location":"runtime/world/world-modules/#example-wind-world-module","text":"Code can query for the plWindWorldModuleInterface using plWorld::GetWorldModule<plWindWorldModuleInterface>() . If a world module that implements this interface exists, the function will return a valid pointer. Things like particle effects can then ask the system for a wind value at their location, to apply wind to particles. Wind can be implemented in different ways. From full 3D fluid simulations with turbulence, over simpler models, down to entirely basic models with just a randomly changing wind vector. What implementation you want may depend on your scene. Therefore, you choose the wind module by adding a corresponding component to the level. Out of the box you can have either no wind, or very simple wind. By adding an plSimpleWindComponent to a scene, that component will make sure a wind module of type plSimpleWindWorldModule is instantiated. Through the component's properties you can configure how the wind behaves. If you want different wind behavior, you can add your own implementation of plWindWorldModuleInterface through a plugin . You would then add your own wind component, which instantiates and configures your custom wind module.","title":"Example: Wind World Module"},{"location":"runtime/world/world-modules/#update-functions","text":"The main feature of world modules is that they can hook into the world update and execute code at specific points. To do so, they need to register update functions using plWorldModule::RegisterUpdateFunction() . This should be done during plWorldModule::Initialize() . To register an update function, you need to fill out an UpdateFunctionDesc . This takes a delegate to the actual function that should be called, and requires you to give a unique name to that function. This way, other world modules can refer to your update function by name. This is useful, when you have dependencies between world modules. Say you need to run one part of the physics update, then a specific animation update and finally another part of the physics update. You can do so, by registering three update functions and set up dependencies. The world will then execute the update functions in the required order.","title":"Update Functions"},{"location":"runtime/world/world-modules/#update-phases","text":"An important aspect of the update functions is in which update phase of the world they are executed. These are the steps in which the world is updated: Pre-async phase: The corresponding update functions are called synchronously in the order of their dependencies. Async phase: The update functions are called in batches, asynchronously on multiple threads. There is no guarantee in which order they are called. It is not allowed to access any data other than the components' own data during this phase. Post-async phase: Another synchronous phase like the pre-async phase. Object deletion: Dead objects and components are removed. Transform update: The global transformation of all dynamic objects is updated. Post-transform phase: Another synchronous phase like the pre-async phase, after the global transformation has been updated. The choice in which phase to run an update function affects performance, how you can access other components, and how recent some data is that you read. Many things must be updated in a single-threaded way. These would typically be done in the pre-async phase. Since everything runs single-threaded here, you can access other components, both to read and to modify them. If you have something that operates solely on the data of a single component and would be safe to be executed for multiple components at the same time, you should put this into the async phase. Your update function will automatically be distributed across multiple threads to speed things up. If you do have an async update, you may need to finalize or clean up some data afterwards, but in a single threaded way. Use the post async phase for that. In all of these phases you can modify the owner game object's local transform , but when you read the global transform you will get the value from the previous frame. For most use cases this is sufficient, but in a few cases you must have the absolutely latest global transform, to prevent things from lagging a frame behind. For those cases you use the post transform phase. Here you can read the latest global transform value that will be used by the renderer. You can still modify the local transform here, but it won't have an effect until the next frame.","title":"Update Phases"},{"location":"runtime/world/world-modules/#see-also","text":"Component Managers The World / Scenegraph System","title":"See Also"},{"location":"runtime/world/world-overview/","text":"The World / Scenegraph System When you build a scene in the editor or through code, the structure of all the objects is stored in something that is commonly referred to as a scenegraph . In Plasma the scenegraph is implemented by the class plWorld , which is why the terms scenegraph and world are used interchangeably in our documentation. ECS In Plasma we use a variation of an E ntity C omponent S ystem ( ECS ). It doesn't matter whether you are familiar with ECSs, but if you are, the main difference of our implementation to a pure ECS is, that in Plasma there is always exactly one system to handle each component type . You can have additional systems (see World Modules ), however, this is not as common as in other engines. The main classes involved are plWorld , plGameObject , plComponent and plWorldModule / plComponentManager . plWorld Each plWorld represents the entire state of a scene. Worlds hold all game objects and all world modules , which in turn hold all components . Each world has its own simulation state, such as a clock and a random number generator. Through the world modules, worlds also hold their own state for other simulation aspects, such as physics . You can have multiple worlds in parallel and they will be completely separated. This is for example the case when you have multiple documents open in the editor. Worlds are described in more detail in this chapter . plGameObject plGameObject is our entity class. The terms entity , game object and node are used interchangeably. Every game object has a position, rotation and scale. It may be attached to a single parent game object and it may have multiple game objects attached as children. The game object hierarchy is a directed acyclic graph (DAG). Game objects by themselves do not have any behavior . Instead, each game object can have an arbitrary number of components attached. The object's transform (position, rotation, scale) is local to its parent node, but it also holds a global transform, which is computed by concatenating the transformations of all parent nodes. Every time a game object or any of its parent nodes is moved, this global transform is updated. Game objects are described in more detail in this chapter . plComponent Components can be attached to game objects. They bring their own data and functionality. Components are used to implement behavior. For example light source components are used to tell the renderer how to light the scene, physics components are used to make objects collide with each other and AI components let creatures run around. By attaching components to game objects, you configure how that game object behaves. Components can interact with or depend on each other. For example a physics actor component would make an object fall to the ground, but it also needs a physics shape component to know whether the object should behave like a box, a sphere or something else. Components are described in more detail in this chapter . plWorldModule / plComponentManager World modules are the systems of the ECS pattern. Worlds are updated in multiple phases. Some phases are multi-threaded, others aren't. World modules can hook into these phases and make sure that they are called at the right time. World modules implement things like stepping third party code (e.g. physics). The most common type of world modules are component managers . Each component type has its own component manager, which is responsible for updating those components. The manager can leverage knowledge from other sources for determining which components need updating, and it can easily update components in a multi-threaded fashion, if it is save to do so. World modules are described in more detail in this chapter and component managers in this chapter . Object Lifetime The Plasma scenegraph does not use any kind of reference counting or garbage collection, however it does provide weak reference semantics through handles , to enable you to delete objects exactly when you need them to be removed, while still being able to detect whether an object is still alive. See the object lifetime chapter for details. Custom Components A large part of writing your own game, is to write your own components. If you need maximum control and performance, you need to write your components in C++ . You can also write components in TypeScript . Their functionality is very similar but a bit more limited. It is possible to use both and communicate between Typescript and C++ components using messages . Messaging When a component gets updated, it can access other components and call functions on them. Of course that requires that the other component type is known at compile time. In practice, that is often not the case. Take the projectile component as an example. Whenever a projectile hits something, it should apply damage to the hit object. However, what it hit was just the physical representation of an object (e.g. a physics actor ). The physics object doesn't have a concept of 'receiving damage' and therefore calling some 'OnDamage' function on the physics component makes no sense. Instead, on the object that has the physics component, there may be another component which knows how it would react to damage, so we want to send the information there. That component may be a custom component, though, which the projectile component knows nothing about, so there is no way to call a function on that. To solve this problem, you can send messages to components. A message is a class derived from plMessage and it can contain arbitrary data. Each component registers message handlers for all the types of messages that it wants to receive. When our projectile component now hits some object, it simply sends a damage message to that object. The engine will then deliver that message to all components which have a matching message handler. The message can be delivered right away, in which case a result can be written back to the message, or with a delay. Using messages decouples code, as components that know nothing of each other can still communicate and interact. The message system is also highly optimized for best performance. Messages are described in more detail in this chapter . Spatial System The world also sorts objects into a spatial system, to enable efficient queries for which objects are within a certain area. Although this is the basis for frustum culling in the renderer, it is also available to other systems. See this chapter for details. See Also Worlds Game Objects Components World Modules Component Managers Messaging Custom Code","title":"The World / Scenegraph System"},{"location":"runtime/world/world-overview/#the-world-scenegraph-system","text":"When you build a scene in the editor or through code, the structure of all the objects is stored in something that is commonly referred to as a scenegraph . In Plasma the scenegraph is implemented by the class plWorld , which is why the terms scenegraph and world are used interchangeably in our documentation.","title":"The World / Scenegraph System"},{"location":"runtime/world/world-overview/#ecs","text":"In Plasma we use a variation of an E ntity C omponent S ystem ( ECS ). It doesn't matter whether you are familiar with ECSs, but if you are, the main difference of our implementation to a pure ECS is, that in Plasma there is always exactly one system to handle each component type . You can have additional systems (see World Modules ), however, this is not as common as in other engines. The main classes involved are plWorld , plGameObject , plComponent and plWorldModule / plComponentManager .","title":"ECS"},{"location":"runtime/world/world-overview/#plworld","text":"Each plWorld represents the entire state of a scene. Worlds hold all game objects and all world modules , which in turn hold all components . Each world has its own simulation state, such as a clock and a random number generator. Through the world modules, worlds also hold their own state for other simulation aspects, such as physics . You can have multiple worlds in parallel and they will be completely separated. This is for example the case when you have multiple documents open in the editor. Worlds are described in more detail in this chapter .","title":"plWorld"},{"location":"runtime/world/world-overview/#plgameobject","text":"plGameObject is our entity class. The terms entity , game object and node are used interchangeably. Every game object has a position, rotation and scale. It may be attached to a single parent game object and it may have multiple game objects attached as children. The game object hierarchy is a directed acyclic graph (DAG). Game objects by themselves do not have any behavior . Instead, each game object can have an arbitrary number of components attached. The object's transform (position, rotation, scale) is local to its parent node, but it also holds a global transform, which is computed by concatenating the transformations of all parent nodes. Every time a game object or any of its parent nodes is moved, this global transform is updated. Game objects are described in more detail in this chapter .","title":"plGameObject"},{"location":"runtime/world/world-overview/#plcomponent","text":"Components can be attached to game objects. They bring their own data and functionality. Components are used to implement behavior. For example light source components are used to tell the renderer how to light the scene, physics components are used to make objects collide with each other and AI components let creatures run around. By attaching components to game objects, you configure how that game object behaves. Components can interact with or depend on each other. For example a physics actor component would make an object fall to the ground, but it also needs a physics shape component to know whether the object should behave like a box, a sphere or something else. Components are described in more detail in this chapter .","title":"plComponent"},{"location":"runtime/world/world-overview/#plworldmodule-plcomponentmanager","text":"World modules are the systems of the ECS pattern. Worlds are updated in multiple phases. Some phases are multi-threaded, others aren't. World modules can hook into these phases and make sure that they are called at the right time. World modules implement things like stepping third party code (e.g. physics). The most common type of world modules are component managers . Each component type has its own component manager, which is responsible for updating those components. The manager can leverage knowledge from other sources for determining which components need updating, and it can easily update components in a multi-threaded fashion, if it is save to do so. World modules are described in more detail in this chapter and component managers in this chapter .","title":"plWorldModule / plComponentManager"},{"location":"runtime/world/world-overview/#object-lifetime","text":"The Plasma scenegraph does not use any kind of reference counting or garbage collection, however it does provide weak reference semantics through handles , to enable you to delete objects exactly when you need them to be removed, while still being able to detect whether an object is still alive. See the object lifetime chapter for details.","title":"Object Lifetime"},{"location":"runtime/world/world-overview/#custom-components","text":"A large part of writing your own game, is to write your own components. If you need maximum control and performance, you need to write your components in C++ . You can also write components in TypeScript . Their functionality is very similar but a bit more limited. It is possible to use both and communicate between Typescript and C++ components using messages .","title":"Custom Components"},{"location":"runtime/world/world-overview/#messaging","text":"When a component gets updated, it can access other components and call functions on them. Of course that requires that the other component type is known at compile time. In practice, that is often not the case. Take the projectile component as an example. Whenever a projectile hits something, it should apply damage to the hit object. However, what it hit was just the physical representation of an object (e.g. a physics actor ). The physics object doesn't have a concept of 'receiving damage' and therefore calling some 'OnDamage' function on the physics component makes no sense. Instead, on the object that has the physics component, there may be another component which knows how it would react to damage, so we want to send the information there. That component may be a custom component, though, which the projectile component knows nothing about, so there is no way to call a function on that. To solve this problem, you can send messages to components. A message is a class derived from plMessage and it can contain arbitrary data. Each component registers message handlers for all the types of messages that it wants to receive. When our projectile component now hits some object, it simply sends a damage message to that object. The engine will then deliver that message to all components which have a matching message handler. The message can be delivered right away, in which case a result can be written back to the message, or with a delay. Using messages decouples code, as components that know nothing of each other can still communicate and interact. The message system is also highly optimized for best performance. Messages are described in more detail in this chapter .","title":"Messaging"},{"location":"runtime/world/world-overview/#spatial-system","text":"The world also sorts objects into a spatial system, to enable efficient queries for which objects are within a certain area. Although this is the basis for frustum culling in the renderer, it is also available to other systems. See this chapter for details.","title":"Spatial System"},{"location":"runtime/world/world-overview/#see-also","text":"Worlds Game Objects Components World Modules Component Managers Messaging Custom Code","title":"See Also"},{"location":"runtime/world/worlds/","text":"Worlds For an introduction what a world is and how it fits into the overall picture, see The World / Scenegraph System . This documentation focuses on the C++ plWorld class. The functionality exposed through other mechanisms, such as TypeScript , may be more limited in scope, but ultimately maps to the C++ implementation. Game Objects Game objects are allocated, destroyed and accessed through the world. For these details, see the chapter about game objects . Components Components are not directly managed by a world. Instead, worlds manage world modules and component managers , which in turn manage components. For details, see the chapter about components . World Modules World modules are bigger systems that manage aspects like particle effects , the Jolt integration , wind and so on. Component managers are a special type of world modules that take care of updating the various component types. Simulation State Each world has its own simulation state, to not affect other worlds. Simulation Enabled Every world can be actively simulated, or paused. plWorld::SetWorldSimulationEnabled() is used to toggle this. Clock Each world has its own plClock which can be retrieved through plWorld::GetClock() . The clock is used to speed up or slow down the simulation or to set a fixed update rate. The clock keeps track of the 'game time', so when a component needs to know the current time, it should query this from the world's clock. Random Number Generator When a component needs a random number, it should query this from the world via plWorld::GetRandomNumberGenerator() . Components or better, component managers can of course also have their own random number generator, for example when they need multi-threaded access to it, or when they want to control the seed value for determinism. The particle systems , for example, do this to achieve deterministic results. Coordinate System The default coordinate system in Plasma is: +X is 'forwards' +Y is 'right` +Z is 'up' That makes it a left-handed coordinate system. You can query these axis in the space of a game object , if you need to. The coordinate system can be changed through plWorld::SetCoordinateSystemProvider() . The plCoordinateSystemProvider can potentially return a different coordinate system at different locations, so you could implement a provider that changes the coordinate system to follow a sphere. Warning: Although components are supposed to not hard-code assumptions about which axis is 'forward', etc, using a non-default coordinate system is not well tested. Using a dynamic coordinate system even less so. Read / Write Access Control Some aspects of the world are updated in a multi-threaded fashion. For instance, rendering generally happens in parallel to other updates. To prevent you from accessing the world in a problematic way, you need to lock the world for reading or writing when you work with it. From within a component update function you don't need to worry, you always have write access to the world while components are being updated. However, if for example you want to load a level or otherwise set it up procedurally at launch, you need to lock it for write access: PLASMA_LOCK(pWorld->GetWriteMarker()); pWorld->CreateObject(...) In developer builds the world will check that you have properly locked it when you try to do certain operations. Therefore, the best way to know where to add such locks, is simply to run your code without a lock and see whether the engine asserts. If so, you can just traverse your callstack to find a reasonable place to insert the lock. World Update To step your world, call plWorld::Update() . The time delta that will be applied depends on whether the world simulation is enabled and how your world clock is configured. See Also Game Objects Components World Modules Object Lifetime Messaging","title":"Worlds"},{"location":"runtime/world/worlds/#worlds","text":"For an introduction what a world is and how it fits into the overall picture, see The World / Scenegraph System . This documentation focuses on the C++ plWorld class. The functionality exposed through other mechanisms, such as TypeScript , may be more limited in scope, but ultimately maps to the C++ implementation.","title":"Worlds"},{"location":"runtime/world/worlds/#game-objects","text":"Game objects are allocated, destroyed and accessed through the world. For these details, see the chapter about game objects .","title":"Game Objects"},{"location":"runtime/world/worlds/#components","text":"Components are not directly managed by a world. Instead, worlds manage world modules and component managers , which in turn manage components. For details, see the chapter about components .","title":"Components"},{"location":"runtime/world/worlds/#world-modules","text":"World modules are bigger systems that manage aspects like particle effects , the Jolt integration , wind and so on. Component managers are a special type of world modules that take care of updating the various component types.","title":"World Modules"},{"location":"runtime/world/worlds/#simulation-state","text":"Each world has its own simulation state, to not affect other worlds.","title":"Simulation State"},{"location":"runtime/world/worlds/#simulation-enabled","text":"Every world can be actively simulated, or paused. plWorld::SetWorldSimulationEnabled() is used to toggle this.","title":"Simulation Enabled"},{"location":"runtime/world/worlds/#clock","text":"Each world has its own plClock which can be retrieved through plWorld::GetClock() . The clock is used to speed up or slow down the simulation or to set a fixed update rate. The clock keeps track of the 'game time', so when a component needs to know the current time, it should query this from the world's clock.","title":"Clock"},{"location":"runtime/world/worlds/#random-number-generator","text":"When a component needs a random number, it should query this from the world via plWorld::GetRandomNumberGenerator() . Components or better, component managers can of course also have their own random number generator, for example when they need multi-threaded access to it, or when they want to control the seed value for determinism. The particle systems , for example, do this to achieve deterministic results.","title":"Random Number Generator"},{"location":"runtime/world/worlds/#coordinate-system","text":"The default coordinate system in Plasma is: +X is 'forwards' +Y is 'right` +Z is 'up' That makes it a left-handed coordinate system. You can query these axis in the space of a game object , if you need to. The coordinate system can be changed through plWorld::SetCoordinateSystemProvider() . The plCoordinateSystemProvider can potentially return a different coordinate system at different locations, so you could implement a provider that changes the coordinate system to follow a sphere. Warning: Although components are supposed to not hard-code assumptions about which axis is 'forward', etc, using a non-default coordinate system is not well tested. Using a dynamic coordinate system even less so.","title":"Coordinate System"},{"location":"runtime/world/worlds/#read-write-access-control","text":"Some aspects of the world are updated in a multi-threaded fashion. For instance, rendering generally happens in parallel to other updates. To prevent you from accessing the world in a problematic way, you need to lock the world for reading or writing when you work with it. From within a component update function you don't need to worry, you always have write access to the world while components are being updated. However, if for example you want to load a level or otherwise set it up procedurally at launch, you need to lock it for write access: PLASMA_LOCK(pWorld->GetWriteMarker()); pWorld->CreateObject(...) In developer builds the world will check that you have properly locked it when you try to do certain operations. Therefore, the best way to know where to add such locks, is simply to run your code without a lock and see whether the engine asserts. If so, you can just traverse your callstack to find a reasonable place to insert the lock.","title":"Read / Write Access Control"},{"location":"runtime/world/worlds/#world-update","text":"To step your world, call plWorld::Update() . The time delta that will be applied depends on whether the world simulation is enabled and how your world clock is configured.","title":"World Update"},{"location":"runtime/world/worlds/#see-also","text":"Game Objects Components World Modules Object Lifetime Messaging","title":"See Also"},{"location":"terrain/heightfield-component/","text":"Heightfield Component The HeightfieldComponent can be used for simple, heightmap based terrain, as well as for adding small details like piles of rubble to a scene. The heightfield component uses an ImageData asset to get the height information. Consequently, the heightmap image has to be imported as such an asset type. Collision Mesh When the tag AutoColMesh is set on the owner game object, the component adds its mesh to the scene collision mesh , just like the greyboxing components . This is only supported for static game objects though. Currently all triangles in the collision mesh will all use the same surface , the one set on the main material. Properties HeightfieldImage: The ImageData asset that contains the terrain height information. Material: The single material used for the entire mesh. If you need multiple layers (grass, dirt, rock, etc) you can write a custom visual shader (TODO) that uses a mask texture to fetch and blend the various layers as desired. HalfExtents: The size of the terrain in X and Y direction. Height: How much to stretch the terrain along the up axis. Tesselation: How densely to tesselate the graphics mesh. TexCoordOffset, TexCoordScale: An offset and scale to shift and stretch the texture coordinates. Use a scale of one, if the material should be stretched once across the entire terrain. Use a large scale value instead, if the material should be repeated many times. ColMeshTesselation: How densely to tesselate the physics mesh. It often makes sense to only use the half resolution of the graphics mesh. Performance Considerations Consider to deactivate casting shadows by removing the CastShadow tag on the owner game object. It is rare that terrain has such prominent features, that they would be cast visible shadows. However, densely tesselated terrain meshes add a high cost to the shadow map update. Deactivate the collision mesh generation, if the terrain (or pile of rubble) is purely visual. See Also Terrain and Vegetation","title":"Heightfield Component"},{"location":"terrain/heightfield-component/#heightfield-component","text":"The HeightfieldComponent can be used for simple, heightmap based terrain, as well as for adding small details like piles of rubble to a scene. The heightfield component uses an ImageData asset to get the height information. Consequently, the heightmap image has to be imported as such an asset type.","title":"Heightfield Component"},{"location":"terrain/heightfield-component/#collision-mesh","text":"When the tag AutoColMesh is set on the owner game object, the component adds its mesh to the scene collision mesh , just like the greyboxing components . This is only supported for static game objects though. Currently all triangles in the collision mesh will all use the same surface , the one set on the main material.","title":"Collision Mesh"},{"location":"terrain/heightfield-component/#properties","text":"HeightfieldImage: The ImageData asset that contains the terrain height information. Material: The single material used for the entire mesh. If you need multiple layers (grass, dirt, rock, etc) you can write a custom visual shader (TODO) that uses a mask texture to fetch and blend the various layers as desired. HalfExtents: The size of the terrain in X and Y direction. Height: How much to stretch the terrain along the up axis. Tesselation: How densely to tesselate the graphics mesh. TexCoordOffset, TexCoordScale: An offset and scale to shift and stretch the texture coordinates. Use a scale of one, if the material should be stretched once across the entire terrain. Use a large scale value instead, if the material should be repeated many times. ColMeshTesselation: How densely to tesselate the physics mesh. It often makes sense to only use the half resolution of the graphics mesh.","title":"Properties"},{"location":"terrain/heightfield-component/#performance-considerations","text":"Consider to deactivate casting shadows by removing the CastShadow tag on the owner game object. It is rare that terrain has such prominent features, that they would be cast visible shadows. However, densely tesselated terrain meshes add a high cost to the shadow map update. Deactivate the collision mesh generation, if the terrain (or pile of rubble) is purely visual.","title":"Performance Considerations"},{"location":"terrain/heightfield-component/#see-also","text":"Terrain and Vegetation","title":"See Also"},{"location":"terrain/terrain-overview/","text":"Terrain and Vegetation Terrain Currently there are no tools for creating terrain. Terrain meshes are best build in external tools and imported as regular static meshes . A popular method to represent terrain are heightmaps - 2D grayscale images, where the brightness of each pixel represents the height of the terrain at that location. Plasma Engine provides a heightfield component which enables you to get such terrain data into the engine easily. However, this is only meant for simple use cases. Vegetation Vegetation can be created with standard meshes. Using custom visual shaders (TODO) , a basic per-vertex wind animation can be applied. See Also Heightfield Component","title":"Terrain and Vegetation"},{"location":"terrain/terrain-overview/#terrain-and-vegetation","text":"","title":"Terrain and Vegetation"},{"location":"terrain/terrain-overview/#terrain","text":"Currently there are no tools for creating terrain. Terrain meshes are best build in external tools and imported as regular static meshes . A popular method to represent terrain are heightmaps - 2D grayscale images, where the brightness of each pixel represents the height of the terrain at that location. Plasma Engine provides a heightfield component which enables you to get such terrain data into the engine easily. However, this is only meant for simple use cases.","title":"Terrain"},{"location":"terrain/terrain-overview/#vegetation","text":"Vegetation can be created with standard meshes. Using custom visual shaders (TODO) , a basic per-vertex wind animation can be applied.","title":"Vegetation"},{"location":"terrain/terrain-overview/#see-also","text":"Heightfield Component","title":"See Also"},{"location":"terrain/procedural/procedural-object-placement/","text":"Procedural Object Placement Creating large terrain with convincing vegetation is a lot of work. Since nature generally follows certain rules, such as on which type of terrain which plant can grow, how densely they are packed and so on, it makes sense to rather build and apply such rules to automatically place objects, than attempting to place vegetation by hand. Plasma comes with a procedural generation feature. This is designed specifically for decorating terrain with vegetation, but can also be used in other scenarios. The system is heavily inspired by the procedural vegetation system in Horizon Zero Dawn . See this GDC talk ( video , slides ) for reference. Technical Overview The procedural generation system is active in a scene once a procedural placement component is added to it. The component defines in which area a certain rule is used to place objects. The rules are set up through ProcGen graph assets . The rules specify which objects to place under which conditions and with what kind of variation. Additional components can be placed to affect the object placement in select areas, such as for clearing an area or increasing the density of a certain type of plant. The procedural placement system only places objects during scene simulation, so without pressing play in the editor, you won't see any placement. The system uses the position of the main camera to determine where to place objects. Object placement is distributed across frames, to prevent stutter. A grid around the camera is used to determine in which area objects have been placed already. When the camera moves, new cells will be populated, and cells that are too far away are cleared again. This way the system makes sure that there is a relatively constant performance impact. Consequently, procedurally placed objects can't have state that needs to be persistent. Each object can have state, for example you could make it place trees that can be burned down, but it must be fine for your game, that the tree will reappear, if the player gets far away and returns. Object placement is deterministic. As long as the placement rules and the conditions (terrain, materials, etc) don't change, the same object will be placed at the same position every single time. See Also ProcGen Graph Asset Procedural Placement Component","title":"Procedural Object Placement"},{"location":"terrain/procedural/procedural-object-placement/#procedural-object-placement","text":"Creating large terrain with convincing vegetation is a lot of work. Since nature generally follows certain rules, such as on which type of terrain which plant can grow, how densely they are packed and so on, it makes sense to rather build and apply such rules to automatically place objects, than attempting to place vegetation by hand. Plasma comes with a procedural generation feature. This is designed specifically for decorating terrain with vegetation, but can also be used in other scenarios. The system is heavily inspired by the procedural vegetation system in Horizon Zero Dawn . See this GDC talk ( video , slides ) for reference.","title":"Procedural Object Placement"},{"location":"terrain/procedural/procedural-object-placement/#technical-overview","text":"The procedural generation system is active in a scene once a procedural placement component is added to it. The component defines in which area a certain rule is used to place objects. The rules are set up through ProcGen graph assets . The rules specify which objects to place under which conditions and with what kind of variation. Additional components can be placed to affect the object placement in select areas, such as for clearing an area or increasing the density of a certain type of plant. The procedural placement system only places objects during scene simulation, so without pressing play in the editor, you won't see any placement. The system uses the position of the main camera to determine where to place objects. Object placement is distributed across frames, to prevent stutter. A grid around the camera is used to determine in which area objects have been placed already. When the camera moves, new cells will be populated, and cells that are too far away are cleared again. This way the system makes sure that there is a relatively constant performance impact. Consequently, procedurally placed objects can't have state that needs to be persistent. Each object can have state, for example you could make it place trees that can be burned down, but it must be fine for your game, that the tree will reappear, if the player gets far away and returns. Object placement is deterministic. As long as the placement rules and the conditions (terrain, materials, etc) don't change, the same object will be placed at the same position every single time.","title":"Technical Overview"},{"location":"terrain/procedural/procedural-object-placement/#see-also","text":"ProcGen Graph Asset Procedural Placement Component","title":"See Also"},{"location":"terrain/procedural/procgen-graph-asset/","text":"ProcGen Graph Asset The ProcGen Graph Asset is used to configure the rules for procedural object placement . In this graph structure you define which objects should be placed under which conditions. Editing the ProcGen Graph The main area of this document is used to place and connect nodes in a graph. Use the context menu to add nodes. Drag and drop pins from left to right to connect outputs to inputs. Pins usually represent single number values, such as a density value or a single channel of a color (e.g. 'red'). By connecting a pin on the right side of a node (output) to a pin on the left side of a node (input), the output value is forwarded into the other node and affects how that node operates. Nodes that only have pins on their right side, are pure input nodes , they only provide data for other nodes to consume. Nodes that only have pins on their left side, are output nodes . They consume various input values and then create some kind of result, for example they decide whether to place an object at a certain location. When you select a node, the property grid shows additional configuration options. The image below shows a graph with three input nodes and three output nodes . Each input node is connected to one output node and thus affects how that output node places objects. Interactions Right-click and drag in the view, to move it around. Mouse-wheel to zoom. Left-click to select nodes. Left-click and drag to select multiple nodes. Ctrl + left-click to add or remove a node from the selection. Right-click on a node, pin or connection for a context-menu in which you can delete the object. Click on an empty spot to open a context menu from which to create new nodes. Left-click and drag any pin to connect it to another pin. The UI will indicate which pins can be connected. Graph Output What exact output the procedural rules generate depends on which output nodes are present in the graph. Currently these types of output are available: ProcGen Graph Placement Output ProcGen Graph Vertex Color Output (TODO) Tip: To learn the system, it is best to start with the placement output node and ignore the rest. The simplest possible graph only contains a single such node and nothing else. Using a ProcGen Graph The ProcGen graph asset doesn't have any kind of pre-visualization. To see what effect a rule has, you need a scene in which the necessary setup is available. The scene should contain some geometry with collision meshes , such that raycasts can hit the geometry. Greyboxing and heightfield components work just fine for that. You also need a component that applies the ProcGen graph. Depending on the output nodes used in the graph this would be one of these: Procedural Placement Component Procedural Vertex Color Component (TODO) Make sure the ProcGen graph asset is transformed . Then press play to simulate it . If everything is set up right, you should see objects getting placed around the camera, within the specified volume. Live Editing When you edit a ProcGen graph asset, most changes trigger a live update in any running scene. That means you can switch back and forth between the asset and a test scene, and see changes update right away. However, this is limited to certain types of changes. Changes to referenced assets (such as color gradients or prefabs ) won't update the already placed objects. If such a change was done, you need to stop simulating a scene, and run it again . Debug Mode It can be difficult to get an idea for the values that a rule graph produces. To visualize the values, you can right-click any pin and enable the Debug flag. This disables all placement output and instead switches to a mode where for every location only a sphere is rendered, and the shade of the sphere represents the value of the pin on which the debug flag is enabled. Black for 0 and white for 1 . When you then switch to a scene and run the simulation, you will see this pattern: Building Complex Rules Once you've figured out the basics, you can build more complex rules. Use the ProcGen graph input nodes (TODO) to receive more information about a location, such as its slope or height. Pass the data through ProcGen graph math nodes to adjust it as necessary. Use ProcGen graph modifier nodes to make it possible to configure the rules locally. See Also Procedural Placement Component ProcGen Graph Placement Output Procedural Vertex Color Component (TODO) ProcGen Graph Vertex Color Output (TODO) Procedural Volume Box Component Procedural Volume Image Component","title":"ProcGen Graph Asset"},{"location":"terrain/procedural/procgen-graph-asset/#procgen-graph-asset","text":"The ProcGen Graph Asset is used to configure the rules for procedural object placement . In this graph structure you define which objects should be placed under which conditions.","title":"ProcGen Graph Asset"},{"location":"terrain/procedural/procgen-graph-asset/#editing-the-procgen-graph","text":"The main area of this document is used to place and connect nodes in a graph. Use the context menu to add nodes. Drag and drop pins from left to right to connect outputs to inputs. Pins usually represent single number values, such as a density value or a single channel of a color (e.g. 'red'). By connecting a pin on the right side of a node (output) to a pin on the left side of a node (input), the output value is forwarded into the other node and affects how that node operates. Nodes that only have pins on their right side, are pure input nodes , they only provide data for other nodes to consume. Nodes that only have pins on their left side, are output nodes . They consume various input values and then create some kind of result, for example they decide whether to place an object at a certain location. When you select a node, the property grid shows additional configuration options. The image below shows a graph with three input nodes and three output nodes . Each input node is connected to one output node and thus affects how that output node places objects.","title":"Editing the ProcGen Graph"},{"location":"terrain/procedural/procgen-graph-asset/#interactions","text":"Right-click and drag in the view, to move it around. Mouse-wheel to zoom. Left-click to select nodes. Left-click and drag to select multiple nodes. Ctrl + left-click to add or remove a node from the selection. Right-click on a node, pin or connection for a context-menu in which you can delete the object. Click on an empty spot to open a context menu from which to create new nodes. Left-click and drag any pin to connect it to another pin. The UI will indicate which pins can be connected.","title":"Interactions"},{"location":"terrain/procedural/procgen-graph-asset/#graph-output","text":"What exact output the procedural rules generate depends on which output nodes are present in the graph. Currently these types of output are available: ProcGen Graph Placement Output ProcGen Graph Vertex Color Output (TODO) Tip: To learn the system, it is best to start with the placement output node and ignore the rest. The simplest possible graph only contains a single such node and nothing else.","title":"Graph Output"},{"location":"terrain/procedural/procgen-graph-asset/#using-a-procgen-graph","text":"The ProcGen graph asset doesn't have any kind of pre-visualization. To see what effect a rule has, you need a scene in which the necessary setup is available. The scene should contain some geometry with collision meshes , such that raycasts can hit the geometry. Greyboxing and heightfield components work just fine for that. You also need a component that applies the ProcGen graph. Depending on the output nodes used in the graph this would be one of these: Procedural Placement Component Procedural Vertex Color Component (TODO) Make sure the ProcGen graph asset is transformed . Then press play to simulate it . If everything is set up right, you should see objects getting placed around the camera, within the specified volume.","title":"Using a ProcGen Graph"},{"location":"terrain/procedural/procgen-graph-asset/#live-editing","text":"When you edit a ProcGen graph asset, most changes trigger a live update in any running scene. That means you can switch back and forth between the asset and a test scene, and see changes update right away. However, this is limited to certain types of changes. Changes to referenced assets (such as color gradients or prefabs ) won't update the already placed objects. If such a change was done, you need to stop simulating a scene, and run it again .","title":"Live Editing"},{"location":"terrain/procedural/procgen-graph-asset/#debug-mode","text":"It can be difficult to get an idea for the values that a rule graph produces. To visualize the values, you can right-click any pin and enable the Debug flag. This disables all placement output and instead switches to a mode where for every location only a sphere is rendered, and the shade of the sphere represents the value of the pin on which the debug flag is enabled. Black for 0 and white for 1 . When you then switch to a scene and run the simulation, you will see this pattern:","title":"Debug Mode"},{"location":"terrain/procedural/procgen-graph-asset/#building-complex-rules","text":"Once you've figured out the basics, you can build more complex rules. Use the ProcGen graph input nodes (TODO) to receive more information about a location, such as its slope or height. Pass the data through ProcGen graph math nodes to adjust it as necessary. Use ProcGen graph modifier nodes to make it possible to configure the rules locally.","title":"Building Complex Rules"},{"location":"terrain/procedural/procgen-graph-asset/#see-also","text":"Procedural Placement Component ProcGen Graph Placement Output Procedural Vertex Color Component (TODO) ProcGen Graph Vertex Color Output (TODO) Procedural Volume Box Component Procedural Volume Image Component","title":"See Also"},{"location":"terrain/procedural/procgen-graph-inputs/","text":"ProcGen Graph Input Nodes These node types are available as inputs for ProcGen graph assets . Slope Input Node When an object gets placed at a specific location, the Slope node calculates the slope of the terrain at that position and determines whether it is within a desired range. The better it is within the range, the closer the output value is to 1 , and if the slope is outside the desired range, the output value is 0 . The output value of this node can be passed unchanged as Density into the placement output node . In this case the slope directly decides whether an object gets placed or not. It may, however, also be passed into other values, for instance to affect the color of an object. On the node you select a MinSlope and a MaxSlope which define the desired range. For example, if the MinSlope is set to 0 (flat ground) and the MaxSlope is set to 20 (slightly uphill), then objects will only be placed on nearly flat terrain. If, however, MinSlope is set to 30 (steep) and MaxSlope is set to 70 (nearly vertical), then objects will only be placed along strong slopes, for example the sides of mountains. The LowerFade and UpperFade values determine how quickly the output value fades towards zero when the slope approaches MinSlope or MaxSlope respectively. With a fade value of zero, the cut off is very abrupt, with a fade value of one, the output value declines earlier, but also more gradually. Node Properties MinSlope , MaxSlope : The slope range (in degree) between which the output value is non-zero. LowerFade , UpperFade : How quickly to fade the output value from one towards zero, when the slope approaches the min (lower) or max (upper) value. Fade values of zero mean an abrupt change from one to zero at the boundaries, a value of one means there is always some fade out, except right at the center of the value range. Height Input Node The Height node works mostly the same way as the Slope node, except that it uses the height (z value) of the potential object position. The Height node determines the z value of the location where an object shall be placed. It then checks whether the value is between MinHeight and MaxHeight . If not, it outputs the value 0 . Otherwise, it outputs a non-zero value. LowerFade and UpperFade are used to decide whether, and how much, to fade the output value from 1 towards 0 . This node can be used to place objects only at specific altitudes, or to change object sizes or colors at higher elevations. Node Properties MinHeight , MaxHeight : The height range between which the output value is non-zero. LowerFade , UpperFade : How quickly to fade the output value from one towards zero, when the height approaches the min (lower) or max (upper) value. Fade values of zero mean an abrupt change from one to zero at the boundaries, a value of one means there is always some fade out, except right at the center of the value range. Mesh Vertex Color Input Node TODO See Also Procedural Object Placement ProcGen Graph Math Nodes","title":"ProcGen Graph Input Nodes"},{"location":"terrain/procedural/procgen-graph-inputs/#procgen-graph-input-nodes","text":"These node types are available as inputs for ProcGen graph assets .","title":"ProcGen Graph Input Nodes"},{"location":"terrain/procedural/procgen-graph-inputs/#slope-input-node","text":"When an object gets placed at a specific location, the Slope node calculates the slope of the terrain at that position and determines whether it is within a desired range. The better it is within the range, the closer the output value is to 1 , and if the slope is outside the desired range, the output value is 0 . The output value of this node can be passed unchanged as Density into the placement output node . In this case the slope directly decides whether an object gets placed or not. It may, however, also be passed into other values, for instance to affect the color of an object. On the node you select a MinSlope and a MaxSlope which define the desired range. For example, if the MinSlope is set to 0 (flat ground) and the MaxSlope is set to 20 (slightly uphill), then objects will only be placed on nearly flat terrain. If, however, MinSlope is set to 30 (steep) and MaxSlope is set to 70 (nearly vertical), then objects will only be placed along strong slopes, for example the sides of mountains. The LowerFade and UpperFade values determine how quickly the output value fades towards zero when the slope approaches MinSlope or MaxSlope respectively. With a fade value of zero, the cut off is very abrupt, with a fade value of one, the output value declines earlier, but also more gradually.","title":"Slope Input Node"},{"location":"terrain/procedural/procgen-graph-inputs/#node-properties","text":"MinSlope , MaxSlope : The slope range (in degree) between which the output value is non-zero. LowerFade , UpperFade : How quickly to fade the output value from one towards zero, when the slope approaches the min (lower) or max (upper) value. Fade values of zero mean an abrupt change from one to zero at the boundaries, a value of one means there is always some fade out, except right at the center of the value range.","title":"Node Properties"},{"location":"terrain/procedural/procgen-graph-inputs/#height-input-node","text":"The Height node works mostly the same way as the Slope node, except that it uses the height (z value) of the potential object position. The Height node determines the z value of the location where an object shall be placed. It then checks whether the value is between MinHeight and MaxHeight . If not, it outputs the value 0 . Otherwise, it outputs a non-zero value. LowerFade and UpperFade are used to decide whether, and how much, to fade the output value from 1 towards 0 . This node can be used to place objects only at specific altitudes, or to change object sizes or colors at higher elevations.","title":"Height Input Node"},{"location":"terrain/procedural/procgen-graph-inputs/#node-properties_1","text":"MinHeight , MaxHeight : The height range between which the output value is non-zero. LowerFade , UpperFade : How quickly to fade the output value from one towards zero, when the height approaches the min (lower) or max (upper) value. Fade values of zero mean an abrupt change from one to zero at the boundaries, a value of one means there is always some fade out, except right at the center of the value range.","title":"Node Properties"},{"location":"terrain/procedural/procgen-graph-inputs/#mesh-vertex-color-input-node","text":"TODO","title":"Mesh Vertex Color Input Node"},{"location":"terrain/procedural/procgen-graph-inputs/#see-also","text":"Procedural Object Placement ProcGen Graph Math Nodes","title":"See Also"},{"location":"terrain/procedural/procgen-graph-math/","text":"ProcGen Graph Math Nodes These node types implement math functions for ProcGen graph assets . Blend Node The Blend node provides the most common math expressions to combine two values. Node Properties Mode : Determines how the two input values get combined. InputA , InputB : Fallback values for pins A and B respectively, in case one of the pins isn't connected. Use this, in case you want to combine one value with a constant (e.g. to multiply A by two, leave B disconnected and set InputB to 2 ). ClampOutput : If this is enabled, the output value is clamped to [0;1] range. Perlin Noise Node The Perlin Noise node outputs a Perlin Noise value for the current location. This value can be used to add variety, however, not through completely random values, but rather ones that gradually change. So for instance, if the quality of soil varies, the look of vegetation may be different. However, bushes that grow next to each other are affected the same way, so their look will be similar. In the two images below, the same plants are placed. However, in the second image, Perlin noise is used to affect their color. Note how in the second image all plants in an area become darker or brighter. Node Properties Scale : Over which area the noise is stretched. Large values mean that the noise value changes slowly over large distances, whereas smaller values result in higher frequency noise. If the value is too small for the used object density, the results lose their gradually changing quality. Offset : Pushes the values along the respective axis. NumOctaves : How many Perlin noise values to combine for the final result. More octaves give a more varied pattern and can be scaled across a larger area without showing obvious repetitions, but also cost more performance to evaluate. OutputMin , OutputMax : The output value will be between these two values. Random Node The Random node outputs a random number for the current location. The result is deterministic for a given position and seed value, however, contrary to the Perlin noise node, the values for locations right next to each other have no correlation. This type of noise can be used to add variation between plants, where there is no shared reason for the variation (such as soil quality). Note: Properties such as object scale and color variation CAN be controlled through input pins and thus random values can be passed in, however, if no such input is provided, they will already vary randomly between their allowed values. Unless more control is needed, it is therefore not necessary to add and connect a Random node. Node Properties Seed : A seed value for the random number generator. If a fixed seed is chosen, the random number output is always exactly the same. OutputMin , OutputMax : The output value will be between these two values. See Also Procedural Object Placement ProcGen Graph Input Nodes (TODO)","title":"ProcGen Graph Math Nodes"},{"location":"terrain/procedural/procgen-graph-math/#procgen-graph-math-nodes","text":"These node types implement math functions for ProcGen graph assets .","title":"ProcGen Graph Math Nodes"},{"location":"terrain/procedural/procgen-graph-math/#blend-node","text":"The Blend node provides the most common math expressions to combine two values.","title":"Blend Node"},{"location":"terrain/procedural/procgen-graph-math/#node-properties","text":"Mode : Determines how the two input values get combined. InputA , InputB : Fallback values for pins A and B respectively, in case one of the pins isn't connected. Use this, in case you want to combine one value with a constant (e.g. to multiply A by two, leave B disconnected and set InputB to 2 ). ClampOutput : If this is enabled, the output value is clamped to [0;1] range.","title":"Node Properties"},{"location":"terrain/procedural/procgen-graph-math/#perlin-noise-node","text":"The Perlin Noise node outputs a Perlin Noise value for the current location. This value can be used to add variety, however, not through completely random values, but rather ones that gradually change. So for instance, if the quality of soil varies, the look of vegetation may be different. However, bushes that grow next to each other are affected the same way, so their look will be similar. In the two images below, the same plants are placed. However, in the second image, Perlin noise is used to affect their color. Note how in the second image all plants in an area become darker or brighter.","title":"Perlin Noise Node"},{"location":"terrain/procedural/procgen-graph-math/#node-properties_1","text":"Scale : Over which area the noise is stretched. Large values mean that the noise value changes slowly over large distances, whereas smaller values result in higher frequency noise. If the value is too small for the used object density, the results lose their gradually changing quality. Offset : Pushes the values along the respective axis. NumOctaves : How many Perlin noise values to combine for the final result. More octaves give a more varied pattern and can be scaled across a larger area without showing obvious repetitions, but also cost more performance to evaluate. OutputMin , OutputMax : The output value will be between these two values.","title":"Node Properties"},{"location":"terrain/procedural/procgen-graph-math/#random-node","text":"The Random node outputs a random number for the current location. The result is deterministic for a given position and seed value, however, contrary to the Perlin noise node, the values for locations right next to each other have no correlation. This type of noise can be used to add variation between plants, where there is no shared reason for the variation (such as soil quality). Note: Properties such as object scale and color variation CAN be controlled through input pins and thus random values can be passed in, however, if no such input is provided, they will already vary randomly between their allowed values. Unless more control is needed, it is therefore not necessary to add and connect a Random node.","title":"Random Node"},{"location":"terrain/procedural/procgen-graph-math/#node-properties_2","text":"Seed : A seed value for the random number generator. If a fixed seed is chosen, the random number output is always exactly the same. OutputMin , OutputMax : The output value will be between these two values.","title":"Node Properties"},{"location":"terrain/procedural/procgen-graph-math/#see-also","text":"Procedural Object Placement ProcGen Graph Input Nodes (TODO)","title":"See Also"},{"location":"terrain/procedural/procgen-graph-modifiers/","text":"ProcGen Graph Modifier Nodes The output that the rules in a ProcGen graph produce, mostly depends on the terrain on which it is applied. Flat plains, steep cliffs and different surface types determine where which kind of vegetation grows. However, often it is necessary to have some more control. By itself, the rules rarely create a clearing in a forest, and even if they do, it is hard to control where it is and how it looks like. The procedural nature of the system takes away control from the level designers. To give this control back, without reintroducing the need for lots of manual work, the system allows you to place modifiers in the world, which affect the rules as you like. The modifiers typically are simple volumes such as spheres and boxes . They are tagged to differentiate what they represent. The rule graph can evaluate whether a location is influenced by certain volumes and change the output accordingly. What the system does with this additional information is up to the person who sets up the rules. You can create volumes that modify the density of specific plants down to zero, meaning that it suppresses their placement. Or a volume may affect the color, size or other parameter. Example The following graph has a single placement output node, which places a single object type. However, both the Density and the ColorIndex inputs are connected to a modifier node. The default value for each input is configured on those nodes. In this case, the default density is 0 , meaning that without an extra modifier volume in the scene, this type of plant won't be placed, at all. The placement node also has a color gradient from which the color of each plant is selected. The default value makes the plant green, but by providing a different ColorIndex , it can be shifted towards red/brown. The image below shows how we can use this. There are two sphere volumes in our scene. One is tagged to be picked up by the node that feeds into the Density input, the other is tagged to be picked up by the ColorIndex modifier node. The sphere on the left overrides the Density to be 1 and thus inside its area of influence, vegetation appears. The sphere on the right overrides the ColorIndex towards red/brown, thus plants inside its volume have a different color. Note that the rules in the graph are not tied to specific volumes in a scene. Rather the modifier nodes use tags to filter which volumes are considered as their inputs. Thus you can use as many volumes as you like to locally control the rule output. In the image below, the same rules are applied, but now there are three spheres in the scene that override Density and another one for color: ApplyVolumes Node The ApplyVolumes node looks up the scene for ProcGen volume components at each location where a plant shall be placed. It uses tags to filter volumes. Thus you need to set up different tags to have volumes affect the placement rules in different ways. The node then takes its own value (either provided through the In pin or the InputValue property) and modifies it with each value provided by local volumes, according to their BlendMode setting. If multiple volumes are found for the same location, their SortOrder and their overall size are used to decide in which order their values are applied. This way a smaller volume typically takes precedence over larger volumes, but using the SortOrder you can force a desired priority. If a volume additionally uses an image (such as the volume image component ) to provide detailed data, the ImageVolumeMode is used to determine either which channel (red, green, blue or alpha) should be considered, or whether a specific color represents whether the volume should be applied. Choosing a channel means that you can only control four different things, but you have smooth values ([0; 1] range) to work with. So a channel could control how strongly to tint a color or how think a density should be. When using a reference color instead, you can have many more inputs, but each one can only be on or off and they can't overlap. This is useful to tag areas of a type. So for instance brown could represent \"swamps\", green \"forests\", blue \"water\", grey \"roads\" and so on. Such general information about the area type can then be used to decide which types of plants to place. Node Properties IncludeTags : These tags control which volumes are considered. If your volumes seem to have no effect, make sure this tag is set correctly both on the node and on the game object to which the volume component is attached. Set up different tags to differentiate what volume shall affect what parameter. InputValue : If no value is provided through the In input pin, this is the default value to use. The final output Value is determined by taking this value and combining it with all the values of the volumes, using their individual BlendMode . The SortOrder on the volumes controls in which order the values are combined. If the sort order values are equal, larger volumes are applied first and then smaller volumes. This way a more local volume have 'the last word'. ImageVolumeMode : If a location is modified by an image volume , this mode specifies how the image data is used. ReferenceColor : The color in the image is compared with RefColor . If the colors roughly match, the volume takes effect, otherwise it is as if the volume wasn't present, at all. Red/Green/Blue/Alpha Channel : The volume always has an effect, but its Value is additionally multiplied with the value from the chosen channel of the image. RefColor : If ImageVolumeMode is set to ReferenceColor , this volume only has an effect, if the image at the sampled location (roughly) matches this color. See Also Procedural Object Placement Procedural Volume Box Component Procedural Volume Sphere Component Procedural Volume Image Component","title":"ProcGen Graph Modifier Nodes"},{"location":"terrain/procedural/procgen-graph-modifiers/#procgen-graph-modifier-nodes","text":"The output that the rules in a ProcGen graph produce, mostly depends on the terrain on which it is applied. Flat plains, steep cliffs and different surface types determine where which kind of vegetation grows. However, often it is necessary to have some more control. By itself, the rules rarely create a clearing in a forest, and even if they do, it is hard to control where it is and how it looks like. The procedural nature of the system takes away control from the level designers. To give this control back, without reintroducing the need for lots of manual work, the system allows you to place modifiers in the world, which affect the rules as you like. The modifiers typically are simple volumes such as spheres and boxes . They are tagged to differentiate what they represent. The rule graph can evaluate whether a location is influenced by certain volumes and change the output accordingly. What the system does with this additional information is up to the person who sets up the rules. You can create volumes that modify the density of specific plants down to zero, meaning that it suppresses their placement. Or a volume may affect the color, size or other parameter.","title":"ProcGen Graph Modifier Nodes"},{"location":"terrain/procedural/procgen-graph-modifiers/#example","text":"The following graph has a single placement output node, which places a single object type. However, both the Density and the ColorIndex inputs are connected to a modifier node. The default value for each input is configured on those nodes. In this case, the default density is 0 , meaning that without an extra modifier volume in the scene, this type of plant won't be placed, at all. The placement node also has a color gradient from which the color of each plant is selected. The default value makes the plant green, but by providing a different ColorIndex , it can be shifted towards red/brown. The image below shows how we can use this. There are two sphere volumes in our scene. One is tagged to be picked up by the node that feeds into the Density input, the other is tagged to be picked up by the ColorIndex modifier node. The sphere on the left overrides the Density to be 1 and thus inside its area of influence, vegetation appears. The sphere on the right overrides the ColorIndex towards red/brown, thus plants inside its volume have a different color. Note that the rules in the graph are not tied to specific volumes in a scene. Rather the modifier nodes use tags to filter which volumes are considered as their inputs. Thus you can use as many volumes as you like to locally control the rule output. In the image below, the same rules are applied, but now there are three spheres in the scene that override Density and another one for color:","title":"Example"},{"location":"terrain/procedural/procgen-graph-modifiers/#applyvolumes-node","text":"The ApplyVolumes node looks up the scene for ProcGen volume components at each location where a plant shall be placed. It uses tags to filter volumes. Thus you need to set up different tags to have volumes affect the placement rules in different ways. The node then takes its own value (either provided through the In pin or the InputValue property) and modifies it with each value provided by local volumes, according to their BlendMode setting. If multiple volumes are found for the same location, their SortOrder and their overall size are used to decide in which order their values are applied. This way a smaller volume typically takes precedence over larger volumes, but using the SortOrder you can force a desired priority. If a volume additionally uses an image (such as the volume image component ) to provide detailed data, the ImageVolumeMode is used to determine either which channel (red, green, blue or alpha) should be considered, or whether a specific color represents whether the volume should be applied. Choosing a channel means that you can only control four different things, but you have smooth values ([0; 1] range) to work with. So a channel could control how strongly to tint a color or how think a density should be. When using a reference color instead, you can have many more inputs, but each one can only be on or off and they can't overlap. This is useful to tag areas of a type. So for instance brown could represent \"swamps\", green \"forests\", blue \"water\", grey \"roads\" and so on. Such general information about the area type can then be used to decide which types of plants to place.","title":"ApplyVolumes Node"},{"location":"terrain/procedural/procgen-graph-modifiers/#node-properties","text":"IncludeTags : These tags control which volumes are considered. If your volumes seem to have no effect, make sure this tag is set correctly both on the node and on the game object to which the volume component is attached. Set up different tags to differentiate what volume shall affect what parameter. InputValue : If no value is provided through the In input pin, this is the default value to use. The final output Value is determined by taking this value and combining it with all the values of the volumes, using their individual BlendMode . The SortOrder on the volumes controls in which order the values are combined. If the sort order values are equal, larger volumes are applied first and then smaller volumes. This way a more local volume have 'the last word'. ImageVolumeMode : If a location is modified by an image volume , this mode specifies how the image data is used. ReferenceColor : The color in the image is compared with RefColor . If the colors roughly match, the volume takes effect, otherwise it is as if the volume wasn't present, at all. Red/Green/Blue/Alpha Channel : The volume always has an effect, but its Value is additionally multiplied with the value from the chosen channel of the image. RefColor : If ImageVolumeMode is set to ReferenceColor , this volume only has an effect, if the image at the sampled location (roughly) matches this color.","title":"Node Properties"},{"location":"terrain/procedural/procgen-graph-modifiers/#see-also","text":"Procedural Object Placement Procedural Volume Box Component Procedural Volume Sphere Component Procedural Volume Image Component","title":"See Also"},{"location":"terrain/procedural/procgen-graph-output-placement/","text":"ProcGen Graph Placement Output The Placement Output node is at the heart of the procedural placement system. This node specifies which prefabs to spawn, what density to use, how to position each object and how to add variety. A ProcGen graph can already work, even if it contains just a single of these nodes. However, you can add as many of these nodes as you like. Every placement node represents a different type of object, with different conditions under which they are spawned. For example one node would be used to spawn stones on rocky surfaces, another node spawns bushes on grassy surfaces, a third spawns trees and so on. Every type of object has a footprint , meaning an certain size of area in which only one of them will be placed. The larger the footprint, the more sparse these objects are spawned. The node itself can already filter whether to place an object, at all, by inspecting the surface . This way vegetation isn't planted on concrete or water. More elaborate filtering can be achieved with modifier nodes . For example an image can represent where exactly what type of object should appear. Node Properties Name : A custom display name that is shown in the node's title bar. This has no function other than to make it easier to find in the graph. Objects : A list of prefabs . When an object gets placed, which one to use is either chosen through the ObjectIndex pin, or randomly, if the pin is not connected. Footprint : The radius (in meters) of the circle in which a single object gets placed. Increase the footprint to make object placement more sparse, decrease it to make it more dense. MinOffset , MaxOffset : How much the position of the placed object may randomly deviate from the center position. If this is large enough and the footprint is too small, neighboring objects may end up overlapping. It is common to set MinOffset to -MaxOffset . If an object may only move upwards, but not downwards, keep MinOffset.z at 0 and only set MaxOffset.z to a positive value. YawRotationSnap : All placed objects are rotated randomly along the up axis. If this is non-zero, the used rotation angles are always a multiple of it. AlignToNormal : How much to blend the placed object up direction towards the underlying surface's slope. If this is zero, the placed object always stands upright, regardless of the slope. If it is set to one, the placed object will fully follow the slope of the terrain. Anything in between, the object bends more or less towards the slope. MinScale , MaxScale : How large the placed object is at least ( MinScale ) and at most ( MaxScale ). The final scale is a blend between min and max, so if both values are uniform (x = y = z), the object's scale is also always uniform. ColorGradient : An optional color gradient from which a random color is chosen to set the mesh color (in fact a plMsgSetColor is sent when the object is spawned, so this could also be used to implement other features). If the ColorIndex pin is connected, the incoming value in the [0;1] range is used instead to lookup the color from the gradient. CullDistance : At what distance to start spawning this type of object. For large objects with a low density, this distance can be increased. Small objects with high density should use a small distance, to prevent performance issues. PlacementMode : How to determine the location where to place objects. Raycast: In this mode a physics ray is cast downwards from the volume of the placement component . Using the CollisionLayer and Surface as filters, the closest intersection point is used. Fixed: In this mode objects are always placed at the height of the placement component . No ray is cast, and no location is filtered out. This can be used for 2D games where no collision geometry exists. Custom filtering can still be achieved through image volumes and other modifiers . CollisionLayer : The collision layer to use when PlacementMode is set to Raycast . The collision layer decides which physical objects will be hit by the raycast and thus on which surfaces objects may get spawned at all. Note that in Raycast mode objects can only be placed, if a collision meshes exist in the scene at all ( greyboxes and heightfields set these up automatically). Surface : An optional surface that's used to filter object placement. Objects will only be placed on surfaces of this type (or derived). This is used to only plant certain vegetation on each type of ground. Filtering by surface can also be very useful to prevent procedural objects from getting spawned on top or inside of other procedural objects. The left image shows gras being placed on top of the procedurally placed rocks, in the right image a surface filter prevents this from happening: Input Pins Density : A value in [0;1] range that determines how likely it is that an object gets spawned. A lower density means that fewer objects get spawned. If this pin is not connected, a default density of 1 is assumed. Connect this pin to a Perlin noise node or an modifier node to vary density by location. The image below shows varying density using Perlin noise. Scale : A value in [0;1] range to blend between MinScale and MaxScale . This can be used to scale objects up or down by location or other environmental influences. If this pin is not connected, every object uses a random scale factor. The image below shows varying scale using Perlin noise. ColorIndex : If a ColorGradient has been set, connecting this pin allows you to decide which color to use for an object. The value is in [0;1] range, with 0 selecting the left-most color from the gradient and 1 the right-most color. The image below shows varying color tint using Perlin noise. ObjectIndex : If more than one prefab is added to the object list, this pin can be used to select a specific one. The value is in normalized [0;1] range, so if two objects are in the list, the first one is selected by any value between 0 and 0.5 and the second by values between 0.5 and 1 . The image below shows how varying prefabs are picked by Perlin noise. See Also Procedural Object Placement Procedural Placement Component ProcGen Graph Asset","title":"ProcGen Graph Placement Output"},{"location":"terrain/procedural/procgen-graph-output-placement/#procgen-graph-placement-output","text":"The Placement Output node is at the heart of the procedural placement system. This node specifies which prefabs to spawn, what density to use, how to position each object and how to add variety. A ProcGen graph can already work, even if it contains just a single of these nodes. However, you can add as many of these nodes as you like. Every placement node represents a different type of object, with different conditions under which they are spawned. For example one node would be used to spawn stones on rocky surfaces, another node spawns bushes on grassy surfaces, a third spawns trees and so on. Every type of object has a footprint , meaning an certain size of area in which only one of them will be placed. The larger the footprint, the more sparse these objects are spawned. The node itself can already filter whether to place an object, at all, by inspecting the surface . This way vegetation isn't planted on concrete or water. More elaborate filtering can be achieved with modifier nodes . For example an image can represent where exactly what type of object should appear.","title":"ProcGen Graph Placement Output"},{"location":"terrain/procedural/procgen-graph-output-placement/#node-properties","text":"Name : A custom display name that is shown in the node's title bar. This has no function other than to make it easier to find in the graph. Objects : A list of prefabs . When an object gets placed, which one to use is either chosen through the ObjectIndex pin, or randomly, if the pin is not connected. Footprint : The radius (in meters) of the circle in which a single object gets placed. Increase the footprint to make object placement more sparse, decrease it to make it more dense. MinOffset , MaxOffset : How much the position of the placed object may randomly deviate from the center position. If this is large enough and the footprint is too small, neighboring objects may end up overlapping. It is common to set MinOffset to -MaxOffset . If an object may only move upwards, but not downwards, keep MinOffset.z at 0 and only set MaxOffset.z to a positive value. YawRotationSnap : All placed objects are rotated randomly along the up axis. If this is non-zero, the used rotation angles are always a multiple of it. AlignToNormal : How much to blend the placed object up direction towards the underlying surface's slope. If this is zero, the placed object always stands upright, regardless of the slope. If it is set to one, the placed object will fully follow the slope of the terrain. Anything in between, the object bends more or less towards the slope. MinScale , MaxScale : How large the placed object is at least ( MinScale ) and at most ( MaxScale ). The final scale is a blend between min and max, so if both values are uniform (x = y = z), the object's scale is also always uniform. ColorGradient : An optional color gradient from which a random color is chosen to set the mesh color (in fact a plMsgSetColor is sent when the object is spawned, so this could also be used to implement other features). If the ColorIndex pin is connected, the incoming value in the [0;1] range is used instead to lookup the color from the gradient. CullDistance : At what distance to start spawning this type of object. For large objects with a low density, this distance can be increased. Small objects with high density should use a small distance, to prevent performance issues. PlacementMode : How to determine the location where to place objects. Raycast: In this mode a physics ray is cast downwards from the volume of the placement component . Using the CollisionLayer and Surface as filters, the closest intersection point is used. Fixed: In this mode objects are always placed at the height of the placement component . No ray is cast, and no location is filtered out. This can be used for 2D games where no collision geometry exists. Custom filtering can still be achieved through image volumes and other modifiers . CollisionLayer : The collision layer to use when PlacementMode is set to Raycast . The collision layer decides which physical objects will be hit by the raycast and thus on which surfaces objects may get spawned at all. Note that in Raycast mode objects can only be placed, if a collision meshes exist in the scene at all ( greyboxes and heightfields set these up automatically). Surface : An optional surface that's used to filter object placement. Objects will only be placed on surfaces of this type (or derived). This is used to only plant certain vegetation on each type of ground. Filtering by surface can also be very useful to prevent procedural objects from getting spawned on top or inside of other procedural objects. The left image shows gras being placed on top of the procedurally placed rocks, in the right image a surface filter prevents this from happening:","title":"Node Properties"},{"location":"terrain/procedural/procgen-graph-output-placement/#input-pins","text":"Density : A value in [0;1] range that determines how likely it is that an object gets spawned. A lower density means that fewer objects get spawned. If this pin is not connected, a default density of 1 is assumed. Connect this pin to a Perlin noise node or an modifier node to vary density by location. The image below shows varying density using Perlin noise. Scale : A value in [0;1] range to blend between MinScale and MaxScale . This can be used to scale objects up or down by location or other environmental influences. If this pin is not connected, every object uses a random scale factor. The image below shows varying scale using Perlin noise. ColorIndex : If a ColorGradient has been set, connecting this pin allows you to decide which color to use for an object. The value is in [0;1] range, with 0 selecting the left-most color from the gradient and 1 the right-most color. The image below shows varying color tint using Perlin noise. ObjectIndex : If more than one prefab is added to the object list, this pin can be used to select a specific one. The value is in normalized [0;1] range, so if two objects are in the list, the first one is selected by any value between 0 and 0.5 and the second by values between 0.5 and 1 . The image below shows how varying prefabs are picked by Perlin noise.","title":"Input Pins"},{"location":"terrain/procedural/procgen-graph-output-placement/#see-also","text":"Procedural Object Placement Procedural Placement Component ProcGen Graph Asset","title":"See Also"},{"location":"terrain/procedural/procgen-graph-output-vertexcolor/","text":"ProcGen Graph Vertex Color Output Node Properties Name : See Also Procedural Object Placement ProcGen Graph Vertex Color Output (TODO) ProcGen Graph Asset","title":"ProcGen Graph Vertex Color Output"},{"location":"terrain/procedural/procgen-graph-output-vertexcolor/#procgen-graph-vertex-color-output","text":"","title":"ProcGen Graph Vertex Color Output"},{"location":"terrain/procedural/procgen-graph-output-vertexcolor/#node-properties","text":"Name :","title":"Node Properties"},{"location":"terrain/procedural/procgen-graph-output-vertexcolor/#see-also","text":"Procedural Object Placement ProcGen Graph Vertex Color Output (TODO) ProcGen Graph Asset","title":"See Also"},{"location":"terrain/procedural/procgen-placement-component/","text":"Procedural Placement Component The procedural placement component is used to apply the rules that are defined by a ProcGen graph asset within a defined volume of space. The most common use case is to procedurally place vegetation. The main purpose of the component is to define the region where the procedural rules are to be evaluated. It is not intended to move the volume around at runtime. Rather, the system already automatically takes care of only placing objects in the vicinity of the camera, and delete objects that have are too far away. Component Properties Resource : The ProcGen graph asset to use for deciding where to place which type of object. Extents : The size of the volume in which to evaluate the placement rules. This has to overlap with the terrain geometry where objects shall be placed. See Also Procedural Object Placement ProcGen Graph Placement Output","title":"Procedural Placement Component"},{"location":"terrain/procedural/procgen-placement-component/#procedural-placement-component","text":"The procedural placement component is used to apply the rules that are defined by a ProcGen graph asset within a defined volume of space. The most common use case is to procedurally place vegetation. The main purpose of the component is to define the region where the procedural rules are to be evaluated. It is not intended to move the volume around at runtime. Rather, the system already automatically takes care of only placing objects in the vicinity of the camera, and delete objects that have are too far away.","title":"Procedural Placement Component"},{"location":"terrain/procedural/procgen-placement-component/#component-properties","text":"Resource : The ProcGen graph asset to use for deciding where to place which type of object. Extents : The size of the volume in which to evaluate the placement rules. This has to overlap with the terrain geometry where objects shall be placed.","title":"Component Properties"},{"location":"terrain/procedural/procgen-placement-component/#see-also","text":"Procedural Object Placement ProcGen Graph Placement Output","title":"See Also"},{"location":"terrain/procedural/procgen-vertex-color-component/","text":"Procedural Vertex Color Component Component Properties Mesh : Color : Materials : Resource : Output Descriptions : See Also Procedural Object Placement ProcGen Graph Vertex Color Output (TODO)","title":"Procedural Vertex Color Component"},{"location":"terrain/procedural/procgen-vertex-color-component/#procedural-vertex-color-component","text":"","title":"Procedural Vertex Color Component"},{"location":"terrain/procedural/procgen-vertex-color-component/#component-properties","text":"Mesh : Color : Materials : Resource : Output Descriptions :","title":"Component Properties"},{"location":"terrain/procedural/procgen-vertex-color-component/#see-also","text":"Procedural Object Placement ProcGen Graph Vertex Color Output (TODO)","title":"See Also"},{"location":"terrain/procedural/procgen-volume-box-component/","text":"Procedural Volume Box Component The procedural volume box component defines a box shaped volume in which the rules of ProcGen graphs are modified. Not every graph has to make use of this information, and what the exact effect is, is up to the ProcGen graph. For more details see the chapter on ProcGen graph modifier nodes . The image below shows a box volume used to locally increase the scale of an object type. A fade out value of 0.5 makes the transition soft. Component Properties Value : A single number value. This is combined with the InputValue from the modifier node in the graph, using the BlendMode formula. SortOrder : If multiple modifier volumes overlap, the SortOrder can be used to control in which order the volumes are evaluated. BlendMode : How to combine Value with the InputValue from the modifier node in the graph. The Set mode just sets the result to Value and ignores the other operand. Extents : The size of the box volume in which the modifier is active. FadeOutStart : The influence of the volume can fade out towards its edges, for smooth transitions. This value controls for every axis, at what distance from the center point the fade out starts. So if this is set to zero, the fade out starts immediately at the middle (towards the closest edge), whereas if it is set to one, there will be no fade out, at all, and rather the influence of the volume stops abruptly at its border. See Also Procedural Object Placement ProcGen Graph Modifier Nodes","title":"Procedural Volume Box Component"},{"location":"terrain/procedural/procgen-volume-box-component/#procedural-volume-box-component","text":"The procedural volume box component defines a box shaped volume in which the rules of ProcGen graphs are modified. Not every graph has to make use of this information, and what the exact effect is, is up to the ProcGen graph. For more details see the chapter on ProcGen graph modifier nodes . The image below shows a box volume used to locally increase the scale of an object type. A fade out value of 0.5 makes the transition soft.","title":"Procedural Volume Box Component"},{"location":"terrain/procedural/procgen-volume-box-component/#component-properties","text":"Value : A single number value. This is combined with the InputValue from the modifier node in the graph, using the BlendMode formula. SortOrder : If multiple modifier volumes overlap, the SortOrder can be used to control in which order the volumes are evaluated. BlendMode : How to combine Value with the InputValue from the modifier node in the graph. The Set mode just sets the result to Value and ignores the other operand. Extents : The size of the box volume in which the modifier is active. FadeOutStart : The influence of the volume can fade out towards its edges, for smooth transitions. This value controls for every axis, at what distance from the center point the fade out starts. So if this is set to zero, the fade out starts immediately at the middle (towards the closest edge), whereas if it is set to one, there will be no fade out, at all, and rather the influence of the volume stops abruptly at its border.","title":"Component Properties"},{"location":"terrain/procedural/procgen-volume-box-component/#see-also","text":"Procedural Object Placement ProcGen Graph Modifier Nodes","title":"See Also"},{"location":"terrain/procedural/procgen-volume-image-component/","text":"Procedural Volume Image Component The procedural volume image component defines a box shaped volume in which the rules of ProcGen graphs are modified. Not every graph has to make use of this information, and what the exact effect is, is up to the ProcGen graph. For more details see the chapter on ProcGen graph modifier nodes . In contrast to the procedural volume box component , this component additionally samples an ImageData asset , such that it can provide fine grained information for every location within the box. This can be utilized to, for example, use a mask of the terrain that defines where roads are, where which type of plant shall grow and so on, to describe precisely how the procedural placement shall look like. The image below shows an image volume used to only place the vegetation on the darker terrain patches. Component Properties Value : A single number value. This is combined with the InputValue from the modifier node in the graph, using the BlendMode formula. SortOrder : If multiple modifier volumes overlap, the SortOrder can be used to control in which order the volumes are evaluated. BlendMode : How to combine Value with the InputValue from the modifier node in the graph. The Set mode just sets the result to Value and ignores the other operand. Extents : The size of the box volume in which the modifier is active. FadeOutStart : The influence of the volume can fade out towards its edges, for smooth transitions. This value controls for every axis, at what distance from the center point the fade out starts. So if this is set to zero, the fade out starts immediately at the middle (towards the closest edge), whereas if it is set to one, there will be no fade out, at all, and rather the influence of the volume stops abruptly at its border. Image : The ImageData asset that is used as the reference data. See Also Procedural Object Placement ProcGen Graph Modifier Nodes","title":"Procedural Volume Image Component"},{"location":"terrain/procedural/procgen-volume-image-component/#procedural-volume-image-component","text":"The procedural volume image component defines a box shaped volume in which the rules of ProcGen graphs are modified. Not every graph has to make use of this information, and what the exact effect is, is up to the ProcGen graph. For more details see the chapter on ProcGen graph modifier nodes . In contrast to the procedural volume box component , this component additionally samples an ImageData asset , such that it can provide fine grained information for every location within the box. This can be utilized to, for example, use a mask of the terrain that defines where roads are, where which type of plant shall grow and so on, to describe precisely how the procedural placement shall look like. The image below shows an image volume used to only place the vegetation on the darker terrain patches.","title":"Procedural Volume Image Component"},{"location":"terrain/procedural/procgen-volume-image-component/#component-properties","text":"Value : A single number value. This is combined with the InputValue from the modifier node in the graph, using the BlendMode formula. SortOrder : If multiple modifier volumes overlap, the SortOrder can be used to control in which order the volumes are evaluated. BlendMode : How to combine Value with the InputValue from the modifier node in the graph. The Set mode just sets the result to Value and ignores the other operand. Extents : The size of the box volume in which the modifier is active. FadeOutStart : The influence of the volume can fade out towards its edges, for smooth transitions. This value controls for every axis, at what distance from the center point the fade out starts. So if this is set to zero, the fade out starts immediately at the middle (towards the closest edge), whereas if it is set to one, there will be no fade out, at all, and rather the influence of the volume stops abruptly at its border. Image : The ImageData asset that is used as the reference data.","title":"Component Properties"},{"location":"terrain/procedural/procgen-volume-image-component/#see-also","text":"Procedural Object Placement ProcGen Graph Modifier Nodes","title":"See Also"},{"location":"terrain/procedural/procgen-volume-sphere-component/","text":"Procedural Volume Sphere Component The procedural volume sphere component defines a sphere shaped volume in which the rules of ProcGen graphs are modified. Not every graph has to make use of this information, and what the exact effect is, is up to the ProcGen graph. For more details see the chapter on ProcGen graph modifier nodes . The image below shows a sphere volume used to set the density of an object type locally to zero. A fade out value of 0.5 makes the transition soft. Component Properties Value : A single number value. This is combined with the InputValue from the modifier node in the graph, using the BlendMode formula. SortOrder : If multiple modifier volumes overlap, the SortOrder can be used to control in which order the volumes are evaluated. BlendMode : How to combine Value with the InputValue from the modifier node in the graph. The Set mode just sets the result to Value and ignores the other operand. Radius : The radius of the sphere volume in which the modifier is active. FadeOutStart : The influence of the volume can fade out towards its edges, for smooth transitions. This value controls at what distance from the center point the fade out starts. So if this is set to zero, the fade out starts immediately at the middle (towards the closest edge), whereas if it is set to one, there will be no fade out, at all, and rather the influence of the volume stops abruptly at its border. See Also Procedural Object Placement ProcGen Graph Modifier Nodes","title":"Procedural Volume Sphere Component"},{"location":"terrain/procedural/procgen-volume-sphere-component/#procedural-volume-sphere-component","text":"The procedural volume sphere component defines a sphere shaped volume in which the rules of ProcGen graphs are modified. Not every graph has to make use of this information, and what the exact effect is, is up to the ProcGen graph. For more details see the chapter on ProcGen graph modifier nodes . The image below shows a sphere volume used to set the density of an object type locally to zero. A fade out value of 0.5 makes the transition soft.","title":"Procedural Volume Sphere Component"},{"location":"terrain/procedural/procgen-volume-sphere-component/#component-properties","text":"Value : A single number value. This is combined with the InputValue from the modifier node in the graph, using the BlendMode formula. SortOrder : If multiple modifier volumes overlap, the SortOrder can be used to control in which order the volumes are evaluated. BlendMode : How to combine Value with the InputValue from the modifier node in the graph. The Set mode just sets the result to Value and ignores the other operand. Radius : The radius of the sphere volume in which the modifier is active. FadeOutStart : The influence of the volume can fade out towards its edges, for smooth transitions. This value controls at what distance from the center point the fade out starts. So if this is set to zero, the fade out starts immediately at the middle (towards the closest edge), whereas if it is set to one, there will be no fade out, at all, and rather the influence of the volume stops abruptly at its border.","title":"Component Properties"},{"location":"terrain/procedural/procgen-volume-sphere-component/#see-also","text":"Procedural Object Placement ProcGen Graph Modifier Nodes","title":"See Also"},{"location":"tools/archivetool/","text":"ArchiveTool The ArchiveTool is used to create or extract .plArchive files. Archives are similar to zip files, they contain all the files in a folder, using compression. plArchive files can be mounted at runtime as data directories . plArchive Format The internal structure of plArchives is optimized to make mounting as a data directory extremely efficient. The files are memory mapped and file lookups are faster than for regular folders. Each file in the archive may use compression or not, depending on whether it would make sense for the particular file. Different compression algorithms are possible, though the main compression used is zstd which yields good compression and is extremely fast to decode. Usage The ArchiveTool is a command line tool. Default Usage The most convenient way to use it, is to pass a single path as the only argument: ArchiveTool.exe C:/my/data When the path points to a folder, it will compress the folder and store the plArchive file next to it. In the example above: C:/my/data.plArchive ArchiveTool.exe C:/your/data.plArchive When the path points to an existing archive, the tool will extract the data to a folder next to the file. In the example above: C:/your/data All Options The following options allow you to be more specific: -pack \"path/to/folder\" \"path/to/another/folder\" ... -unpack \"path/to/file.plArchive\" \"another/file.plArchive\" -out \"path/to/file/or/folder\" -pack and -unpack can take multiple inputs to either aggregate multiple folders into one archive (pack) or to unpack multiple archives at the same time. -out specifies the target to pack or unpack things to. For packing mode it has to be a file. The file will be overwritten, if it already exists. For unpacking the target should be a folder (may or may not exist) into which the archives get extracted. If no -out is specified, it is determined to be where the input file is located. If neither -pack nor -unpack is specified, the mode is detected automatically from the list of inputs: If all inputs are folders, mode is going to be 'pack'. If all inputs are files, mode is going to be 'unpack'. Examples Pack all data in \"C:\\Stuff\" into \"C:\\Stuff.plArchive\": ArchiveTool.exe \"C:\\Stuff\" Pack all data in \"C:\\Stuff\" into \"C:\\MyStuff.plArchive\": ArchiveTool.exe \"C:\\Stuff\" -out \"C:\\MyStuff.plArchive\" Unpack all data from the archive into \"C:\\Stuff\" ArchiveTool.exe \"C:\\Stuff.plArchive\" Unpack all data from the archive into \"C:\\MyStuff\" ArchiveTool.exe \"C:\\Stuff.plArchive\" -out \"C:\\MyStuff\" See Also Data Directories FileSystem","title":"ArchiveTool"},{"location":"tools/archivetool/#archivetool","text":"The ArchiveTool is used to create or extract .plArchive files. Archives are similar to zip files, they contain all the files in a folder, using compression. plArchive files can be mounted at runtime as data directories .","title":"ArchiveTool"},{"location":"tools/archivetool/#plarchive-format","text":"The internal structure of plArchives is optimized to make mounting as a data directory extremely efficient. The files are memory mapped and file lookups are faster than for regular folders. Each file in the archive may use compression or not, depending on whether it would make sense for the particular file. Different compression algorithms are possible, though the main compression used is zstd which yields good compression and is extremely fast to decode.","title":"plArchive Format"},{"location":"tools/archivetool/#usage","text":"The ArchiveTool is a command line tool.","title":"Usage"},{"location":"tools/archivetool/#default-usage","text":"The most convenient way to use it, is to pass a single path as the only argument: ArchiveTool.exe C:/my/data When the path points to a folder, it will compress the folder and store the plArchive file next to it. In the example above: C:/my/data.plArchive ArchiveTool.exe C:/your/data.plArchive When the path points to an existing archive, the tool will extract the data to a folder next to the file. In the example above: C:/your/data","title":"Default Usage"},{"location":"tools/archivetool/#all-options","text":"The following options allow you to be more specific: -pack \"path/to/folder\" \"path/to/another/folder\" ... -unpack \"path/to/file.plArchive\" \"another/file.plArchive\" -out \"path/to/file/or/folder\" -pack and -unpack can take multiple inputs to either aggregate multiple folders into one archive (pack) or to unpack multiple archives at the same time. -out specifies the target to pack or unpack things to. For packing mode it has to be a file. The file will be overwritten, if it already exists. For unpacking the target should be a folder (may or may not exist) into which the archives get extracted. If no -out is specified, it is determined to be where the input file is located. If neither -pack nor -unpack is specified, the mode is detected automatically from the list of inputs: If all inputs are folders, mode is going to be 'pack'. If all inputs are files, mode is going to be 'unpack'.","title":"All Options"},{"location":"tools/archivetool/#examples","text":"Pack all data in \"C:\\Stuff\" into \"C:\\Stuff.plArchive\": ArchiveTool.exe \"C:\\Stuff\" Pack all data in \"C:\\Stuff\" into \"C:\\MyStuff.plArchive\": ArchiveTool.exe \"C:\\Stuff\" -out \"C:\\MyStuff.plArchive\" Unpack all data from the archive into \"C:\\Stuff\" ArchiveTool.exe \"C:\\Stuff.plArchive\" Unpack all data from the archive into \"C:\\MyStuff\" ArchiveTool.exe \"C:\\Stuff.plArchive\" -out \"C:\\MyStuff\"","title":"Examples"},{"location":"tools/archivetool/#see-also","text":"Data Directories FileSystem","title":"See Also"},{"location":"tools/fileserve/","text":"FileServe This is the GUI front-end and the server of the file-serving functionality. It is used to stream project data over a network to a connected (mobile) device. This feature is fully functional, but currently undocumented. See Also FileSystem","title":"FileServe"},{"location":"tools/fileserve/#fileserve","text":"This is the GUI front-end and the server of the file-serving functionality. It is used to stream project data over a network to a connected (mobile) device. This feature is fully functional, but currently undocumented.","title":"FileServe"},{"location":"tools/fileserve/#see-also","text":"FileSystem","title":"See Also"},{"location":"tools/headercheck/","text":"HeaderCheck Tool Types of Header Files The code in Plasma Engine differentiates between two types of header files: Public Header Files : Public header files are header files that can be included by third party. These header files should not leak any implementation details like platform headers. A third party is any library or executable outside of the currently compiled library / executable. For example when plFoundation is compiled, everything else is considered a third party. Internal Header Files : Internal header files may include platform headers and leak implementation detail, but can only be used within a subcomponent of Plasma Engine (for example only inside plFoundation). Using them from outside of the component will cause a compiler error. To mark up a header file as a internal header file, first include the component's internal.h file and then use the component specific macro. The component's internal header file is called ComponentInternal.h and the macro is called PLASMA_COMPONENT_INTERNAL_HEADER . The following example shows how to mark a header file as internal for plFoundation: #include <Foundation/FoundationInternal.h> PLASMA_FOUNDATION_INTERNAL_HEADER The Header Checker Tool The header checker tool will automatically be run by the continues integration to check for leakage of implementation detail. If a leak is found the build will fail. Usually you will see an error message such as: Including 'wrl/wrappers/corewrappers.h' in Plasma/Code/Engine/Foundation/Strings/StringConversion.h:9 leaks underlying implementation detail. Including system or thirdparty headers in public Plasma header files is not allowed. Please use an interface, factory or pimpl to hide the implementation and avoid the include. In this example including wrl/wrappers/corewrappers.h is illegal. This header file is included from Plasma/Code/Engine/Foundation/Strings/StringConversion.h at line 9. To fix these issues follow one of the techniques below to hide implementation details. Hiding Implementation Detail To consider the different options of hiding implementation detail have a look at the following example #include <d3d11.h> class plTexture2D { public: void Bind(); private: ID3D11Texture2D* m_ptr; }; If a user includes this header file, the underlying implementation detail is leaked as the user will need the d3d11.h header in order to compile the code. Furthermore the user might need exactly the same version of the d3d11.h file in order for the code to compile. This is a leaky abstraction. Ideally classes that wrap functionality should not leak any of their implementation details to the user. The following techniques can be used to hide implementation detail. Forward Declarations Forward declarations can be used to remove the need to include a header file, therefor removing the leaky abstraction. Consider the following fixed version of the plTexture2D class: struct ID3D11Texture2D; // Forward declare ID3D11Texture2D class plTexture2D { public: void Bind(); private: ID3D11Texture2D* m_ptr; }; This header is no longer a leaky abstraction as the user is no longer required to have a copy of d3d11.h . Forward declarations can be made for: Class or struct members if they are pointers or references. All types used as arguments to functions. Template arguments if the usage follows the two above rules. Forward declarations can't be made for: Class or struct members that are 'inline' because the compiler needs to know the size and alignment. Base classes. Enums can be forward declared if they are given an explicit storage type. So ideally to make enums forward declarable always manually specify a storage type. enum MyEnum : int; // Forward declaration enum MyEnum : int // declaration { One, Two }; Nested types can never be forward declared. A nested type is a type that is inside a class or struct. // does not work // struct Outer::Inner; struct Outer { struct Inner { int i; }; }; So prefer to put nested types into namespaces instead of structs or classes: // Forward declaration namespace Outer { struct Inner; } // Declaration namespace Outer { struct Inner { int i; }; } Templates can also be forward declared: // forward declaration template<typename> struct Example; // Usage of forward declaration void bar(const Example<int>& arg); // declaration template<typename T> struct Example { T t; }; Advantages: No runtime overhead Disadvantages: Forward declarations and actual declaration have to be kept in sync. Moving Implementation Details Out Of Templates Consider the following example which leaks implementation details: // Application.h #include <roapi.h> template <typename AppClass> void RunApplication(AppClass& app) { RoInitialize(RO_INIT_MULTITHREADED); app.Init(); while(!app.Run()) {} app.DeInit(); RoUninitialize(); } The two functions RoInitialize and RoUninitialize are platform specific functions and require the include roapi.h . We can't move the function into a .cpp because the implementation for templates needs to be known when using them. As a result this template leaks its implementation detail. To fix this issue we need to wrap the leaking function calls into separate functions and forward declare these functions. // Application.h void InitPlatform(); void DeInitPlatform(); template <typename AppClass> void RunApplication(AppClass& app) { InitPlatform(); app.Init(); while(!app.Run()) {} app.DeInit(); DeInitPlatform(); } // Application.cpp #include \"Application.h\" #include <roapi.h> void InitPlatform() { RoInitialize(RO_INIT_MULTITHREADED); } void DeInitPlatform() { RoUninitialize(); } As you can see we removed the include to roapi.h from the header file and moved it into the cpp file. This way our header no longer leaks underlying implementation details, as the user won't see the cpp file when using our library. If considerable parts of the template don't depend on the template arguments this pattern can also be used to reduce code bloat by moving the non dependent parts out into non-templated functions. Pimpl Light The pattern that I call \"Pimpl light\" can be used to hide implementation detail at the cost of an additional allocation: Consider our original plTexture2D example it would be modified like this: // Texture2D.h class plTexture2D { public: plTexture2D(); ~plTexture2D(); void Bind(); private: struct Impl; // forward declaration plUniquePtr<Impl> m_pImpl; }; // Texture2D.cpp #include \"Texture2D.h\" #include <d3d11.h> // Declaration of plTexture2D::Impl struct struct plTexture2D::Impl { ID3D11Texture2D* m_ptr; }; plTexture2D::plTexture2D() : m_pImpl(PLASMA_DEFAULT_NEW(Impl)) { } // all constructors / destructors / assignment operators must be in .cpp file otherwise forward declaration will not work. plTexture2D::~plTexture2D() { } plTexture2D::Bind() { // Use the implementation detail m_pImpl->m_ptr->Bind(); } This is an easy pattern to hide implementation details. Advantages: Simple to implement, hides nasty implementation details well Disadvantages: Additional allocation Additional indirection Pimpl Inheritance The Pimpl pattern can also be implemented by using inheritance instead of a forward declared struct. For our plTexture2D example it would look like this: // Texture2D.h class plTexture2D { public: plUniquePtr<plTexture2D> Make(); // factory function, could also return a shared ptr. virtual ~plTexture2D(); void Bind(); private: plTexture2D(); // All constructors must be private friend class plTexture2DImpl; // This is the only class allowed to derive from plTexture2D }; // Texture2D.cpp #include \"Texture2D.h\" #include <d3d11.h> // Actual implementation class plTexture2DImpl : public plTexture2D { public: plTexture2DImpl() : plTexture2D() {} ~plTexture2DImpl(){} ID3D11Texture2D* m_ptr; }; plTexture2D::plTexture2D() {} plTexture2D::~plTexture2D() {} plUniquePtr<plTexture2D> plTexture2D::Make() { return plUniquePtr<plTexture2D>(PLASMA_DEFAULT_NEW(plTexture2DImpl)); } plTexture2D::Bind() { // Use the implementation detail reinterpret_cast<plTexture2DImpl*>(this)->m_ptr->Bind(); } As you see this version of pimpl hides the implementation detail similar to pimpl light. Advantages: No additional indirection (compared to pimpl light) Disadvantages: Additional allocation Can no longer inherit from plTexture2D plTexture2D can't be final Opaque array of bytes We can also place an opaque array of bytes large enough to store our implementation detail. Considering our plTexture2D example it would look like this: // plTexture2D.h class plTexture2D { public: void Bind(); private: #if PLASMA_ENABLED(PLASMA_PLATFORM_32BIT) struct PLASMA_ALIGN(Impl, 4) { plUInt8 m_Data[4]; }; #else struct PLASMA_ALIGN(Impl, 8) { plUInt8 m_Data[8]; }; #endif Impl m_impl; }; // plTexture2D.cpp #include \"Texture2D.h\" struct plTexture2DImpl { D3D11Texture2D* m_ptr; }; static_assert(sizeof(plTexture2D::Impl) == sizeof(plTexture2DImpl), \"plTexture2D::Impl has incorrect size\"); static_assert(alignof(plTexture2D::Impl) == alignof(plTexture2DImpl), \"plTexture2D::Impl has incorrect alignment\"); void plTexture2D::Bind() { // Use implementation detail reinterpret_cast<plTexture2DImpl*>(&m_impl)->m_ptr->Bind(); } This again hides the implementation details in the header file. Advantages: No runtime overhead Disadvantages: High maintenance burden. Especially if implementation detail size varies on different platforms. Ignore the problem You can choose to ignore the leaky abstraction issue and tell the header checker tool to ignore a certain file to be included or give a certain file the permission to include anything. Each module in PlasmaEngine that uses the header checker has a headerCkeckerIgnore.json file where you can add ignores. It looks like this: { \"includeTarget\" : { \"byName\" : [ \"a.h\" ] }, \"includeSource\" : { \"byName\" : [ \"b.h\" ] } } In the above file every time a.h is included and would generate an error in the header checker tool, that error will be ignored. Every time b.h includes a header file that would cause an error, this error will also be ignored. Advantages: Less work Disadvantages: Longer compile times Conflicts due to global namespace pollution Requires users to have all header files for implementation details available See Also","title":"HeaderCheck Tool"},{"location":"tools/headercheck/#headercheck-tool","text":"","title":"HeaderCheck Tool"},{"location":"tools/headercheck/#types-of-header-files","text":"The code in Plasma Engine differentiates between two types of header files: Public Header Files : Public header files are header files that can be included by third party. These header files should not leak any implementation details like platform headers. A third party is any library or executable outside of the currently compiled library / executable. For example when plFoundation is compiled, everything else is considered a third party. Internal Header Files : Internal header files may include platform headers and leak implementation detail, but can only be used within a subcomponent of Plasma Engine (for example only inside plFoundation). Using them from outside of the component will cause a compiler error. To mark up a header file as a internal header file, first include the component's internal.h file and then use the component specific macro. The component's internal header file is called ComponentInternal.h and the macro is called PLASMA_COMPONENT_INTERNAL_HEADER . The following example shows how to mark a header file as internal for plFoundation: #include <Foundation/FoundationInternal.h> PLASMA_FOUNDATION_INTERNAL_HEADER","title":"Types of Header Files"},{"location":"tools/headercheck/#the-header-checker-tool","text":"The header checker tool will automatically be run by the continues integration to check for leakage of implementation detail. If a leak is found the build will fail. Usually you will see an error message such as: Including 'wrl/wrappers/corewrappers.h' in Plasma/Code/Engine/Foundation/Strings/StringConversion.h:9 leaks underlying implementation detail. Including system or thirdparty headers in public Plasma header files is not allowed. Please use an interface, factory or pimpl to hide the implementation and avoid the include. In this example including wrl/wrappers/corewrappers.h is illegal. This header file is included from Plasma/Code/Engine/Foundation/Strings/StringConversion.h at line 9. To fix these issues follow one of the techniques below to hide implementation details.","title":"The Header Checker Tool"},{"location":"tools/headercheck/#hiding-implementation-detail","text":"To consider the different options of hiding implementation detail have a look at the following example #include <d3d11.h> class plTexture2D { public: void Bind(); private: ID3D11Texture2D* m_ptr; }; If a user includes this header file, the underlying implementation detail is leaked as the user will need the d3d11.h header in order to compile the code. Furthermore the user might need exactly the same version of the d3d11.h file in order for the code to compile. This is a leaky abstraction. Ideally classes that wrap functionality should not leak any of their implementation details to the user. The following techniques can be used to hide implementation detail.","title":"Hiding Implementation Detail"},{"location":"tools/headercheck/#forward-declarations","text":"Forward declarations can be used to remove the need to include a header file, therefor removing the leaky abstraction. Consider the following fixed version of the plTexture2D class: struct ID3D11Texture2D; // Forward declare ID3D11Texture2D class plTexture2D { public: void Bind(); private: ID3D11Texture2D* m_ptr; }; This header is no longer a leaky abstraction as the user is no longer required to have a copy of d3d11.h . Forward declarations can be made for: Class or struct members if they are pointers or references. All types used as arguments to functions. Template arguments if the usage follows the two above rules. Forward declarations can't be made for: Class or struct members that are 'inline' because the compiler needs to know the size and alignment. Base classes. Enums can be forward declared if they are given an explicit storage type. So ideally to make enums forward declarable always manually specify a storage type. enum MyEnum : int; // Forward declaration enum MyEnum : int // declaration { One, Two }; Nested types can never be forward declared. A nested type is a type that is inside a class or struct. // does not work // struct Outer::Inner; struct Outer { struct Inner { int i; }; }; So prefer to put nested types into namespaces instead of structs or classes: // Forward declaration namespace Outer { struct Inner; } // Declaration namespace Outer { struct Inner { int i; }; } Templates can also be forward declared: // forward declaration template<typename> struct Example; // Usage of forward declaration void bar(const Example<int>& arg); // declaration template<typename T> struct Example { T t; }; Advantages: No runtime overhead Disadvantages: Forward declarations and actual declaration have to be kept in sync.","title":"Forward Declarations"},{"location":"tools/headercheck/#moving-implementation-details-out-of-templates","text":"Consider the following example which leaks implementation details: // Application.h #include <roapi.h> template <typename AppClass> void RunApplication(AppClass& app) { RoInitialize(RO_INIT_MULTITHREADED); app.Init(); while(!app.Run()) {} app.DeInit(); RoUninitialize(); } The two functions RoInitialize and RoUninitialize are platform specific functions and require the include roapi.h . We can't move the function into a .cpp because the implementation for templates needs to be known when using them. As a result this template leaks its implementation detail. To fix this issue we need to wrap the leaking function calls into separate functions and forward declare these functions. // Application.h void InitPlatform(); void DeInitPlatform(); template <typename AppClass> void RunApplication(AppClass& app) { InitPlatform(); app.Init(); while(!app.Run()) {} app.DeInit(); DeInitPlatform(); } // Application.cpp #include \"Application.h\" #include <roapi.h> void InitPlatform() { RoInitialize(RO_INIT_MULTITHREADED); } void DeInitPlatform() { RoUninitialize(); } As you can see we removed the include to roapi.h from the header file and moved it into the cpp file. This way our header no longer leaks underlying implementation details, as the user won't see the cpp file when using our library. If considerable parts of the template don't depend on the template arguments this pattern can also be used to reduce code bloat by moving the non dependent parts out into non-templated functions.","title":"Moving Implementation Details Out Of Templates"},{"location":"tools/headercheck/#pimpl-light","text":"The pattern that I call \"Pimpl light\" can be used to hide implementation detail at the cost of an additional allocation: Consider our original plTexture2D example it would be modified like this: // Texture2D.h class plTexture2D { public: plTexture2D(); ~plTexture2D(); void Bind(); private: struct Impl; // forward declaration plUniquePtr<Impl> m_pImpl; }; // Texture2D.cpp #include \"Texture2D.h\" #include <d3d11.h> // Declaration of plTexture2D::Impl struct struct plTexture2D::Impl { ID3D11Texture2D* m_ptr; }; plTexture2D::plTexture2D() : m_pImpl(PLASMA_DEFAULT_NEW(Impl)) { } // all constructors / destructors / assignment operators must be in .cpp file otherwise forward declaration will not work. plTexture2D::~plTexture2D() { } plTexture2D::Bind() { // Use the implementation detail m_pImpl->m_ptr->Bind(); } This is an easy pattern to hide implementation details. Advantages: Simple to implement, hides nasty implementation details well Disadvantages: Additional allocation Additional indirection","title":"Pimpl Light"},{"location":"tools/headercheck/#pimpl-inheritance","text":"The Pimpl pattern can also be implemented by using inheritance instead of a forward declared struct. For our plTexture2D example it would look like this: // Texture2D.h class plTexture2D { public: plUniquePtr<plTexture2D> Make(); // factory function, could also return a shared ptr. virtual ~plTexture2D(); void Bind(); private: plTexture2D(); // All constructors must be private friend class plTexture2DImpl; // This is the only class allowed to derive from plTexture2D }; // Texture2D.cpp #include \"Texture2D.h\" #include <d3d11.h> // Actual implementation class plTexture2DImpl : public plTexture2D { public: plTexture2DImpl() : plTexture2D() {} ~plTexture2DImpl(){} ID3D11Texture2D* m_ptr; }; plTexture2D::plTexture2D() {} plTexture2D::~plTexture2D() {} plUniquePtr<plTexture2D> plTexture2D::Make() { return plUniquePtr<plTexture2D>(PLASMA_DEFAULT_NEW(plTexture2DImpl)); } plTexture2D::Bind() { // Use the implementation detail reinterpret_cast<plTexture2DImpl*>(this)->m_ptr->Bind(); } As you see this version of pimpl hides the implementation detail similar to pimpl light. Advantages: No additional indirection (compared to pimpl light) Disadvantages: Additional allocation Can no longer inherit from plTexture2D plTexture2D can't be final","title":"Pimpl Inheritance"},{"location":"tools/headercheck/#opaque-array-of-bytes","text":"We can also place an opaque array of bytes large enough to store our implementation detail. Considering our plTexture2D example it would look like this: // plTexture2D.h class plTexture2D { public: void Bind(); private: #if PLASMA_ENABLED(PLASMA_PLATFORM_32BIT) struct PLASMA_ALIGN(Impl, 4) { plUInt8 m_Data[4]; }; #else struct PLASMA_ALIGN(Impl, 8) { plUInt8 m_Data[8]; }; #endif Impl m_impl; }; // plTexture2D.cpp #include \"Texture2D.h\" struct plTexture2DImpl { D3D11Texture2D* m_ptr; }; static_assert(sizeof(plTexture2D::Impl) == sizeof(plTexture2DImpl), \"plTexture2D::Impl has incorrect size\"); static_assert(alignof(plTexture2D::Impl) == alignof(plTexture2DImpl), \"plTexture2D::Impl has incorrect alignment\"); void plTexture2D::Bind() { // Use implementation detail reinterpret_cast<plTexture2DImpl*>(&m_impl)->m_ptr->Bind(); } This again hides the implementation details in the header file. Advantages: No runtime overhead Disadvantages: High maintenance burden. Especially if implementation detail size varies on different platforms.","title":"Opaque array of bytes"},{"location":"tools/headercheck/#ignore-the-problem","text":"You can choose to ignore the leaky abstraction issue and tell the header checker tool to ignore a certain file to be included or give a certain file the permission to include anything. Each module in PlasmaEngine that uses the header checker has a headerCkeckerIgnore.json file where you can add ignores. It looks like this: { \"includeTarget\" : { \"byName\" : [ \"a.h\" ] }, \"includeSource\" : { \"byName\" : [ \"b.h\" ] } } In the above file every time a.h is included and would generate an error in the header checker tool, that error will be ignored. Every time b.h includes a header file that would cause an error, this error will also be ignored. Advantages: Less work Disadvantages: Longer compile times Conflicts due to global namespace pollution Requires users to have all header files for implementation details available","title":"Ignore the problem"},{"location":"tools/headercheck/#see-also","text":"","title":"See Also"},{"location":"tools/inspector/","text":"plInspector plInspector is a tool to monitor some internal state of an application. It helps observing how the application operates, which resources it accesses and why it might behave as it does. The current version allows to monitor the following data: Log : The Log panel displays all the log messages. It allows to filter by severity and search by keywords. Memory Usage : The Memory panel displays the number of allocations, the amount of memory in use (per allocator) and a time-line how memory usage changes. Input : Shows which physical buttons are available and what their state is. Also displays the virtual input actions, by which keys they get triggered and what their current state is. Subsystems : Displays information about all the available subsystems in the engine. Plugins : Shows which plugins are loaded and which other plugins they depend on. Global Events : Shows which global events are registered and how often they occur. File Operations : This panel shows which files get accessed by the engine, whether they occur on the main thread, how much data is read or written per operation, how much time that takes (and thus why an application might be blocking or stuttering). Allows to sort and filter by different criteria to get a better grasp at what and how data is accessed. CVars : This panel displays all CVars that are available. You can not only see their current values, but also modify them, such that you can change the behavior of the application without restarting it. This allows to quickly change parameters of things that you are trying out, such that you can see the effects immediately. Console : The CVar panel also displays a console window, where you can type commands the same way as in the in-game console . Pressing TAB auto-completes input, arrow up/down searches the history, and so on. This can be used to modify CVar state, but also to execute console functions . Stats : Using plStats a game can display the status of certain internals. This allows to make it easy to inspect what a game object is doing or what state some component is in. So instead of printing this debug information on screen inside the game, you can watch it with plInspector. Additionally plInspector allows to mark stats as 'favorites' which means you can output hundreds of stats in your game, but easily only display the subset that you are currently interested in inside plInspector. Additionally, it is now possible to display the history of a stat variable in a separate panel as a graph. This makes it easy to observe how some stats behave over time (such as frame time, frames-per-second, etc.). Time : Displays all plClock instances that are active. Shows the raw time step and the filtered time step, which allows to see hiccups and general performance characteristics of the application. Reflection : Shows all reflected types and their class hierarchy. Also shows which properties each reflected type provides. Data Transfer : This panel allows to pull data from an application. What data can be pulled is determined by what the application provides. For example an application might provide the G-Buffer as a set of images to be pulled. See plDataTransfer for further details. Resources : This panel shows all loaded resources. You can filter by type and name and you can sort the resources by various criteria. Setting up your game to support plInspector Note: None of this setup is required when you use plGameApplication as your application base class, or even better, your game only implements an plGameState and is run directly through plPlayer . The inspection functionality is implemented in the plInspectorPlugin plugin, so you need to have that compiled. In your application you can then either simply always link against that plugin to activate the functionality, or you can load it dynamically at runtime. Additionally the plInspectorPlugin uses plTelemetry to phone home, so you need to have that activated. // Activate plTelemetry such that the inspector plugin can use the network connection. plTelemetry::CreateServer(); // Load the inspector plugin // The plugin contains automatic configuration code (through the plStartup system), // so it will configure itself properly when the engine is initialized by calling plStartup::StartupCore(). // When you are using plApplication, this is done automatically. plPlugin::LoadPlugin(\"plInspectorPlugin\"); You should insert this code somewhere in the engine initialization. When you are using plApplication , put this into the AfterEngineInit function. Additionally you need to make sure that plTelemetry is updated once per frame, to ensure that all changes are sent to plInspector regularly: // Call this once per frame to make sure all changes are transmitted plTelemetry::PerFrameUpdate(); And that's it! The rest is done automatically. Connecting to a Process If you run a custom app or plPlayer on the same machine, plInspector should manage to automatically connect. If you are running your app on a different machine, you need the respective IP address. For plPlayer and custom apps, the default port is 1040 . For EditorEngineProcess.exe , the default port is 1050 . So if you want to connect with plInspector to the editor, you need to provide this port instead. How to get the best out of plInspector Some tips, what to do to benefit from the inspection functionality: Use the logging system plLog to output what your application is doing, and which errors it encounters. Use PLASMA_LOG_BLOCK to group logging information. Use the plStats system to write out information about what is going on in your application. The more information that you track, the easier you can figure out what is going wrong. Use CVars to allow configuration of your code at runtime. It is easy to add CVars and thus you should use them whenever you are working on something new, to be able to tweak its behavior quickly. Once you are finished with something you should strip all unnecessary CVars again, but often it makes sense to still keep some configuration options for later. When you are developing larger subsystems that you might want to know the memory consumption of, use a custom allocator for all allocations in that subsystem, then you can track its memory behavior better. See Also CVars Logging Stats","title":"plInspector"},{"location":"tools/inspector/#plinspector","text":"plInspector is a tool to monitor some internal state of an application. It helps observing how the application operates, which resources it accesses and why it might behave as it does. The current version allows to monitor the following data: Log : The Log panel displays all the log messages. It allows to filter by severity and search by keywords. Memory Usage : The Memory panel displays the number of allocations, the amount of memory in use (per allocator) and a time-line how memory usage changes. Input : Shows which physical buttons are available and what their state is. Also displays the virtual input actions, by which keys they get triggered and what their current state is. Subsystems : Displays information about all the available subsystems in the engine. Plugins : Shows which plugins are loaded and which other plugins they depend on. Global Events : Shows which global events are registered and how often they occur. File Operations : This panel shows which files get accessed by the engine, whether they occur on the main thread, how much data is read or written per operation, how much time that takes (and thus why an application might be blocking or stuttering). Allows to sort and filter by different criteria to get a better grasp at what and how data is accessed. CVars : This panel displays all CVars that are available. You can not only see their current values, but also modify them, such that you can change the behavior of the application without restarting it. This allows to quickly change parameters of things that you are trying out, such that you can see the effects immediately. Console : The CVar panel also displays a console window, where you can type commands the same way as in the in-game console . Pressing TAB auto-completes input, arrow up/down searches the history, and so on. This can be used to modify CVar state, but also to execute console functions . Stats : Using plStats a game can display the status of certain internals. This allows to make it easy to inspect what a game object is doing or what state some component is in. So instead of printing this debug information on screen inside the game, you can watch it with plInspector. Additionally plInspector allows to mark stats as 'favorites' which means you can output hundreds of stats in your game, but easily only display the subset that you are currently interested in inside plInspector. Additionally, it is now possible to display the history of a stat variable in a separate panel as a graph. This makes it easy to observe how some stats behave over time (such as frame time, frames-per-second, etc.). Time : Displays all plClock instances that are active. Shows the raw time step and the filtered time step, which allows to see hiccups and general performance characteristics of the application. Reflection : Shows all reflected types and their class hierarchy. Also shows which properties each reflected type provides. Data Transfer : This panel allows to pull data from an application. What data can be pulled is determined by what the application provides. For example an application might provide the G-Buffer as a set of images to be pulled. See plDataTransfer for further details. Resources : This panel shows all loaded resources. You can filter by type and name and you can sort the resources by various criteria.","title":"plInspector"},{"location":"tools/inspector/#setting-up-your-game-to-support-plinspector","text":"Note: None of this setup is required when you use plGameApplication as your application base class, or even better, your game only implements an plGameState and is run directly through plPlayer . The inspection functionality is implemented in the plInspectorPlugin plugin, so you need to have that compiled. In your application you can then either simply always link against that plugin to activate the functionality, or you can load it dynamically at runtime. Additionally the plInspectorPlugin uses plTelemetry to phone home, so you need to have that activated. // Activate plTelemetry such that the inspector plugin can use the network connection. plTelemetry::CreateServer(); // Load the inspector plugin // The plugin contains automatic configuration code (through the plStartup system), // so it will configure itself properly when the engine is initialized by calling plStartup::StartupCore(). // When you are using plApplication, this is done automatically. plPlugin::LoadPlugin(\"plInspectorPlugin\"); You should insert this code somewhere in the engine initialization. When you are using plApplication , put this into the AfterEngineInit function. Additionally you need to make sure that plTelemetry is updated once per frame, to ensure that all changes are sent to plInspector regularly: // Call this once per frame to make sure all changes are transmitted plTelemetry::PerFrameUpdate(); And that's it! The rest is done automatically.","title":"Setting up your game to support plInspector"},{"location":"tools/inspector/#connecting-to-a-process","text":"If you run a custom app or plPlayer on the same machine, plInspector should manage to automatically connect. If you are running your app on a different machine, you need the respective IP address. For plPlayer and custom apps, the default port is 1040 . For EditorEngineProcess.exe , the default port is 1050 . So if you want to connect with plInspector to the editor, you need to provide this port instead.","title":"Connecting to a Process"},{"location":"tools/inspector/#how-to-get-the-best-out-of-plinspector","text":"Some tips, what to do to benefit from the inspection functionality: Use the logging system plLog to output what your application is doing, and which errors it encounters. Use PLASMA_LOG_BLOCK to group logging information. Use the plStats system to write out information about what is going on in your application. The more information that you track, the easier you can figure out what is going wrong. Use CVars to allow configuration of your code at runtime. It is easy to add CVars and thus you should use them whenever you are working on something new, to be able to tweak its behavior quickly. Once you are finished with something you should strip all unnecessary CVars again, but often it makes sense to still keep some configuration options for later. When you are developing larger subsystems that you might want to know the memory consumption of, use a custom allocator for all allocations in that subsystem, then you can track its memory behavior better.","title":"How to get the best out of plInspector"},{"location":"tools/inspector/#see-also","text":"CVars Logging Stats","title":"See Also"},{"location":"tools/minidumptool/","text":"MiniDump Tool The MiniDumpTool writes a mini-dump (memory, call-stacks) of an application. The mini dump can be used for diagnosing why an application crashed. The tool is used by the unit tests . Arguments The tool takes exactly two arguments: MiniDumpTool -PID 1234 -f \"C:/crashdumps/justnow.dmp\" The first argument is the Process ID of the process for which the memory shall be dumped, the second argument specifies the file where the dump should be written to. Automatic Execution You can integrate writing crash dumps into your application by setting the plCrashHandler_WriteMiniDump through plCrashHandler::SetCrashHandler() . plCrashHandler_WriteMiniDump has options to generate the filename automatically using the current date and time. See Also Unit Tests","title":"MiniDump Tool"},{"location":"tools/minidumptool/#minidump-tool","text":"The MiniDumpTool writes a mini-dump (memory, call-stacks) of an application. The mini dump can be used for diagnosing why an application crashed. The tool is used by the unit tests .","title":"MiniDump Tool"},{"location":"tools/minidumptool/#arguments","text":"The tool takes exactly two arguments: MiniDumpTool -PID 1234 -f \"C:/crashdumps/justnow.dmp\" The first argument is the Process ID of the process for which the memory shall be dumped, the second argument specifies the file where the dump should be written to.","title":"Arguments"},{"location":"tools/minidumptool/#automatic-execution","text":"You can integrate writing crash dumps into your application by setting the plCrashHandler_WriteMiniDump through plCrashHandler::SetCrashHandler() . plCrashHandler_WriteMiniDump has options to generate the filename automatically using the current date and time.","title":"Automatic Execution"},{"location":"tools/minidumptool/#see-also","text":"Unit Tests","title":"See Also"},{"location":"tools/player/","text":"plPlayer The plPlayer is a stand-alone application that can run any Plasma Engine game that is properly embedded in its own DLL. The plEditor can launch a scene directly in the plPlayer application. The plPlayer is meant for testing and as a very slim example of how to write a custom game application. Arguments The Player takes these command line arguments: Player.exe -project \"ProjectPath\" -scene \"ScenePath\" [-wnd \"optional/path/to/Window.ddl\"] [-profile \"OptionalAssetProfileName\"] With: ProjectPath : The absolute path to the project directory. ScenePath : A relative path to a scene file. It is relative to the data directory that it resides in. If it is a path to an .plScene or .plPrefab file, plPlayer automatically redirects the path to the corresponding exported .plObjectGraph file in the AssetCache . Typically you only need to pass the path to the project and scene (or prefab) file. The other options are used by the plEditor to select different configurations. Execution The Player will automatically detect the projects directory by searching the file system for an plProject file. It then executes the core Plasma Engine functionality, meaning it reads the configuration for the data directories as well as the engine plugins from the project config files. If the scene requires custom (game) plugins, they must be referenced in those config files. Then it will execute the regular game loop. Thus, if the scene contains game objects to spawn a character controller or otherwise handle input and move the camera, you will be able to interact with it. If a custom plugin implements a custom game state that will be instantiated and can take full control over the application logic. If no such functionality is available, the Player will instantiate the plFallbackGameState which will spawn a player object, if a plPlayerStartPointComponent is part of the scene. Otherwise it will provide a simple WASD camera movement scheme. If plCameraComponent s are placed in the scene, you can cycle through them using Page Up and Page Down . Pressing Escape will close the Player application (unless overridden by a custom game state). Common Application Features Since plPlayer is built on the application (TODO) framework, it provides a set of useful features common to all Plasma applications. See this page for details. See Also Game States Engine Plugins Projects plEditor Overview","title":"plPlayer"},{"location":"tools/player/#plplayer","text":"The plPlayer is a stand-alone application that can run any Plasma Engine game that is properly embedded in its own DLL. The plEditor can launch a scene directly in the plPlayer application. The plPlayer is meant for testing and as a very slim example of how to write a custom game application.","title":"plPlayer"},{"location":"tools/player/#arguments","text":"The Player takes these command line arguments: Player.exe -project \"ProjectPath\" -scene \"ScenePath\" [-wnd \"optional/path/to/Window.ddl\"] [-profile \"OptionalAssetProfileName\"] With: ProjectPath : The absolute path to the project directory. ScenePath : A relative path to a scene file. It is relative to the data directory that it resides in. If it is a path to an .plScene or .plPrefab file, plPlayer automatically redirects the path to the corresponding exported .plObjectGraph file in the AssetCache . Typically you only need to pass the path to the project and scene (or prefab) file. The other options are used by the plEditor to select different configurations.","title":"Arguments"},{"location":"tools/player/#execution","text":"The Player will automatically detect the projects directory by searching the file system for an plProject file. It then executes the core Plasma Engine functionality, meaning it reads the configuration for the data directories as well as the engine plugins from the project config files. If the scene requires custom (game) plugins, they must be referenced in those config files. Then it will execute the regular game loop. Thus, if the scene contains game objects to spawn a character controller or otherwise handle input and move the camera, you will be able to interact with it. If a custom plugin implements a custom game state that will be instantiated and can take full control over the application logic. If no such functionality is available, the Player will instantiate the plFallbackGameState which will spawn a player object, if a plPlayerStartPointComponent is part of the scene. Otherwise it will provide a simple WASD camera movement scheme. If plCameraComponent s are placed in the scene, you can cycle through them using Page Up and Page Down . Pressing Escape will close the Player application (unless overridden by a custom game state).","title":"Execution"},{"location":"tools/player/#common-application-features","text":"Since plPlayer is built on the application (TODO) framework, it provides a set of useful features common to all Plasma applications. See this page for details.","title":"Common Application Features"},{"location":"tools/player/#see-also","text":"Game States Engine Plugins Projects plEditor Overview","title":"See Also"},{"location":"tools/shadercompiler/","text":"ShaderCompiler The ShaderCompiler is a command-line application to precompile shader permutations. This tool can be used to prepare shaders and shader permutations before they are needed at runtime. On platforms where runtime shader compilation is possible, Plasma will compile shader permutations on demand. This leads to a small delay when a new permutation is encountered, but is very convenient during development. By precompiling all necessary permutations, it is possible to prevent this delay. On platforms that do not allow runtime shader compilation this is even necessary for the shaders to be available, at all. Command Line Options For the full list of available command line options, run ShaderCompiler.exe -help -project <path> : The compiler takes a path to a project to resolve paths relative to the project directory. -platform <name> : The name of the target platform for which to compile the shaders. For example DX11_SM50 . See the command line help for all options. -shader <path1;path2> : Semicolon separated list of paths to shader files or folders containing shaders. Paths may be absolute or relative to the -project directory. If a path to a folder is specified, all .plShader files in that folder are compiled. -perm <PERM1=TRUE PERM2=11> : List of permutation variables to set to fixed values. For all other permutation variables, all possible combinations are used to compile the shaders. Spaces are used to separate multiple arguments. Example ShaderCompiler.exe -project \"C:\\pl\\Data\\Base\" -platform DX11_SM50 -shader \"Shaders\\Debug\" -perm TOPOLOGY=TOPOLOGY_LINES CAMERA_MODE=CAMERA_MODE_PERSPECTIVE See Also Shaders","title":"ShaderCompiler"},{"location":"tools/shadercompiler/#shadercompiler","text":"The ShaderCompiler is a command-line application to precompile shader permutations. This tool can be used to prepare shaders and shader permutations before they are needed at runtime. On platforms where runtime shader compilation is possible, Plasma will compile shader permutations on demand. This leads to a small delay when a new permutation is encountered, but is very convenient during development. By precompiling all necessary permutations, it is possible to prevent this delay. On platforms that do not allow runtime shader compilation this is even necessary for the shaders to be available, at all.","title":"ShaderCompiler"},{"location":"tools/shadercompiler/#command-line-options","text":"For the full list of available command line options, run ShaderCompiler.exe -help -project <path> : The compiler takes a path to a project to resolve paths relative to the project directory. -platform <name> : The name of the target platform for which to compile the shaders. For example DX11_SM50 . See the command line help for all options. -shader <path1;path2> : Semicolon separated list of paths to shader files or folders containing shaders. Paths may be absolute or relative to the -project directory. If a path to a folder is specified, all .plShader files in that folder are compiled. -perm <PERM1=TRUE PERM2=11> : List of permutation variables to set to fixed values. For all other permutation variables, all possible combinations are used to compile the shaders. Spaces are used to separate multiple arguments.","title":"Command Line Options"},{"location":"tools/shadercompiler/#example","text":"ShaderCompiler.exe -project \"C:\\pl\\Data\\Base\" -platform DX11_SM50 -shader \"Shaders\\Debug\" -perm TOPOLOGY=TOPOLOGY_LINES CAMERA_MODE=CAMERA_MODE_PERSPECTIVE","title":"Example"},{"location":"tools/shadercompiler/#see-also","text":"Shaders","title":"See Also"},{"location":"tools/staticlinkutil/","text":"StaticLinkUtil When statically linking libraries into an application the linker will only pull in all the functions and variables that are inside translation units (CPP files) that somehow get referenced. In Plasma a lot of stuff happens automatically (e.g. types register themselves etc.), which is accomplished through global variables that execute code in their constructor during the application's startup phase. This only works when those global variables are actually put into the application by the linker. If the linker does not do that, functionality will not work as intended. Mitigation Contrary to common believe, the linker is not allowed to optimize away global variables. The only reason for not including a global variable into the final binary, is when the entire translation unit where a variable is defined in, is never referenced and thus never even looked at by the linker. To fix this, the StaticLinkUtil inserts macros into each and every file which reference each other. Afterwards every file in a library will reference every other file in that same library and thus once a library is used in any way in some program, the entire library will be pulled in and will then work as intended. These references are accomplished through empty functions that are called in one central location (where PLASMA_STATICLINK_LIBRARY is defined), though the code actually never really calls those functions, but it is enough to force the linker to look at all the other files. Usage Call this tool with the path to the root folder of some library as the sole command line argument: StaticLinkUtil.exe \"C:\\PlasmaEngine\\Code\\Engine\\Foundation\" This will iterate over all files below that folder and insert the proper macros. Also make sure that exactly one file in each library contains the text PLASMA_STATICLINK_LIBRARY(); The parameters and function bodies will be generated automatically and later updated, you do not need to provide more. You only need to run this tool, if you intend to link statically, which is only needed on some platforms. Even then, most new code will work even without always keeping the static link macros up to date, as the issues that it fixes are not too common. If, however, you notice that some types are missing (such as new components) that were just added, you should rerun this tool on the affected libraries. See Also","title":"StaticLinkUtil"},{"location":"tools/staticlinkutil/#staticlinkutil","text":"When statically linking libraries into an application the linker will only pull in all the functions and variables that are inside translation units (CPP files) that somehow get referenced. In Plasma a lot of stuff happens automatically (e.g. types register themselves etc.), which is accomplished through global variables that execute code in their constructor during the application's startup phase. This only works when those global variables are actually put into the application by the linker. If the linker does not do that, functionality will not work as intended.","title":"StaticLinkUtil"},{"location":"tools/staticlinkutil/#mitigation","text":"Contrary to common believe, the linker is not allowed to optimize away global variables. The only reason for not including a global variable into the final binary, is when the entire translation unit where a variable is defined in, is never referenced and thus never even looked at by the linker. To fix this, the StaticLinkUtil inserts macros into each and every file which reference each other. Afterwards every file in a library will reference every other file in that same library and thus once a library is used in any way in some program, the entire library will be pulled in and will then work as intended. These references are accomplished through empty functions that are called in one central location (where PLASMA_STATICLINK_LIBRARY is defined), though the code actually never really calls those functions, but it is enough to force the linker to look at all the other files.","title":"Mitigation"},{"location":"tools/staticlinkutil/#usage","text":"Call this tool with the path to the root folder of some library as the sole command line argument: StaticLinkUtil.exe \"C:\\PlasmaEngine\\Code\\Engine\\Foundation\" This will iterate over all files below that folder and insert the proper macros. Also make sure that exactly one file in each library contains the text PLASMA_STATICLINK_LIBRARY(); The parameters and function bodies will be generated automatically and later updated, you do not need to provide more. You only need to run this tool, if you intend to link statically, which is only needed on some platforms. Even then, most new code will work even without always keeping the static link macros up to date, as the issues that it fixes are not too common. If, however, you notice that some types are missing (such as new components) that were just added, you should rerun this tool on the affected libraries.","title":"Usage"},{"location":"tools/staticlinkutil/#see-also","text":"","title":"See Also"},{"location":"tools/texconv/","text":"plTexConv TexConv is a command-line tool to process textures from typical input formats like PNG, TGA, JPEG and DDS into optimized formats for runtime consumption. The most common scenario is to convert a single input file A.xxx into an optimized format B.yyy . However, the tool has many additional options for advanced uses. Command-line Help Run TexConv.exe with the --help parameter to list all available options. Additionally, TexConv prints the used options when it is executed, to help understand what it is doing. Consult this output for details. General Usage TexConv always produces exactly one output file. It may use multiple input files to assemble the output from. For the assembly, it also needs a channel mapping , which tells it which channel ( Red, Green, Blue or Alpha ) to take from which input file and move it into which channel of the output image. The most straight forward command line is this: TexConv.exe -out D:/result.dds -in0 D:/img.jpg -rgba in0 -out specifies the output file and format -in0 specifies the first input image -rgba tells it that the output image should use all four channels and that they should be taken 1:1 from the input image Multiple Input Files To assemble the output from multiple input files, specify each input file using the -in option with an increasing number: -in0 D:/img0.jpg -in1 D:/img1.jpg -in2 D:/img2.jpg ... When assembling a cubemap from 2D textures, one can also use -right , -left , -top , -bottom , -front , -back or -px , -nx , -py , -ny , -pz , -nz . To map these inputs to the output file, a proper channel mapping is needed. Channel Mappings The channel-mapping options specify from which input to fill the given output channels. You can specify the input for each channel individually like this: -r in0.b -g in0.g -b in0.r -a in1.r Here the RGB channels of the output would be filled using the first input image, but red and blue will get swapped. The alpha channel of the output would be filled with the values from the red channel of the second input image. Specifying the mapping for each channel separately gives the greatest flexibility. For convenience the same can be written using \"swizzling\" operators: -rgb in0.bgr -a in1.r Output Channels The following channel-mapping options are available: -r , -g , -b , -a : These specify single channel assignments. -rg : Specify the red and green channel assignments. -rgb : Specify the red, green and blue channel assignments. -rgba : Specifies all four channel assignments. Mentioning only the R, RG or RGB channel, instructs TexConv to create an output file with only 1, 2 or 3 channels respectively. Input Swizzling When stating which input texture should fill which output channel, one can swizzle the input: -rgba in0 is equivalent to -rgba in0.rgba -rgba in0.bgra will swizzle the input channels -rgb in0.rrr will duplicate the red channel into all channels One may also fill channels with either black or white: -rgb in0 -a white will create a 4 channel output texture but set alpha to fully opaque -rg black -b white will create an entirely blue texture Common Options The most interesting options are listed below. More options are listed by TexConv --help . Output Type -type 2D : The output will be a regular 2D image. -type Cubemap : The output will be a cubemap image. Only supported for DDS output files. When this is specified, one can assemble the cubemap from 6 regular 2D input images. Image Compression -compression none : The output image will be uncompressed. -compression medium : If supported, the output image will use compression without sacrificing too much quality. -compression high : If supported, the output image will use compression and sacrifice quality in favor of a smaller file. Mipmaps By default, TexConv generates mipmaps when the output format supports it. -mipmaps none : Mipmaps will not be generated. -mipmaps Linear : If supported, mipmaps will be generated using a box filter. Usage (sRGB / Gamma Correction) The -usage option specifies the purpose of the output and thus tells TexConv whether to apply gamma correction to the input and output files. The usage only affects the RGB channels. The alpha channel is always considered to contain 'linear' values. If usage is not specified, the 'auto' mode will try to detect the usage from the format and file-name of the first input image. For instance, single and dual channel output formats are always linear. Check the output to see what decision TexConv made. -usage Linear : The output image contains values that do not represent colors. This is typically the case for metallic and roughness textures, as well as all kinds of masks. -usage Color : The output image represents color, such as diffuse/albedo maps. The sRGB flag will be set in the output DDS header. -usage HDR : The output file should use more than 8 bits per pixel for encoding. Consequently all values are stored in linear space. For HDR textures it does not matter whether the data represents color or other data. -usage NormalMap : The output image represents a tangent-space normal map. Values will be normalized and mipmap computation will be optimized slightly. -usage NormalMap_Inverted : The output is a tangent-space normal map with Y pointing in the opposite direction than the input. Image Rescaling -minRes 64 : Specifies the minimum resolution of the output. If the input image is smaller, it will get upscaled. -maxRes 1024 : Specifies the maximum resolution of the output. If the input image is larger, it will get downscaled. -downscale 1 : If this is larger than 0, the input images will be halved in resolution N times. Use this to apply an overall quality reduction. Examples Convert a Color Texture TexConv.exe -out D:/diffuse.dds -in0 D:/diffuse.jpg -rgba in0 -usage color Convert a Normal Map TexConv.exe -out D:/normalmap.dds -in0 D:/normalmap.png -rgb in0 -usage normalmap Create an HDR Cubemap TexConv.exe -out \"D:/skybox.dds\" -in0 \"D:/skymap.hdr\" -rgba in0 -type cubemap -usage hdr A great source for HDR cubemaps is hdrihaven.com . Bake Multiple Images into One TexConv.exe -out \"D:/Baked.dds\" -in0 \"D:/metal.tga\" -in1 \"D:/roughness.png\" -in2 \"D:/DiffuseAlpha.dds\" -r in1.r -g in0.r -b black -a in2.a -usage linear Extract a Single Channel TexConv.exe -out D:/alpha-mask-only.dds -in0 D:/DiffuseAlpha.dds -r in0.a See Also","title":"plTexConv"},{"location":"tools/texconv/#pltexconv","text":"TexConv is a command-line tool to process textures from typical input formats like PNG, TGA, JPEG and DDS into optimized formats for runtime consumption. The most common scenario is to convert a single input file A.xxx into an optimized format B.yyy . However, the tool has many additional options for advanced uses.","title":"plTexConv"},{"location":"tools/texconv/#command-line-help","text":"Run TexConv.exe with the --help parameter to list all available options. Additionally, TexConv prints the used options when it is executed, to help understand what it is doing. Consult this output for details.","title":"Command-line Help"},{"location":"tools/texconv/#general-usage","text":"TexConv always produces exactly one output file. It may use multiple input files to assemble the output from. For the assembly, it also needs a channel mapping , which tells it which channel ( Red, Green, Blue or Alpha ) to take from which input file and move it into which channel of the output image. The most straight forward command line is this: TexConv.exe -out D:/result.dds -in0 D:/img.jpg -rgba in0 -out specifies the output file and format -in0 specifies the first input image -rgba tells it that the output image should use all four channels and that they should be taken 1:1 from the input image","title":"General Usage"},{"location":"tools/texconv/#multiple-input-files","text":"To assemble the output from multiple input files, specify each input file using the -in option with an increasing number: -in0 D:/img0.jpg -in1 D:/img1.jpg -in2 D:/img2.jpg ... When assembling a cubemap from 2D textures, one can also use -right , -left , -top , -bottom , -front , -back or -px , -nx , -py , -ny , -pz , -nz . To map these inputs to the output file, a proper channel mapping is needed.","title":"Multiple Input Files"},{"location":"tools/texconv/#channel-mappings","text":"The channel-mapping options specify from which input to fill the given output channels. You can specify the input for each channel individually like this: -r in0.b -g in0.g -b in0.r -a in1.r Here the RGB channels of the output would be filled using the first input image, but red and blue will get swapped. The alpha channel of the output would be filled with the values from the red channel of the second input image. Specifying the mapping for each channel separately gives the greatest flexibility. For convenience the same can be written using \"swizzling\" operators: -rgb in0.bgr -a in1.r","title":"Channel Mappings"},{"location":"tools/texconv/#output-channels","text":"The following channel-mapping options are available: -r , -g , -b , -a : These specify single channel assignments. -rg : Specify the red and green channel assignments. -rgb : Specify the red, green and blue channel assignments. -rgba : Specifies all four channel assignments. Mentioning only the R, RG or RGB channel, instructs TexConv to create an output file with only 1, 2 or 3 channels respectively.","title":"Output Channels"},{"location":"tools/texconv/#input-swizzling","text":"When stating which input texture should fill which output channel, one can swizzle the input: -rgba in0 is equivalent to -rgba in0.rgba -rgba in0.bgra will swizzle the input channels -rgb in0.rrr will duplicate the red channel into all channels One may also fill channels with either black or white: -rgb in0 -a white will create a 4 channel output texture but set alpha to fully opaque -rg black -b white will create an entirely blue texture","title":"Input Swizzling"},{"location":"tools/texconv/#common-options","text":"The most interesting options are listed below. More options are listed by TexConv --help .","title":"Common Options"},{"location":"tools/texconv/#output-type","text":"-type 2D : The output will be a regular 2D image. -type Cubemap : The output will be a cubemap image. Only supported for DDS output files. When this is specified, one can assemble the cubemap from 6 regular 2D input images.","title":"Output Type"},{"location":"tools/texconv/#image-compression","text":"-compression none : The output image will be uncompressed. -compression medium : If supported, the output image will use compression without sacrificing too much quality. -compression high : If supported, the output image will use compression and sacrifice quality in favor of a smaller file.","title":"Image Compression"},{"location":"tools/texconv/#mipmaps","text":"By default, TexConv generates mipmaps when the output format supports it. -mipmaps none : Mipmaps will not be generated. -mipmaps Linear : If supported, mipmaps will be generated using a box filter.","title":"Mipmaps"},{"location":"tools/texconv/#usage-srgb-gamma-correction","text":"The -usage option specifies the purpose of the output and thus tells TexConv whether to apply gamma correction to the input and output files. The usage only affects the RGB channels. The alpha channel is always considered to contain 'linear' values. If usage is not specified, the 'auto' mode will try to detect the usage from the format and file-name of the first input image. For instance, single and dual channel output formats are always linear. Check the output to see what decision TexConv made. -usage Linear : The output image contains values that do not represent colors. This is typically the case for metallic and roughness textures, as well as all kinds of masks. -usage Color : The output image represents color, such as diffuse/albedo maps. The sRGB flag will be set in the output DDS header. -usage HDR : The output file should use more than 8 bits per pixel for encoding. Consequently all values are stored in linear space. For HDR textures it does not matter whether the data represents color or other data. -usage NormalMap : The output image represents a tangent-space normal map. Values will be normalized and mipmap computation will be optimized slightly. -usage NormalMap_Inverted : The output is a tangent-space normal map with Y pointing in the opposite direction than the input.","title":"Usage (sRGB / Gamma Correction)"},{"location":"tools/texconv/#image-rescaling","text":"-minRes 64 : Specifies the minimum resolution of the output. If the input image is smaller, it will get upscaled. -maxRes 1024 : Specifies the maximum resolution of the output. If the input image is larger, it will get downscaled. -downscale 1 : If this is larger than 0, the input images will be halved in resolution N times. Use this to apply an overall quality reduction.","title":"Image Rescaling"},{"location":"tools/texconv/#examples","text":"","title":"Examples"},{"location":"tools/texconv/#convert-a-color-texture","text":"TexConv.exe -out D:/diffuse.dds -in0 D:/diffuse.jpg -rgba in0 -usage color","title":"Convert a Color Texture"},{"location":"tools/texconv/#convert-a-normal-map","text":"TexConv.exe -out D:/normalmap.dds -in0 D:/normalmap.png -rgb in0 -usage normalmap","title":"Convert a Normal Map"},{"location":"tools/texconv/#create-an-hdr-cubemap","text":"TexConv.exe -out \"D:/skybox.dds\" -in0 \"D:/skymap.hdr\" -rgba in0 -type cubemap -usage hdr A great source for HDR cubemaps is hdrihaven.com .","title":"Create an HDR Cubemap"},{"location":"tools/texconv/#bake-multiple-images-into-one","text":"TexConv.exe -out \"D:/Baked.dds\" -in0 \"D:/metal.tga\" -in1 \"D:/roughness.png\" -in2 \"D:/DiffuseAlpha.dds\" -r in1.r -g in0.r -b black -a in2.a -usage linear","title":"Bake Multiple Images into One"},{"location":"tools/texconv/#extract-a-single-channel","text":"TexConv.exe -out D:/alpha-mask-only.dds -in0 D:/DiffuseAlpha.dds -r in0.a","title":"Extract a Single Channel"},{"location":"tools/texconv/#see-also","text":"","title":"See Also"},{"location":"ui/imgui/","text":"ImGui Dear ImGui is a well known library for building immediate mode GUIs . The ImGui library was built to make it quick and easy to create GUIs that need to be functional, but not pretty. ImGui is popular with programmers, because it only takes a few lines of code to build UI panels with buttons, sliders, text boxes, checkboxs and many more. A very common use case for ImGui is for quick developer tools and for exposing options in tech demos. On the other hand, ImGui is not meant to be styled. Changing the appearance of ImGui elements is difficult, and controlling the layout of elements is only very basic. Using ImGui Dear ImGui is integrated by the singleton class plImgui . To use ImGui, you need to allocate one such instance first: #ifdef BUILDSYSTEM_ENABLE_IMGUI_SUPPORT if (plImgui::GetSingleton() == nullptr) { PLASMA_DEFAULT_NEW(plImgui); } #endif This can be done for example in OnActivation() of a custom game state . Similarly, you should delete the plImgui instance at shutdown: #ifdef BUILDSYSTEM_ENABLE_IMGUI_SUPPORT if (plImgui::GetSingleton() != nullptr) { plImgui* pImgui = plImgui::GetSingleton(); PLASMA_DEFAULT_DELETE(pImgui); } #endif During a frame, the plImgui instance needs to know which view to render the UI elements to. Therefore you should call this every frame: plImgui::GetSingleton()->SetCurrentContextForView(m_hMainView); Often you only want to pass input to ImGui during certain phases of your game. This can be controlled via plImgui::SetPassInputToImgui() and whether ImGui currently has focus in a certain UI element can be queried through plImGui::WantsInput() . From there on, all the functionality of the Dear ImGui library is used directly, without any Plasma specific wrappers. For example a simple panel is created like this: ImGui::SetNextWindowSize(ImVec2(200, 100), ImGuiCond_FirstUseEver); ImGui::Begin(\"Imgui Window\", &window); ImGui::Text(\"Hello World!\"); ImGui::SliderFloat(\"Slider\", &slider, 0.0f, 1.0f); ImGui::ColorEdit3(\"Color\", color); if (ImGui::Button(\"Toggle Stats\")) { stats = !stats; } if (stats) { ImGui::Text(\"Application average %.3f ms/frame (%.1f FPS)\", 1000.0f / ImGui::GetIO().Framerate, ImGui::GetIO().Framerate); } ImGui::End(); Limitations It is very difficult to adjust the appearance of ImGui elements. Use ImGui for developer tools and to prototype ideas quickly. Prefer to use RmlUi once you need more control over the look and feel. See Also Ingame UI RmlUi","title":"ImGui"},{"location":"ui/imgui/#imgui","text":"Dear ImGui is a well known library for building immediate mode GUIs . The ImGui library was built to make it quick and easy to create GUIs that need to be functional, but not pretty. ImGui is popular with programmers, because it only takes a few lines of code to build UI panels with buttons, sliders, text boxes, checkboxs and many more. A very common use case for ImGui is for quick developer tools and for exposing options in tech demos. On the other hand, ImGui is not meant to be styled. Changing the appearance of ImGui elements is difficult, and controlling the layout of elements is only very basic.","title":"ImGui"},{"location":"ui/imgui/#using-imgui","text":"Dear ImGui is integrated by the singleton class plImgui . To use ImGui, you need to allocate one such instance first: #ifdef BUILDSYSTEM_ENABLE_IMGUI_SUPPORT if (plImgui::GetSingleton() == nullptr) { PLASMA_DEFAULT_NEW(plImgui); } #endif This can be done for example in OnActivation() of a custom game state . Similarly, you should delete the plImgui instance at shutdown: #ifdef BUILDSYSTEM_ENABLE_IMGUI_SUPPORT if (plImgui::GetSingleton() != nullptr) { plImgui* pImgui = plImgui::GetSingleton(); PLASMA_DEFAULT_DELETE(pImgui); } #endif During a frame, the plImgui instance needs to know which view to render the UI elements to. Therefore you should call this every frame: plImgui::GetSingleton()->SetCurrentContextForView(m_hMainView); Often you only want to pass input to ImGui during certain phases of your game. This can be controlled via plImgui::SetPassInputToImgui() and whether ImGui currently has focus in a certain UI element can be queried through plImGui::WantsInput() . From there on, all the functionality of the Dear ImGui library is used directly, without any Plasma specific wrappers. For example a simple panel is created like this: ImGui::SetNextWindowSize(ImVec2(200, 100), ImGuiCond_FirstUseEver); ImGui::Begin(\"Imgui Window\", &window); ImGui::Text(\"Hello World!\"); ImGui::SliderFloat(\"Slider\", &slider, 0.0f, 1.0f); ImGui::ColorEdit3(\"Color\", color); if (ImGui::Button(\"Toggle Stats\")) { stats = !stats; } if (stats) { ImGui::Text(\"Application average %.3f ms/frame (%.1f FPS)\", 1000.0f / ImGui::GetIO().Framerate, ImGui::GetIO().Framerate); } ImGui::End();","title":"Using ImGui"},{"location":"ui/imgui/#limitations","text":"It is very difficult to adjust the appearance of ImGui elements. Use ImGui for developer tools and to prototype ideas quickly. Prefer to use RmlUi once you need more control over the look and feel.","title":"Limitations"},{"location":"ui/imgui/#see-also","text":"Ingame UI RmlUi","title":"See Also"},{"location":"ui/rmlui-canvas2d-component/","text":"RmlUI Canvas 2D Component The RML UI integration is in a good state, and building UIs is very much possible, however, the functionality is currently undocumented. See Also RmlUi","title":"RmlUI Canvas 2D Component"},{"location":"ui/rmlui-canvas2d-component/#rmlui-canvas-2d-component","text":"The RML UI integration is in a good state, and building UIs is very much possible, however, the functionality is currently undocumented.","title":"RmlUI Canvas 2D Component"},{"location":"ui/rmlui-canvas2d-component/#see-also","text":"RmlUi","title":"See Also"},{"location":"ui/rmlui/","text":"RmlUi RmlUi is a third-party GUI library that uses an HTML-like syntax to describe UI elements, and CSS to style them. RmlUi is lightweight, yet flexible. Support for RmlUi is provided through a dedicated engine plugin . To enable it in your project, activate the plugin in the project settings . Rml Documentation The documentation for RmlUi can be found here . Please refer to that documenation for any questions around how to use RmlUi. Work in Progress The integration of RmlUi into Plasma Engine is functional, but still a work-in-progress. At the moment you can only build 2D GUIs that appear on top of the screen. In the future we plan to also support placing GUI elements inside the 3D environment. Also, currently you need to work directly with the RmlUi C++ code to interact with the UI. We may add convenience functionality to make simple use cases of GUIs easier. There is no TypeScript binding for RmlUi. Most likely this will never change, however, if we get around to adding some convenience features, that may make it possible to use a limited feature set from script code. See Also Ingame UI ImGui","title":"RmlUi"},{"location":"ui/rmlui/#rmlui","text":"RmlUi is a third-party GUI library that uses an HTML-like syntax to describe UI elements, and CSS to style them. RmlUi is lightweight, yet flexible. Support for RmlUi is provided through a dedicated engine plugin . To enable it in your project, activate the plugin in the project settings .","title":"RmlUi"},{"location":"ui/rmlui/#rml-documentation","text":"The documentation for RmlUi can be found here . Please refer to that documenation for any questions around how to use RmlUi.","title":"Rml Documentation"},{"location":"ui/rmlui/#work-in-progress","text":"The integration of RmlUi into Plasma Engine is functional, but still a work-in-progress. At the moment you can only build 2D GUIs that appear on top of the screen. In the future we plan to also support placing GUI elements inside the 3D environment. Also, currently you need to work directly with the RmlUi C++ code to interact with the UI. We may add convenience functionality to make simple use cases of GUIs easier. There is no TypeScript binding for RmlUi. Most likely this will never change, however, if we get around to adding some convenience features, that may make it possible to use a limited feature set from script code.","title":"Work in Progress"},{"location":"ui/rmlui/#see-also","text":"Ingame UI ImGui","title":"See Also"},{"location":"ui/ui/","text":"Ingame UI To create ingame user interfaces, there are two options available: ImGui RmlUi ImGui is very easy to use. With just a few lines of code you can add a 2D user interface to your game. It is very limited in styling, though. It's main purpose is either to mock up a UI quickly, or to expose developer tools. RmlUi on the other hand is meant for building proper game UI, and therefore supports lots of styling options through CSS and and HTML like syntax for layouting. It is more work to set up, but gives you all the flexibilty you need. It is recommended to use ImGui during early development and switch to RmlUi once your game reaches a more mature state. It is also possible to use both UI systems in parallel. See Also ImGui RmlUi","title":"Ingame UI"},{"location":"ui/ui/#ingame-ui","text":"To create ingame user interfaces, there are two options available: ImGui RmlUi ImGui is very easy to use. With just a few lines of code you can add a 2D user interface to your game. It is very limited in styling, though. It's main purpose is either to mock up a UI quickly, or to expose developer tools. RmlUi on the other hand is meant for building proper game UI, and therefore supports lots of styling options through CSS and and HTML like syntax for layouting. It is more work to set up, but gives you all the flexibilty you need. It is recommended to use ImGui during early development and switch to RmlUi once your game reaches a more mature state. It is also possible to use both UI systems in parallel.","title":"Ingame UI"},{"location":"ui/ui/#see-also","text":"ImGui RmlUi","title":"See Also"}]}